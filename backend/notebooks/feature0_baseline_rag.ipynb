{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "p0-title-md",
   "metadata": {},
   "source": [
    "# The Baseline RAG Pipeline\n",
    "\n",
    "---\n",
    "\n",
    "This notebook is your **starting point**. The pipeline is already built and working: run it, explore its outputs, question it, and find its limits.\n",
    "\n",
    "**How to use this notebook**\n",
    "\n",
    "| | |\n",
    "|---|---|\n",
    "| ðŸ“– **Read** | The explanations use plain language, no coding background needed |\n",
    "| â–¶ï¸ **Run** | Execute cells top to bottom with Shift+Enter to see the pipeline in action |\n",
    "| ðŸ’¬ **Discuss** | Talk about the outputs with your peers, do they make sense? Would you trust them? |\n",
    "| ðŸ”§ **Experiment** | Modify queries, tweak parameters, break things on purpose |\n",
    "| ðŸš€ **Extend** | The Tasks section points to what you can take further |\n",
    "\n",
    "> You don't need to understand every line of code. Focus on what the system gets right and wrong and on thinking about how this would apply in your own context.\n",
    "\n",
    "---\n",
    "\n",
    "## What This Notebook Covers\n",
    "\n",
    "**Retrieval-Augmented Generation (RAG)** combines a search engine with an AI assistant. Instead of the AI making things up from memory, it first searches your documents and then answers based on what it finds. The answer can always be traced back to a source.\n",
    "\n",
    "```\n",
    "Your question  ->  Search your documents  ->  AI answers using only those documents\n",
    "```\n",
    "\n",
    "| Notebook | Focus |\n",
    "|---|---|\n",
    "| **Baseline (this notebook)** | Working baseline prototype |\n",
    "| Feature Track 1 | How documents are split into searchable pieces |\n",
    "| Feature Track 2 | How to measure answer quality |\n",
    "| Feature Track 3 | Reliable, structured outputs |\n",
    "| Feature Track 4 | Better retrieval strategies |\n",
    "| Feature Track 5 | Multi-step agent workflows |\n",
    "| Feature Track 6 | Connecting to the chat frontend |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p0-why-rag-md",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Why RAG? The Problem with a Standalone LLM\n",
    "\n",
    "### The Scenario\n",
    "**PrimePack AG** buys packaging materials (pallets, cardboard boxes, tape) from multiple suppliers. Sustainability claims are increasingly scrutinised by customers and regulators. Employees need to answer questions like:\n",
    "> *\"What is the GWP of the Logypal 1 pallet, and is the figure verified?\"*  \n",
    "> *\"Can we tell a customer that the tesa tape is PFAS-free?\"*  \n",
    "> *\"Which of our suppliers have a certified EPD?\"*\n",
    "\n",
    "### Why Not Just Ask ChatGPT?\n",
    "A general-purpose LLM has three fundamental problems for this task:\n",
    "\n",
    "| Problem | Why It Matters |\n",
    "|---|---|\n",
    "| **Internal document** | LLMs don't know about internal company documents. |\n",
    "| **Hallucination** | When asked about unknown products the LLM invents plausible-sounding but false figures. |\n",
    "| **No evidence trail** | Even when correct, a raw LLM answer cannot be traced back to a source document. |\n",
    "\n",
    "### The RAG Solution\n",
    "RAG adds a **retrieval step** between the user's question and the LLM:\n",
    "\n",
    "```\n",
    " Documents â”€â”€â–º Chunker â”€â”€â–º Embedder â”€â”€â–º Vector DB\n",
    "                                              â”‚\n",
    " User query â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º Embedder â”€â”€â”€â”€â”€â–º  Retriever â”€â”€â–º Top-k Chunks\n",
    "                                                                      â”‚\n",
    "                                                               LLM + Prompt\n",
    "                                                                      â”‚\n",
    "                                                               Answer + Sources\n",
    "```\n",
    "\n",
    "The LLM only sees documents that are **actually in the corpus**. The answer can be traced to specific source chunks. If the corpus does not contain the answer, the LLM is instructed to say so.\n",
    "\n",
    "### What RAG Does *Not* Fix\n",
    "RAG shifts the problem from hallucination to **retrieval quality**. If the right chunk is not retrieved, the answer will still be wrong (or absent). The later feature tracks address exactly this: better chunking, better retrieval, and better output structure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p0-concepts-md",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Core Concepts\n",
    "\n",
    "### Chunks\n",
    "\n",
    "A **chunk** is a short excerpt from a source document, a section of a PDF, one sheet of a spreadsheet, or one heading-delimited paragraph of a Markdown file. Chunks are the unit of indexing and retrieval.\n",
    "\n",
    "```python\n",
    "@dataclass\n",
    "class Chunk:\n",
    "    id: str           # unique identifier\n",
    "    title: str        # e.g. section heading\n",
    "    content: str      # the text that gets embedded\n",
    "    metadata: dict    # source_file, page, ...\n",
    "```\n",
    "\n",
    "### Embeddings\n",
    "An **embedding** converts text to a dense numeric vector (e.g. 384 dimensions). Semantically similar texts produce similar vectors. Here we use `all-MiniLM-L6-v2`, a compact local model that runs without an API key.\n",
    "\n",
    "### Vector Store (ChromaDB)\n",
    "A **vector store** persists chunk embeddings on disk and supports approximate nearest-neighbour search. Given a query embedding, it returns the `top_k` most similar chunks in milliseconds.\n",
    "\n",
    "### Retriever\n",
    "A **retriever** wraps a vector store and exposes a single `retrieve(query)` method. The baseline uses a `VectorStoreRetriever` with `top_k=5`.\n",
    "\n",
    "### RAG Agent\n",
    "The **RAG agent** combines a retriever and an LLM. Its `answer()` method:\n",
    "1. Embeds the query\n",
    "2. Retrieves the top-k chunks\n",
    "3. Formats chunks as XML `<source>` tags in the prompt\n",
    "4. Calls the LLM and returns the answer + cited sources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p0-setup-md",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Setup\n",
    "\n",
    "**Prerequisites:** `conversational-toolkit` and `backend` must be installed in editable mode (`pip install -e conversational-toolkit/ && pip install -e backend/`). For the **Ollama** backend, start `ollama serve` and pull the model (`ollama pull mistral-nemo:12b`). For **OpenAI**, set `OPENAI_API_KEY` in your environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "p0-setup-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from conversational_toolkit.agents.base import QueryWithContext\n",
    "from conversational_toolkit.embeddings.sentence_transformer import (\n",
    "    SentenceTransformerEmbeddings,\n",
    ")\n",
    "from conversational_toolkit.retriever.vectorstore_retriever import VectorStoreRetriever\n",
    "\n",
    "from sme_kt_zh_collaboration_rag.feature0_baseline_rag import (\n",
    "    load_chunks,\n",
    "    inspect_chunks,\n",
    "    build_vector_store,\n",
    "    inspect_retrieval,\n",
    "    build_agent,\n",
    "    build_llm,\n",
    "    ask,\n",
    "    DATA_DIR,\n",
    "    VS_PATH,\n",
    "    EMBEDDING_MODEL,\n",
    "    RETRIEVER_TOP_K,\n",
    ")\n",
    "\n",
    "# Choose your LLM backend: \"ollama\" (local, requires `ollama serve`) or \"openai\" (requires OPENAI_API_KEY)\n",
    "BACKEND = \"ollama\"  # set this before running\n",
    "\n",
    "if not BACKEND:\n",
    "    raise ValueError(\n",
    "        'BACKEND is not set. Edit the line above and set it to \"ollama\", or \"openai\".\\n'\n",
    "        \"See Renku_README.md for setup instructions.\"\n",
    "    )\n",
    "\n",
    "ROOT = Path().resolve().parents[1]\n",
    "print(f\"Project root : {ROOT}\")\n",
    "print(f\"Data dir     : {DATA_DIR}\")\n",
    "print(f\"Vector store : {VS_PATH}\")\n",
    "print(f\"LLM backend  : {BACKEND}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0mf2j4xq79v",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Before the RAG Pipeline: The LLM on Its Own\n",
    "\n",
    "A **large language model (LLM)** is a neural network trained on billions of words of text. It can summarise documents, answer questions, and generate structured output, but only from knowledge baked into its weights during training. It has no direct access to your internal documents.\n",
    "\n",
    "Before building the RAG pipeline, let's interact with the LLM directly to understand what it can and cannot do on its own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3tx6k25ljdx",
   "metadata": {},
   "outputs": [],
   "source": [
    "from conversational_toolkit.llms.base import LLMMessage, Roles\n",
    "\n",
    "# Reuse the backend you chose in the Setup cell above\n",
    "llm_standalone = build_llm(backend=BACKEND)\n",
    "\n",
    "# A question the LLM can answer from general training data\n",
    "general_question = \"What does GWP stand for, and what unit is it typically measured in?\"\n",
    "\n",
    "response_general = await llm_standalone.generate(\n",
    "    [LLMMessage(role=Roles.USER, content=general_question)]\n",
    ")\n",
    "print(\"---------------------------\")\n",
    "print(f\"Q: {general_question}\\n\")\n",
    "print(\"---------------------------\")\n",
    "print(f\"A: {response_general.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vkh4o5519",
   "metadata": {},
   "source": [
    "The LLM answers that correctly, GWP is a well-known concept covered in its training data.\n",
    "\n",
    "Now ask something specific to PrimePack AG's product portfolio:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sy3pgyhowo9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A product-specific question the LLM has never seen in training\n",
    "primepack_question = \"What is the Global Warming Potential (GWP) of the Logypal 1 pallet sold by PrimePack AG, and is the figure third-party verified? Provide the link to PrimePack AG's official website.\"\n",
    "\n",
    "response_pp = await llm_standalone.generate(\n",
    "    [LLMMessage(role=Roles.USER, content=primepack_question)]\n",
    ")\n",
    "print(\"---------------------------\")\n",
    "print(f\"Q: {primepack_question}\\n\")\n",
    "print(\"---------------------------\")\n",
    "print(f\"A: {response_pp.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99690afe",
   "metadata": {},
   "source": [
    "PrimePack AG is a fictional company, no training data exists for this product\n",
    "- If the model gave a specific figure or website link: it is hallucinated.\n",
    "- If the model said 'I don't know': that is honest, but still not useful.\n",
    "\n",
    "Either way, the LLM cannot provide the actual figure with a verifiable source."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a69d9b",
   "metadata": {},
   "source": [
    "> **Task: Compare LLM Backends**\n",
    "> 1. **Switch backends.** Change BACKEND to \"ollama\" (or \"openai\" if you started with Ollama) and re-run the two question cells above. Does one model hallucinate a specific figure or website link while the other declines? How confident does each answer sound?\n",
    "> 2. **Change the question.** Replace the tesa question with something you could imagine being asked in a real supplier audit. Does the standalone LLM give you an answer you would trust?\n",
    "> 3. **Note the pattern.** Regardless of whether the model hallucinates or says \"I don't know\", ask: could you send this response to a customer? What is missing?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fgvbri1fhj8",
   "metadata": {},
   "source": [
    "### Why do different models behave differently?\n",
    "\n",
    "**OpenAI models** (GPT-4o, GPT-4o-mini) are extensively trained with human feedback (Reinforcement Learning from Human Feedback(RLHF)) to decline when they lack reliable information. For a fictional company like PrimePack AG, with no public web presence, the model has learned to say \"I don't know\" rather than confabulate a specific figure.\n",
    "\n",
    "**Smaller local models** (Mistral, LLaMA 7â€“13 B) are typically less safety-fine-tuned. Without the reinforcement signal that penalises confident wrong answers, they are more likely to generate a plausible-sounding but fabricated answer.\n",
    "\n",
    "**The problem in either case:** \"I don't know\" and a hallucinated answer are equally useless to an employee who needs to respond to a CSRD audit. The correct response, *\"The verified GWP is X kg COâ‚‚e according to the 2023 EPD (source: EPD_pallet_relicyc_logypal1.pdf)\"*, requires access to the actual document."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qw0u2tdx1kj",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Choosing a Backend: OpenAI API vs Local Models\n",
    "\n",
    "Two LLM backends are available in this workshop. Both expose the same interface, switching requires changing a single variable.\n",
    "\n",
    "### Comparison\n",
    "\n",
    "| | **OpenAI API** (`gpt-4o-mini`, `gpt-4o`, â€¦) | **Ollama -> local** (`mistral-nemo:12b`, `llama3.2`, â€¦) |\n",
    "|---|---|---|\n",
    "| **Data security** | Queries and document chunks are sent to OpenAI's servers. You can request zero-data-retention. | Everything stays on-premise. Nothing leaves the machine. Suitable for confidential documents without any external data agreements. |\n",
    "| **Model capability** | State-of-the-art. Follows complex instructions reliably, structures output well, handles edge cases. `gpt-4o-mini` is the default for this workshop, it is much cheaper than `gpt-4o` with most of the capability for RAG tasks. | Smaller models (7â€“13 B parameters) are weaker on complex reasoning and strict rule-following. For straightforward retrieval and summarisation tasks the quality gap narrows considerably. |\n",
    "| **Cost** | Per-token billing. A typical RAG query costs a fraction of a cent. See the cost estimation section below. | No API cost: you pay for hardware (CPU/GPU) and electricity. |\n",
    "| **Latency** | ~1â€“2 s per query (network + model time) | CPU-only: ~20â€“60 s. GPU: ~2â€“5 s |\n",
    "| **Setup** | One API key, no local hardware required | `ollama serve` + model download |\n",
    "\n",
    "### Self-Hosting Larger Models\n",
    "\n",
    "The quality gap between a 12 B local model and GPT-4o can be substantially closed at larger model sizes:\n",
    "\n",
    "- **LLaMA 3.1 70 B, Mistral Large 2, Qwen 2.5 72 B**: run on GPUs. Quality can approach GPT-4o on structured tasks like RAG.\n",
    "- **Quantised models (GGUF / GPTQ):** Reduce memory requirements by 50â€“75% with a modest quality trade-off, making larger models accessible on smaller hardware.\n",
    "- **Production stacks:** `vLLM` and `llama.cpp` server provide OpenAI-compatible APIs with batching and much higher throughput than `ollama` alone. \n",
    "\n",
    "**For this workshop** `gpt-4o-mini` (OpenAI) and `mistral-nemo:12b` (Ollama) are both sufficient to demonstrate the full RAG pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "clh6qsmd1uk",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Cost Estimation\n",
    "\n",
    "API costs scale with the number of tokens processed. **Input tokens** (your system prompt and retrieved document chunks) are cheaper than **output tokens** (the model's generated answer).\n",
    "\n",
    "OpenAI API pricing for all models can be found [here](https://developers.openai.com/api/docs/pricing/). As an example, prices for `gpt-4o-mini` are:\n",
    "\n",
    "| Token type | Price |\n",
    "|---|---|\n",
    "| Input | $0.15 / 1 M tokens |\n",
    "| Output | $0.60 / 1 M tokens |\n",
    "\n",
    "A rough rule of thumb: **1 token â‰ˆ 4 characters** of English text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vz7aliuggbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cost estimation for gpt-4o-mini\n",
    "INPUT_PRICE_PER_TOKEN = 0.15 / 1_000_000  # USD\n",
    "OUTPUT_PRICE_PER_TOKEN = 0.60 / 1_000_000  # USD\n",
    "\n",
    "\n",
    "def estimate_tokens(text: str) -> int:\n",
    "    \"\"\"Rough estimate: 1 token â‰ˆ 4 characters.\"\"\"\n",
    "    return max(1, len(text) // 4)\n",
    "\n",
    "\n",
    "def estimate_cost(input_text: str, output_text: str) -> dict:\n",
    "    input_tok = estimate_tokens(input_text)\n",
    "    output_tok = estimate_tokens(output_text)\n",
    "    cost = input_tok * INPUT_PRICE_PER_TOKEN + output_tok * OUTPUT_PRICE_PER_TOKEN\n",
    "    return {\"input_tokens\": input_tok, \"output_tokens\": output_tok, \"cost_usd\": cost}\n",
    "\n",
    "\n",
    "# Simulate a typical RAG query: system prompt + 5 retrieved chunks + user question --> short generated answer\n",
    "example_input = (\n",
    "    \"You are a helpful AI assistant specialised in sustainability for PrimePack AG. \"\n",
    "    \"Answer only using the provided document excerpts. Cite your sources.\\n\\n\"\n",
    "    \"Source: EPD_pallet_relicyc_logypal1.pdf\\n\"\n",
    "    \"The Logypal 1 pallet has a declared GWP of 3.2 kg CO\\u2082e per functional unit (A1\\u2013A3), \"\n",
    "    \"verified by an independent third-party auditor under ISO\\u202014044.\\n\\n\"\n",
    "    \"Source: ART_product_catalog.md\\n\"\n",
    "    \"The Logypal 1 (Product ID: 20-100) is a recycled-plastic pallet supplied by Relicyc. \"\n",
    "    \"It is listed as the primary pallet for heavy-duty use in the PrimePack AG portfolio.\\n\\n\"\n",
    "    \"[... 3 more retrieved chunks ...]\\n\\n\"\n",
    "    \"Q: What is the GWP of the Logypal 1 pallet, and is it verified?\"\n",
    ")\n",
    "example_output = (\n",
    "    \"The Logypal 1 pallet has a GWP of 3.2\\u202fkg\\u202fCO\\u2082e per functional unit (A1\\u2013A3), \"\n",
    "    \"according to the third-party verified EPD (EPD_pallet_relicyc_logypal1.pdf). \"\n",
    "    \"The figure has been independently audited under ISO\\u202014044.\"\n",
    ")\n",
    "\n",
    "info = estimate_cost(example_input, example_output)\n",
    "print(f\"Input  : ~{info['input_tokens']:>5,} tokens (prompt + chunks + question)\")\n",
    "print(f\"Output : ~{info['output_tokens']:>5,} tokens (generated answer)\")\n",
    "print(f\"Cost   : ${info['cost_usd']:.6f} per query\")\n",
    "print()\n",
    "print(f\"At 1,000 queries / day     ->  ${info['cost_usd'] * 1_000:>8.4f} / day\")\n",
    "print(f\"At 10,000 queries / day    ->  ${info['cost_usd'] * 10_000:>8.4f} / day\")\n",
    "print(f\"At 1,000,000 queries / day ->  ${info['cost_usd'] * 1_000_000:>8.2f} / day\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a29b0c8a",
   "metadata": {},
   "source": [
    "The largest cost driver is context length. More retrieved chunks = more input tokens.\n",
    "top_k=5 (~2,000 input tokens) is a reasonable starting point for RAG."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4rue9d6isf",
   "metadata": {},
   "source": [
    "> **Consider:** For your use case, how many queries would the system handle per day? At what volume does the per-query cost become meaningful? Would data-security requirements push you towards a local model even at lower throughput?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934fe3d0",
   "metadata": {},
   "source": [
    "---\n",
    "# RAG Pipeline\n",
    "\n",
    "Now that we have seen what an LLM can and cannot do on its own, we are ready to build the retrieval layer that makes it genuinely useful. The following five steps walk through the full RAG pipeline end-to-end, from loading the documents all the way to a sourced answer.\n",
    "\n",
    "```\n",
    " Documents â”€â”€â–º Chunker â”€â”€â–º Embedder â”€â”€â–º Vector DB\n",
    "                                              â”‚\n",
    " User query â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º Embedder â”€â”€â”€â”€â”€â–º  Retriever â”€â”€â–º Top-k Chunks\n",
    "                                                                      â”‚\n",
    "                                                               LLM + Prompt\n",
    "                                                                      â”‚\n",
    "                                                               Answer + Sources\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p0-step1-md",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 1: Load and Chunk Documents\n",
    "\n",
    "The `load_chunks()` function walks `data/` and dispatches each file to the right chunker:\n",
    "\n",
    "| Extension | Chunker | Strategy |\n",
    "|---|---|---|\n",
    "| `.pdf` | `PDFChunker` | Convert to Markdown via `pymupdf4llm`, split on `#` headings |\n",
    "| `.xlsx`, `.xls` | `ExcelChunker` | One chunk per sheet, serialised as a Markdown table |\n",
    "| `.md`, `.txt` | `MarkdownChunker` | Split on `#` headings |\n",
    "\n",
    "The result is a flat `list[Chunk]`, the same structure regardless of the original format.\n",
    "\n",
    "You can use `max_files=5` here for speed. Remove the limit (or set `None`) to load the full corpus.\n",
    "\n",
    "> **Feature Track 1** explores the importance of ingestion and alternative chunking strategies in more depth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "p0-step1-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load documents from DATA_DIR and split them into chunks.\n",
    "chunks = load_chunks(max_files=None)\n",
    "# Print a statistical summary and sampled content for visual inspection.\n",
    "inspect_chunks(chunks)\n",
    "\n",
    "# Print size distribution\n",
    "char_lengths = [len(c.content) for c in chunks]\n",
    "over_limit = sum(1 for n in char_lengths if n > 1024)\n",
    "print(f\"\\nChunks total       : {len(chunks)}\")\n",
    "print(f\"Mean length (chars): {sum(char_lengths) // len(char_lengths)}\")\n",
    "print(f\"Over 1024-char limit (â‰ˆ256 tok embedding limit): {over_limit} / {len(chunks)}\")\n",
    "print(\"\\nSuccessfully loaded and chunked the documents!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p0-step1-inspect-md",
   "metadata": {},
   "source": [
    "### What a Chunk Looks Like\n",
    "\n",
    "Each chunk carries a `title` (the heading), the raw text `content`, and a `metadata` dict\n",
    "with the source file name. This metadata is returned alongside the answer so the user can\n",
    "trace every claim back to its origin document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "p0-step1-inspect-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print 3 representative chunks\n",
    "for c in chunks[:3]:\n",
    "    print(f\"--- [{c.metadata.get('source_file', '?')}] ---\")\n",
    "    print(f\"Title  : {c.title!r}\")\n",
    "    print(f\"Length : {len(c.content)} chars\")\n",
    "    print(f\"Preview: {c.content[:200].strip()!r}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p0-step2-md",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 2: Embed Chunks and Build the Vector Store\n",
    "\n",
    "`SentenceTransformerEmbeddings` converts every chunk's `content` to a 384-dimensional vector using `all-MiniLM-L6-v2`. The resulting matrix (shape `[n_chunks, 384]`) is inserted into a persistent `ChromaDBVectorStore`.\n",
    "\n",
    "**On subsequent runs**, leave `reset=False` (the default) to skip re-embedding, it takes time and the store on disk is already correct. Pass `reset=True` only when the corpus or chunking strategy changes.\n",
    "\n",
    "---\n",
    "\n",
    "### Embedding Models\n",
    "Two model families are implemented in the toolkit. The choice affects retrieval quality, cost, context limit, and data security.\n",
    "\n",
    "| | `all-MiniLM-L6-v2` (default) | Other SentenceTransformer models | `text-embedding-3-small` (OpenAI) |\n",
    "|---|---|---|---|\n",
    "| **Dimensions** | 384 | 384â€“1024 (model-dependent) | 1024 (toolkit setting) |\n",
    "| **Context limit** | 256 tokens | 256â€“8 192 (model-dependent) | 8 191 tokens |\n",
    "| **Cost** | Free, local | Free, local | ~$0.02 / 1 M tokens |\n",
    "| **Data security** | Fully local | Fully local | Sent to OpenAI |\n",
    "| **Quality** | Good for short, focused text | Varies; some match OpenAI | State-of-the-art |\n",
    "| **Setup** | No API key | No API key | `OPENAI_API_KEY` required |\n",
    "\n",
    "**SentenceTransformer: free alternatives from HuggingFace**\n",
    "Any model from HuggingFace that is compatible with the `sentence-transformers` library works with our `SentenceTransformerEmbeddings` by passing a different `model_name`. Browse quality rankings on the [MTEB leaderboard](https://huggingface.co/spaces/mteb/leaderboard).\n",
    "\n",
    "Hardware matters: larger models are slower on CPU. A useful rule of thumb: models under ~150 MB run comfortably on CPU; larger models benefit from a GPU.\n",
    "\n",
    "| Model | Size | CPU speed | Max input tokens (truncation limit) | Notes |\n",
    "|---|---|---|---|---|\n",
    "| `all-MiniLM-L6-v2` | 90 MB | Very fast | 256 | Default; good for short technical text |\n",
    "| `sentence-transformers/all-mpnet-base-v2` | 420 MB | Moderate | 384 | Better English quality, same 512-token limit |\n",
    "| `BAAI/bge-m3` | 2.3 GB | Very slow, GPU recommended | 8192 | Best multilingual quality; 8192-token limit |\n",
    "| `thenlper/gte-large` | 1.3 GB | Slow, GPU recommended | 512 | Strong English quality |\n",
    "\n",
    "> On Renku (CPU-only sessions), the top two rows are practical choices. \n",
    "\n",
    "**OpenAI embeddings**\n",
    "`OpenAIEmbeddings` from `conversational_toolkit.embeddings.openai` calls the OpenAI API. The toolkit requests 1024 dimensions using OpenAI's Matryoshka dimension reduction, a technique that allows truncating full embeddings to a smaller size with minimal quality loss. Requires `OPENAI_API_KEY`.\n",
    "\n",
    "```python\n",
    "from conversational_toolkit.embeddings.openai import OpenAIEmbeddings\n",
    "embedding_model = OpenAIEmbeddings(model_name=\"text-embedding-3-small\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c159c2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Vector Store\n",
    "\n",
    "A vector store persists chunk embeddings on disk and provides approximate nearest-neighbour search. Two implementations are in the toolkit.\n",
    "\n",
    "**`ChromaDBVectorStore`** (used in this notebook)\n",
    "- Embedded database: no separate server process, data stored as files on disk at `VS_PATH`.\n",
    "- Survives session restarts, which is why `reset=False` is safe by default.\n",
    "- Uses L2 distance for search. For unit-length vectors (which both embedding models produce) this gives the same ranking as cosine similarity -> different numbers, identical top-k order.\n",
    "- Well-suited for corpora up to ~100 k chunks. Not designed for concurrent writes or multi-user access.\n",
    "\n",
    "**`PGVectorStore`** (also in the toolkit)\n",
    "- PostgreSQL with the `pgvector` extension. Requires a running Postgres instance.\n",
    "- Uses cosine similarity natively (also supports other).\n",
    "- Supports rich metadata filtering, concurrent reads and writes, and standard SQL queries alongside vector search.\n",
    "- The right choice when you already have a Postgres infrastructure, need concurrent access, or want to combine vector search with relational data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "p0-step2-code",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-22 13:23:56.861 | DEBUG    | conversational_toolkit.embeddings.sentence_transformer:__init__:57 - Sentence Transformer embeddings model loaded: sentence-transformers/all-MiniLM-L6-v2 with kwargs: {}\n",
      "2026-02-22 13:23:56.869 | INFO     | sme_kt_zh_collaboration_rag.feature0_baseline_rag:build_vector_store:260 - Vector store already contains 78 chunks â€” skipping embedding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding model: sentence-transformers/all-MiniLM-L6-v2\n",
      "Vector store ready.\n"
     ]
    }
   ],
   "source": [
    "embedding_model = SentenceTransformerEmbeddings(model_name=EMBEDDING_MODEL)\n",
    "print(f\"Embedding model: {EMBEDDING_MODEL}\")\n",
    "\n",
    "# Set reset=True to rebuild the store from scratch\n",
    "vector_store = await build_vector_store(\n",
    "    chunks, embedding_model, db_path=VS_PATH, reset=False\n",
    ")\n",
    "print(\"Vector store ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p0-step2-embed-md",
   "metadata": {},
   "source": [
    "### Similarity in Embedding Space\n",
    "\n",
    "Embeddings that are close in vector space share semantic meaning. The cell below embeds several sentences and measures their cosine similarity: a value between -1 (opposite) and 1 (identical). You can change the sentences to see the impact on cosine similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "p0-step2-embed-code",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-22 13:24:04.480 | DEBUG    | conversational_toolkit.embeddings.sentence_transformer:get_embeddings:76 - sentence-transformers/all-MiniLM-L6-v2 embeddings size: (2, 384)\n",
      "2026-02-22 13:24:04.492 | DEBUG    | conversational_toolkit.embeddings.sentence_transformer:get_embeddings:76 - sentence-transformers/all-MiniLM-L6-v2 embeddings size: (2, 384)\n",
      "2026-02-22 13:24:04.502 | DEBUG    | conversational_toolkit.embeddings.sentence_transformer:get_embeddings:76 - sentence-transformers/all-MiniLM-L6-v2 embeddings size: (2, 384)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarities:\n",
      "0.133  -->  'carbon footprint of a pallet'  vs  'GWP value for the Logypal 1'\n",
      "-0.064  -->  'carbon footprint of a pallet'  vs  'PFAS-free tape declaration'\n",
      "0.014  -->  'carbon footprint of a pallet'  vs  'the annual report of a software firm'\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "sentence1 = \"carbon footprint of a pallet\"\n",
    "sentence2 = \"GWP value for the Logypal 1\"\n",
    "sentence3 = \"PFAS-free tape declaration\"\n",
    "sentence4 = \"the annual report of a software firm\"\n",
    "\n",
    "\n",
    "async def cosine_similarity(a: str, b: str) -> float:\n",
    "    vecs = await embedding_model.get_embeddings([a, b])\n",
    "    return float(\n",
    "        np.dot(vecs[0], vecs[1]) / (np.linalg.norm(vecs[0]) * np.linalg.norm(vecs[1]))\n",
    "    )\n",
    "\n",
    "\n",
    "pairs = [\n",
    "    (sentence1, sentence2),\n",
    "    (sentence1, sentence3),\n",
    "    (sentence1, sentence4),\n",
    "]\n",
    "\n",
    "print(\"Cosine similarities:\")\n",
    "for a, b in pairs:\n",
    "    sim = await cosine_similarity(a, b)\n",
    "    print(f\"{sim:.3f}  -->  {a!r}  vs  {b!r}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aqsb3dt213",
   "metadata": {},
   "source": [
    "### Comparing Embedding Models on Your Documents\n",
    "\n",
    "A quick way to compare models without running a full evaluation: pick a query, a relevant chunk, and an irrelevant chunk, then measure the **cosine similarity gap** -> how much more similar the relevant chunk is to the query than the irrelevant one. A larger gap means the model discriminates better between useful and noise results, which translates directly to higher retrieval precision.\n",
    "\n",
    "The cell below runs this for three CPU-friendly models (and OpenAI if the key is set). The HuggingFace models are downloaded on first use (~400 MB each, takes a minute)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b45sgmln8q9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-22 13:28:04.896 | DEBUG    | conversational_toolkit.embeddings.sentence_transformer:__init__:57 - Sentence Transformer embeddings model loaded: all-MiniLM-L6-v2 with kwargs: {}\n",
      "2026-02-22 13:28:07.350 | DEBUG    | conversational_toolkit.embeddings.sentence_transformer:__init__:57 - Sentence Transformer embeddings model loaded: sentence-transformers/all-mpnet-base-v2 with kwargs: {}\n",
      "2026-02-22 13:28:10.111 | DEBUG    | conversational_toolkit.embeddings.sentence_transformer:__init__:57 - Sentence Transformer embeddings model loaded: sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2 with kwargs: {}\n",
      "2026-02-22 13:28:10.141 | DEBUG    | conversational_toolkit.embeddings.openai:__init__:20 - OpenAI embeddings model loaded: text-embedding-3-small\n",
      "2026-02-22 13:28:10.278 | DEBUG    | conversational_toolkit.embeddings.sentence_transformer:get_embeddings:76 - all-MiniLM-L6-v2 embeddings size: (3, 384)\n",
      "2026-02-22 13:28:10.324 | DEBUG    | conversational_toolkit.embeddings.sentence_transformer:get_embeddings:76 - sentence-transformers/all-mpnet-base-v2 embeddings size: (3, 768)\n",
      "2026-02-22 13:28:10.336 | DEBUG    | conversational_toolkit.embeddings.sentence_transformer:get_embeddings:76 - sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2 embeddings size: (3, 384)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query     : 'What is the carbon footprint of the Logypal 1 pallet?'\n",
      "Relevant  : 'Logypal 1 â€” GWP: 3.2 kg CO2e per functional unit (A1-A3). Figure verif'...\n",
      "Irrelevant: 'PrimePack AG Supplier Code of Conduct. All suppliers must comply with '...\n",
      "\n",
      "Model                                         sim(relevant)  sim(irrelevant)     gap\n",
      "------------------------------------------------------------------------------------\n",
      "all-MiniLM-L6-v2        (90 MB, 384-dim)              0.573            0.065   0.509\n",
      "all-mpnet-base-v2       (420 MB, 768-dim)             0.582            0.287   0.295\n",
      "multilingual-MiniLM-L12 (470 MB, 384-dim)             0.675            0.166   0.509\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-22 13:28:10.521 | INFO     | conversational_toolkit.embeddings.openai:get_embeddings:38 - OpenAI embeddings shape: (3, 1024)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text-embedding-3-small  (API, 1024-dim)               0.597            0.295   0.302\n",
      "\n",
      "Gap = sim(relevant) - sim(irrelevant). Larger gap -> better discrimination.\n",
      "This is a quick proxy for retrieval quality. Feature Track 2 runs a proper evaluation.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from conversational_toolkit.embeddings.sentence_transformer import (\n",
    "    SentenceTransformerEmbeddings,\n",
    ")\n",
    "from conversational_toolkit.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "\n",
    "def cosine_sim(a: np.ndarray, b: np.ndarray) -> float:\n",
    "    return float(np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b)))\n",
    "\n",
    "\n",
    "# Test query and two chunks from the corpus\n",
    "query = \"What is the carbon footprint of the Logypal 1 pallet?\"\n",
    "\n",
    "chunk_relevant = \"Logypal 1 â€” GWP: 3.2 kg CO2e per functional unit (A1-A3). Figure verified by independent third-party auditor under ISO 14044.\"\n",
    "chunk_irrelevant = \"PrimePack AG Supplier Code of Conduct. All suppliers must comply with applicable environmental regulations and report annually on progress.\"\n",
    "\n",
    "# Models to compare (CPU-friendly by default)\n",
    "models_to_compare: dict = {\n",
    "    \"all-MiniLM-L6-v2        (90 MB, 384-dim)\": SentenceTransformerEmbeddings(\n",
    "        \"all-MiniLM-L6-v2\"\n",
    "    ),\n",
    "    \"all-mpnet-base-v2       (420 MB, 768-dim)\": SentenceTransformerEmbeddings(\n",
    "        \"sentence-transformers/all-mpnet-base-v2\"\n",
    "    ),\n",
    "    \"multilingual-MiniLM-L12 (470 MB, 384-dim)\": SentenceTransformerEmbeddings(\n",
    "        \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
    "    ),\n",
    "}\n",
    "# Add OpenAI if key is available\n",
    "if os.getenv(\"OPENAI_API_KEY\"):\n",
    "    models_to_compare[\"text-embedding-3-small  (API, 1024-dim)\"] = OpenAIEmbeddings(\n",
    "        \"text-embedding-3-small\"\n",
    "    )\n",
    "\n",
    "# Run comparison\n",
    "print(\"---------------------------\")\n",
    "print(f\"Query     : {query!r}\")\n",
    "print(f\"Relevant  : {chunk_relevant[:70]!r}...\")\n",
    "print(f\"Irrelevant: {chunk_irrelevant[:70]!r}...\")\n",
    "print()\n",
    "print(f\"{'Model':<44}  {'sim(relevant)':>13}  {'sim(irrelevant)':>15}  {'gap':>6}\")\n",
    "print(\"-\" * 84)\n",
    "\n",
    "for name, model in models_to_compare.items():\n",
    "    vecs = await model.get_embeddings([query, chunk_relevant, chunk_irrelevant])\n",
    "    sim_rel = cosine_sim(vecs[0], vecs[1])\n",
    "    sim_irr = cosine_sim(vecs[0], vecs[2])\n",
    "    print(f\"{name:<44}  {sim_rel:>13.3f}  {sim_irr:>15.3f}  {sim_rel - sim_irr:>6.3f}\")\n",
    "\n",
    "print(\"\\nGap = sim(relevant) - sim(irrelevant). Larger gap -> better discrimination.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7624e3",
   "metadata": {},
   "source": [
    "**Disclaimer**: This is a quick example, not a rigorous evaluation. A single (query, chunk) pair can be misleading â€” one model may score higher here and worse on a different example. Reliable model selection requires testing across many diverse queries and aggregating a metric like MRR or NDCG. Feature Track 2 shows how to do this systematically.\n",
    "\n",
    "> Task: Probe the comparison with your own examples\n",
    "> 1. Replace query, chunk_relevant, and chunk_irrelevant with other combinations \n",
    "> 2. Try a query where your phrasing differs (e.g. \"carbon emissions\" instead of \"GWP\"). Does any model bridge the gap better?\n",
    "> 3. Try a query in German. Does paraphrase-multilingual-MiniLM-L12-v2 show a larger gap than the English-only default?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p0-step3-md",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 3: Inspect Retrieval (Before the LLM Sees Anything)\n",
    "\n",
    "This is the **most important diagnostic step** in the whole pipeline:\n",
    "\n",
    "> If the retrieved chunks are wrong, the final answer will be wrong regardless of how good the LLM is.\n",
    "\n",
    "`inspect_retrieval()` runs the query through the embedding model, fetches the top-k most similar chunks from ChromaDB, and prints them with scores. Use this to verify that relevant documents are in the index, tune `top_k`, compare different query phrasings, and identify retrieval gaps before blaming the LLM.\n",
    "\n",
    "The **similarity score** is the L2 distance, range [0,4], lower = more similar. L2 distance is used becuase it works for any vectors, normalised or not. Cosine similarity only makes sense for direction (magnitude doesn't matter), so it requires that vectors be unit-length to be meaningful. L2 makes no such assumption, making it the safer general default. ChromaDB defaults to L2 because it's simpler to compute and works even if vector magnitudes vary. Since our embedding model always produces equal-length vectors, we get cosine-equivalent ranking. The score numbers look different, but the top-5 results would be identical either way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "p0-step3-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY = \"What materials is the Logypal 1 pallet made from?\"\n",
    "\n",
    "results = await inspect_retrieval(QUERY, vector_store, embedding_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p0-step3-fail-md",
   "metadata": {},
   "source": [
    "### Retrieval for a Product Outside the Portfolio\n",
    "\n",
    "The PrimePack AG product catalog defines the portfolio boundary. The **Lara Pallet** is not in the catalog, it does not exist. Watch which chunks are returned and what scores they have. A **higher** minimum score (large L2 distance) signals *weaker semantic match*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "p0-step3-fail-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_OOK = \"What materials is the Lara pallet made from?\"\n",
    "\n",
    "results_ook = await inspect_retrieval(QUERY_OOK, vector_store, embedding_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p0-step3-fail-obs-md",
   "metadata": {},
   "source": [
    "> **Observation:** The retriever always returns the *closest* chunks it can find, it has no concept of \"no match\". For an unknown product the L2 distances are **higher** (the closest chunks are still about other pallets), but without a score-threshold guard the LLM receives those chunks anyway and may silently answer about the wrong product.\n",
    "> **Feature Track 3** shows how to combat this issue."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p0-step4-md",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 4: Build the RAG Agent\n",
    "\n",
    "`build_agent()` assembles the three components:\n",
    "\n",
    "```\n",
    "VectorStoreRetriever\n",
    "    â””â”€ ChromaDBVectorStore (on disk, persists across runs)\n",
    "    â””â”€ SentenceTransformerEmbeddings\n",
    "\n",
    "RAG Agent\n",
    "    â”œâ”€ LLM (Ollama / OpenAI)\n",
    "    â”œâ”€ Retriever\n",
    "    â””â”€ System prompt\n",
    "```\n",
    "\n",
    "### The System Prompt\n",
    "\n",
    "The system prompt is the primary lever for controlling LLM behaviour. It is prepended to every conversation and defines the rules the model must follow:\n",
    "\n",
    "```\n",
    "You are a helpful AI assistant specialised in sustainability and product compliance\n",
    "for PrimePack AG.\n",
    "\n",
    "You will receive document excerpts relevant to the user's question.\n",
    "Produce the best possible answer using only the information in those excerpts.\n",
    "\n",
    "Rules:\n",
    "- Use the provided excerpts as your only source of truth. Do not rely on outside knowledge.\n",
    "- Use all relevant excerpts when forming your answer.\n",
    "- If the answer cannot be found in the excerpts, clearly say that you do not know.\n",
    "- Always cite the source document for any claim you make.\n",
    "- If excerpts contain conflicting information, report both values and flag the conflict.\n",
    "- Distinguish between third-party verified claims (EPDs) and self-declared supplier claims.\n",
    "```\n",
    "\n",
    "Each rule addresses a concrete failure mode:\n",
    "\n",
    "| Rule | Failure mode it prevents |\n",
    "|---|---|\n",
    "| Only use provided excerpts | Hallucination from general knowledge |\n",
    "| Say \"I don't know\" if not found | Confident wrong answer about missing products |\n",
    "| Cite the source document | Untraceable claims |\n",
    "| Flag conflicting information | Silently picking the wrong figure when sources disagree |\n",
    "| Distinguish EPD vs. self-declared | Presenting unverified marketing claims as verified facts |\n",
    "\n",
    "> **Note:** Stronger models (especially GPT-4o) follow these rules reliably even with a minimal prompt. Smaller local models (Ollama `mistral-nemo:12b`) benefit significantly from explicit, detailed rules. Section 4 probes these failure modes â€” if you are using OpenAI you may need to use a weaker model or a more leading query to observe them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "p0-step4-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = build_llm(backend=BACKEND)\n",
    "agent = build_agent(\n",
    "    vector_store=vector_store,\n",
    "    embedding_model=embedding_model,\n",
    "    llm=llm,\n",
    "    top_k=RETRIEVER_TOP_K,\n",
    "    number_query_expansion=0,  # 0 = no expansion; see Feature Track 4 for more\n",
    ")\n",
    "print(\"RAG agent assembled.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p0-step5-md",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 5: Ask a Question\n",
    "\n",
    "`ask()` sends the query to the agent and returns the answer string. The internal flow is:\n",
    "\n",
    "1. Embed the query\n",
    "2. Retrieve top-k chunks\n",
    "3. Build the prompt: `<system>` + `<sources>` XML block + user question\n",
    "4. Generate the answer with the LLM\n",
    "5. Return the answer and a list of cited source chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "p0-step5-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY = \"What materials is the Logypal 1 pallet made from?\"\n",
    "\n",
    "answer = await ask(agent, QUERY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fx3o0zxxg5v",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 5b: Query Expansion\n",
    "\n",
    "A single query embedding is a single point in vector space. If the user's phrasing differs from the document's phrasing, the best matching chunk may not rank in the top-k.\n",
    "\n",
    "**Query expansion** generates several alternative formulations of the same question, retrieves independently for each, and merges the results. The agent already supports this via the `number_query_expansion` parameter.\n",
    "\n",
    "```\n",
    "Original query: \"What is the carbon footprint of the Logypal 1?\"\n",
    "        â”‚\n",
    "        â–¼  LLM generates N variants\n",
    "        â”œâ”€â”€ \"GWP of the Logypal 1 pallet in kg COâ‚‚e\"\n",
    "        â”œâ”€â”€ \"Environmental impact, global warming potential, Logypal 1\"\n",
    "        â””â”€â”€ \"Relicyc pallet carbon emissions A1-A3\"\n",
    "        â”‚\n",
    "        â–¼  Retrieve top-k for each variant â†’ deduplicate â†’ merge\n",
    "        â””â”€â”€ Combined result set â†’ LLM generates final answer\n",
    "```\n",
    "\n",
    "The cost is one extra LLM call per expansion (to generate the variants), but retrieval recall typically improves substantially for domain-specific vocabulary differences.\n",
    "\n",
    "> **Feature Track 4** explores this and other advanced retrieval strategies (hybrid search, re-ranking) in depth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08vv0bu8v0sm",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare retrieval with and without query expansion\n",
    "\n",
    "QUERY_EXPANSION = \"What is the carbon footprint of the Logypal 1?\"\n",
    "\n",
    "# Without expansion â€” single query, top-5 chunks\n",
    "results_no_exp = await inspect_retrieval(QUERY_EXPANSION, vector_store, embedding_model)\n",
    "\n",
    "print(\"\\n--- With 3 expanded queries ---\\n\")\n",
    "\n",
    "# With expansion â€” 3 variants generated, deduplicated results\n",
    "agent_with_expansion = build_agent(\n",
    "    vector_store=vector_store,\n",
    "    embedding_model=embedding_model,\n",
    "    llm=llm,\n",
    "    top_k=RETRIEVER_TOP_K,\n",
    "    number_query_expansion=3,\n",
    ")\n",
    "answer_expanded = await ask(agent_with_expansion, QUERY_EXPANSION)\n",
    "\n",
    "print(f\"\\nChunks retrieved without expansion : {len(results_no_exp)}\")\n",
    "print(\"(with expansion the agent internally deduplicates across all variant queries)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p0-queries-md",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Probing Failure Modes\n",
    "\n",
    "The corpus was designed with three deliberate challenges. Run the queries below and observe the answers.\n",
    "\n",
    "### 4a: Out-of-Portfolio Query\n",
    "\n",
    "The **Lara Pallet** does not exist. A good RAG must say so instead of describing a different pallet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "p0-query-ook-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_ook = await ask(agent, \"What materials is the Lara pallet made from?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p0-query-gap-md",
   "metadata": {},
   "source": [
    "### 4b: Missing Data (LogyLight Pallet)\n",
    "\n",
    "The LogyLight datasheet marks all LCA fields as *\"not yet available\"*. The correct answer is that we don't have the data, not a fabricated figure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "p0-query-gap-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_gap = await ask(agent, \"What is the GWP of the LogyLight pallet?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p0-query-conflict-md",
   "metadata": {},
   "source": [
    "### 4c: Conflicting Evidence (Relicyc GWP Figures)\n",
    "\n",
    "The 2021 Relicyc datasheet reports **4.1 kg COâ‚‚e** per pallet. The 2023 EPD (third-party verified) reports a different, more recent figure. The RAG should flag the conflict and prefer the verified, more recent source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "p0-query-conflict-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_conflict = await ask(\n",
    "    agent, \"What is the GWP of the Logypal 1 pallet, and how reliable is the figure?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p0-query-unverified-md",
   "metadata": {},
   "source": [
    "### 4d: Unverified Supplier Claim (Tesa ECO Tape)\n",
    "\n",
    "The tesa supplier brochure claims **68% COâ‚‚ reduction** compared to conventional tape. This is a self-declared marketing claim, there is no independent EPD. The RAG should report the claim but flag that it is unverified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "p0-query-unverified-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_claim = await ask(\n",
    "    agent,\n",
    "    \"How much lower is the carbon footprint of tesa ECO tape compared to standard tape?\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5948f40",
   "metadata": {},
   "source": [
    "> ðŸ’¬ **Discuss with your group:** Which of the four failure modes above concerns you most in a real deployment? What would the consequence be if a colleague trusted a wrong answer about a sustainability claim?\n",
    ">\n",
    "> Can you think of other ways the system might fail that aren't shown here?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p0-multiturn-md",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Multi-Turn Conversation\n",
    "\n",
    "The `ask()` function accepts a `history` argument, a list of prior `LLMMessage` objects. When history is provided the agent first **rewrites the query** to be self-contained (*\"it\"* becomes the actual product name) before retrieval.\n",
    "\n",
    "This prevents the retriever from embedding vague pronouns that match nothing in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "p0-multiturn-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "from conversational_toolkit.llms.base import LLMMessage, Roles\n",
    "\n",
    "history: list[LLMMessage] = []\n",
    "\n",
    "\n",
    "async def conversation_turn(query: str) -> str:\n",
    "    global history\n",
    "    answer = await agent.answer(QueryWithContext(query=query, history=history))\n",
    "    history.append(LLMMessage(role=Roles.USER, content=query))\n",
    "    history.append(LLMMessage(role=Roles.ASSISTANT, content=answer.content))\n",
    "    return answer.content\n",
    "\n",
    "\n",
    "# Turn 1: ask about a specific product\n",
    "reply1 = await conversation_turn(\n",
    "    \"Which pallets in our portfolio have a third-party verified EPD?\"\n",
    ")\n",
    "print(\"User: Which pallets in our portfolio have a third-party verified EPD?\")\n",
    "print(f\"Assistant: {reply1}\\n\")\n",
    "\n",
    "# Turn 2: follow-up using a pronoun â€” the agent should resolve \"it\" before retrieval\n",
    "reply2 = await conversation_turn(\n",
    "    \"What is the GWP figure reported in it for the Logypal 1?\"\n",
    ")\n",
    "print(\"User: What is the GWP figure reported in it for the Logypal 1?\")\n",
    "print(f\"Assistant: {reply2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p0-arch-md",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Running the Full Pipeline in One Call\n",
    "\n",
    "The `run_pipeline()` convenience function executes all five steps end-to-end. It is also\n",
    "what the `__main__` entry point calls.\n",
    "\n",
    "Use it for quick one-shot queries. Use the individual step functions above when you need\n",
    "to inspect intermediate results or iterate on a specific stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "p0-arch-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sme_kt_zh_collaboration_rag.feature0_baseline_rag import run_pipeline\n",
    "\n",
    "answer = await run_pipeline(\n",
    "    backend=BACKEND,\n",
    "    query=\"What sustainability certifications do the pallets in the portfolio have?\",\n",
    "    reset_vs=False,\n",
    ")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p0-backends-md",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Switching LLM Backends\n",
    "\n",
    "The pipeline abstracts the LLM behind a common interface. Only `build_llm()` needs to change.\n",
    "\n",
    "| Backend | `BACKEND=` | Prerequisite |\n",
    "|---|---|---|\n",
    "| Ollama (local) | `\"ollama\"` | `ollama serve` + `ollama pull mistral-nemo:12b` |\n",
    "| OpenAI | `\"openai\"` | `OPENAI_API_KEY` env variable |\n",
    "\n",
    "You can also override the model within a backend:\n",
    "\n",
    "```python\n",
    "llm = build_llm(backend=\"openai\", model_name=\"gpt-4o\") # stronger model\n",
    "llm = build_llm(backend=\"ollama\", model_name=\"llama3.2\") # smaller local model\n",
    "```\n",
    "\n",
    "The RAG pipeline is **backend-agnostic**, the retrieval step is identical regardless of which LLM is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "p0-backends-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test openai\n",
    "\n",
    "QUERY = \"What materials is the Lara pallet made from?\"\n",
    "\n",
    "llm_openai = build_llm(backend=\"openai\", model_name=\"gpt-4o-mini\")\n",
    "agent_openai = build_agent(vector_store, embedding_model, llm_openai)\n",
    "answer_openai = await ask(agent_openai, QUERY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p0-exercises-md",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Tasks & Discussion\n",
    "\n",
    "Work through these as a group. You don't need to do them all â€” pick what interests you or what matches your background. Mix and match freely.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ’¬ Explore & Discuss\n",
    "\n",
    "**A. Build a test set together**\n",
    "Go through the data files (run the scratch cell below to see what's there) and write down five questions where the answer *is* in the documents, two questions about products that don't exist or data that isn't there, and one question where you suspect conflicting information. Run them through the system. Does it pass your test?\n",
    "\n",
    "**B. Evaluate the outputs â€” would you trust them?**\n",
    "Look back at the failure mode answers in Section 4. For each one: would you have trusted this answer without knowing it might be wrong? What would a *good* response look like? What would need to change in the system to prevent this failure?\n",
    "\n",
    "**C. Think about your own context**\n",
    "If you were to deploy this in your organisation, what documents would go into the knowledge base? What questions should it answer well? Who would use it, and what are the consequences of a wrong answer?\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”§ Code Experiments\n",
    "\n",
    "**1. Retrieval inspection**\n",
    "Call `inspect_retrieval()` with several different queries. Look at the L2 scores â€” at roughly what value do retrieved chunks stop being relevant? Could you use this as a confidence threshold?\n",
    "\n",
    "**2. Top-k sensitivity**\n",
    "Change `top_k` from 5 to 1 and re-run a query from the failure mode section. Does the answer change? Try 10. Is more context always better, or does noise creep in?\n",
    "\n",
    "**3. System prompt ablation**\n",
    "In `feature0_baseline_rag.py`, find `SYSTEM_PROMPT`. Remove the rule about flagging conflicting information. Rebuild the agent and re-run the Relicyc GWP query (Section 4c). Does the answer change?\n",
    "\n",
    "**4. Query phrasing experiment**\n",
    "Try these three phrasings for the same underlying question: `\"COâ‚‚ footprint Logypal 1\"`, `\"carbon emissions recycled pallet\"`, and `\"GWP A1-A3 EPD pallet\"`. Do the retrieved chunks differ? Which phrasing finds the most relevant result?\n",
    "\n",
    "**5. Query expansion**\n",
    "Set `number_query_expansion=3` (Step 5b) and repeat Task 4. Does expansion reduce sensitivity to phrasing?\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸš€ Possible Extensions\n",
    "\n",
    "| Extension | Effort | What it enables |\n",
    "|---|---|---|\n",
    "| Load the full corpus (`max_files=None`) | Minimal | More documents, better coverage |\n",
    "| Try `gpt-4o` instead of `gpt-4o-mini` | Minimal | Stronger failure mode handling |\n",
    "| Add a new document to `data/` | Easy | Extend the knowledge base with your own materials |\n",
    "| Multi-lingual queries | Easy to test | Ask questions in German / French |\n",
    "| Score threshold filter | Medium | Detect when a query falls outside the known portfolio |\n",
    "| Structured outputs (Feature Track 3) | Medium | Every answer includes: product, figure, source, verified? |\n",
    "| Query expansion + re-ranking (Feature Track 4) | Medium | Better recall for domain-specific vocabulary |\n",
    "| Chat frontend (Feature Track 6) | Medium | Share the prototype with colleagues via a browser interface |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "p0-exercises-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scratch cell â€” run your experiments here\n",
    "async def quick_retrieve(query: str, top_k: int = 5):\n",
    "    retriever = VectorStoreRetriever(embedding_model, vector_store, top_k=top_k)\n",
    "    results = await retriever.retrieve(query)\n",
    "    print(f\"Query: {query!r}  (top_k={top_k})\")\n",
    "    for r in results:\n",
    "        src = r.metadata.get(\"source_file\", \"?\")\n",
    "        print(f\"  score={r.score:.4f}  {src}  {r.title!r}\")\n",
    "\n",
    "\n",
    "await quick_retrieve(\"PFAS-free tape declaration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p0-summary-md",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "| Step | Function |\n",
    "|---|---|\n",
    "| 1. Load & chunk | `load_chunks(max_files)` |\n",
    "| 2. Embed & index | `build_vector_store(chunks, emb, reset)` |\n",
    "| 3. Inspect retrieval | `inspect_retrieval(query, vs, emb)` |\n",
    "| 4. Build agent | `build_agent(vs, emb, llm, top_k, number_query_expansion)` |\n",
    "| 5. Generate answer | `ask(agent, query, history)` |\n",
    "\n",
    "### Three Core Failure Modes\n",
    "\n",
    "| Failure mode | Symptom | Root cause | Addressed in |\n",
    "|---|---|---|---|\n",
    "| **Wrong entity** | RAG answers about the wrong product | Retriever returns high-scoring chunks for a product that doesn't exist in corpus; LLM picks the closest match | Feature Track 3 (structured output + confidence filtering) |\n",
    "| **Missing data** | LLM invents a figure that isn't in any document | Retriever returns vaguely related chunks; LLM fills the gap with general knowledge | Feature Track 3 (strict source-only rules) |\n",
    "| **Low recall** | Correct chunk exists but isn't retrieved | Query phrasing doesn't match document vocabulary; top-k too small | Feature Track 4 (query expansion, hybrid search) |\n",
    "\n",
    "> **With OpenAI models these failures are often subtle** â€” GPT-4o follows the \"say I don't know\" rule reliably. They become more pronounced with local models (Ollama) or with adversarial query phrasing. The experiments in the Tasks section above give guidance on how to observe them.\n",
    "\n",
    "**Next â€” Feature Track 1:** Explore chunking strategies and understand how chunk size and strategy affect retrieval quality."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_venv (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
