{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "p0-title-md",
   "metadata": {},
   "source": [
    "# The Baseline RAG Pipeline\n",
    "\n",
    "---\n",
    "\n",
    "This notebook is your **starting point**. The pipeline is already built and working â€” run it, explore its outputs, question it, and find its limits.\n",
    "\n",
    "**How to use this notebook**\n",
    "\n",
    "| | |\n",
    "|---|---|\n",
    "| ðŸ“– **Read** | The explanations use plain language â€” no coding background needed |\n",
    "| â–¶ï¸ **Run** | Execute cells top to bottom with Shift+Enter to see the pipeline in action |\n",
    "| ðŸ’¬ **Discuss** | Talk about the outputs with your group â€” do they make sense? Would you trust them? |\n",
    "| ðŸ”§ **Experiment** | Modify queries, tweak parameters, break things on purpose |\n",
    "| ðŸš€ **Extend** | The Tasks section points to what you can take further |\n",
    "\n",
    "> You don't need to understand every line of code. Focus on what the system gets right and wrong â€” and on thinking about how this would apply in your own context.\n",
    "\n",
    "---\n",
    "\n",
    "## What This Notebook Covers\n",
    "\n",
    "**Retrieval-Augmented Generation (RAG)** combines a search engine with an AI assistant. Instead of the AI making things up from memory, it first searches your documents and then answers based on what it finds. The answer can always be traced back to a source.\n",
    "\n",
    "```\n",
    "Your question  ->  Search your documents  ->  AI answers using only those documents\n",
    "```\n",
    "\n",
    "| Notebook | Focus |\n",
    "|---|---|\n",
    "| **Baseline (this notebook)** | Working prototype â€” start here |\n",
    "| Feature Track 1 | How documents are split into searchable pieces |\n",
    "| Feature Track 2 | How to measure answer quality |\n",
    "| Feature Track 3 | Reliable, structured outputs |\n",
    "| Feature Track 4 | Better retrieval strategies |\n",
    "| Feature Track 5 | Multi-step agent workflows |\n",
    "| Feature Track 6 | Connecting to the chat frontend |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p0-why-rag-md",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Why RAG? The Problem with a Standalone LLM\n",
    "\n",
    "### The Scenario\n",
    "**PrimePack AG** buys packaging materials (pallets, cardboard boxes, tape) from multiple suppliers. Sustainability claims are increasingly scrutinised by customers and regulators. Employees need to answer questions like:\n",
    "> *\"What is the GWP of the Logypal 1 pallet, and is the figure verified?\"*  \n",
    "> *\"Can we tell a customer that the tesa tape is PFAS-free?\"*  \n",
    "> *\"Which of our suppliers have a certified EPD?\"*\n",
    "\n",
    "### Why Not Just Ask ChatGPT?\n",
    "A general-purpose LLM has three fundamental problems for this task:\n",
    "\n",
    "| Problem | Why It Matters |\n",
    "|---|---|\n",
    "| **No product knowledge** | LLMs know nothing about Logypal 1, PrimePack's specific portfolio, or the individual supplier documents. |\n",
    "| **Hallucination** | When asked about unknown products the LLM invents plausible-sounding but false figures. |\n",
    "| **No evidence trail** | Even when correct, a raw LLM answer cannot be traced back to a source document. |\n",
    "\n",
    "### The RAG Solution\n",
    "RAG adds a **retrieval step** between the user's question and the LLM:\n",
    "\n",
    "```\n",
    " Documents â”€â”€â–º Chunker â”€â”€â–º Embedder â”€â”€â–º Vector DB\n",
    "                                              â”‚\n",
    " User query â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º Embedder â”€â”€â”€â”€â”€â–º  Retriever â”€â”€â–º Top-k Chunks\n",
    "                                                                      â”‚\n",
    "                                                               LLM + Prompt\n",
    "                                                                      â”‚\n",
    "                                                               Answer + Sources\n",
    "```\n",
    "\n",
    "The LLM only sees documents that are **actually in the corpus**. The answer can be traced to specific source chunks. If the corpus does not contain the answer, the LLM is instructed to say so.\n",
    "\n",
    "### What RAG Does *Not* Fix\n",
    "RAG shifts the problem from hallucination to **retrieval quality**. If the right chunk is not retrieved, the answer will still be wrong (or absent). The later feature tracks address exactly this: better chunking, better retrieval, and better output structure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p0-concepts-md",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Core Concepts\n",
    "\n",
    "### Chunks\n",
    "\n",
    "A **chunk** is a short excerpt from a source document, a section of a PDF, one sheet of a spreadsheet, or one heading-delimited paragraph of a Markdown file. Chunks are the unit of indexing and retrieval.\n",
    "\n",
    "```python\n",
    "@dataclass\n",
    "class Chunk:\n",
    "    id: str           # unique identifier\n",
    "    title: str        # e.g. section heading\n",
    "    content: str      # the text that gets embedded\n",
    "    metadata: dict    # source_file, page, ...\n",
    "```\n",
    "\n",
    "### Embeddings\n",
    "An **embedding** converts text to a dense numeric vector (e.g. 384 dimensions). Semantically similar texts produce similar vectors. Here we use `all-MiniLM-L6-v2`, a compact local model that runs without an API key.\n",
    "\n",
    "### Vector Store (ChromaDB)\n",
    "A **vector store** persists chunk embeddings on disk and supports approximate nearest-neighbour search. Given a query embedding, it returns the `top_k` most similar chunks in milliseconds.\n",
    "\n",
    "### Retriever\n",
    "A **retriever** wraps a vector store and exposes a single `retrieve(query)` method. The baseline uses a `VectorStoreRetriever` with `top_k=5`.\n",
    "\n",
    "### RAG Agent\n",
    "The **RAG agent** combines a retriever and an LLM. Its `answer()` method:\n",
    "1. Embeds the query\n",
    "2. Retrieves the top-k chunks\n",
    "3. Formats chunks as XML `<source>` tags in the prompt\n",
    "4. Calls the LLM and returns the answer + cited sources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p0-setup-md",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Setup\n",
    "\n",
    "**Prerequisites:** `conversational-toolkit` and `backend` must be installed in editable mode (`pip install -e conversational-toolkit/ && pip install -e backend/`), already done on Renku. For the **Ollama** backend, start `ollama serve` and pull the model (`ollama pull mistral-nemo:12b`). For **OpenAI**, set `OPENAI_API_KEY` in your environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "p0-setup-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "\n",
    "from conversational_toolkit.agents.base import QueryWithContext\n",
    "from conversational_toolkit.embeddings.sentence_transformer import (\n",
    "    SentenceTransformerEmbeddings,\n",
    ")\n",
    "from conversational_toolkit.retriever.vectorstore_retriever import VectorStoreRetriever\n",
    "\n",
    "from sme_kt_zh_collaboration_rag.feature0_baseline_rag import (\n",
    "    load_chunks,\n",
    "    inspect_chunks,\n",
    "    build_vector_store,\n",
    "    inspect_retrieval,\n",
    "    build_agent,\n",
    "    build_llm,\n",
    "    ask,\n",
    "    DATA_DIR,\n",
    "    VS_PATH,\n",
    "    EMBEDDING_MODEL,\n",
    "    RETRIEVER_TOP_K,\n",
    ")\n",
    "\n",
    "# Choose your LLM backend: \"ollama\" (local, requires `ollama serve`) or \"openai\" (requires OPENAI_API_KEY)\n",
    "BACKEND = \"openai\"  # set this before running\n",
    "\n",
    "if not BACKEND:\n",
    "    raise ValueError(\n",
    "        'BACKEND is not set. Edit the line above and set it to \"ollama\", or \"openai\".\\n'\n",
    "        \"See Renku_README.md for setup instructions.\"\n",
    "    )\n",
    "\n",
    "ROOT = Path().resolve().parents[1]  # backend/notebooks/ â†’ project root\n",
    "print(f\"Project root : {ROOT}\")\n",
    "print(f\"Data dir     : {DATA_DIR}\")\n",
    "print(f\"Vector store : {VS_PATH}\")\n",
    "print(f\"LLM backend  : {BACKEND}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p0-step1-md",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 1: Load and Chunk Documents\n",
    "\n",
    "The `load_chunks()` function walks `data/` and dispatches each file to the right chunker:\n",
    "\n",
    "| Extension | Chunker | Strategy |\n",
    "|---|---|---|\n",
    "| `.pdf` | `PDFChunker` | Convert to Markdown via `pymupdf4llm`, split on `#` headings |\n",
    "| `.xlsx`, `.xls` | `ExcelChunker` | One chunk per sheet, serialised as a Markdown table |\n",
    "| `.md`, `.txt` | `MarkdownChunker` | Split on `#` headings |\n",
    "\n",
    "The result is a flat `list[Chunk]`, the same structure regardless of the original format.\n",
    "\n",
    "You can use `max_files=5` here for speed. Remove the limit (or set `None`) to load the full corpus.\n",
    "\n",
    "> **Feature Track 1** explores alternative chunking strategies in depth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "p0-step1-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = load_chunks(max_files=None)\n",
    "inspect_chunks(chunks)\n",
    "\n",
    "# Quick size distribution\n",
    "char_lengths = [len(c.content) for c in chunks]\n",
    "over_limit = sum(1 for n in char_lengths if n > 1024)\n",
    "print(f\"\\nChunks total       : {len(chunks)}\")\n",
    "print(f\"Mean length (chars): {sum(char_lengths) // len(char_lengths)}\")\n",
    "print(f\"Over 1024-char limit (â‰ˆ256 tok embedding limit): {over_limit} / {len(chunks)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p0-step1-inspect-md",
   "metadata": {},
   "source": [
    "### What a Chunk Looks Like\n",
    "\n",
    "Each chunk carries a `title` (the heading), the raw text `content`, and a `metadata` dict\n",
    "with the source file name. This metadata is returned alongside the answer so the user can\n",
    "trace every claim back to its origin document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "p0-step1-inspect-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print 3 representative chunks\n",
    "for c in chunks[:3]:\n",
    "    print(f\"--- [{c.metadata.get('source_file', '?')}] ---\")\n",
    "    print(f\"Title  : {c.title!r}\")\n",
    "    print(f\"Length : {len(c.content)} chars\")\n",
    "    print(f\"Preview: {c.content[:250].strip()!r}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p0-step2-md",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 2: Embed Chunks and Build the Vector Store\n",
    "\n",
    "`SentenceTransformerEmbeddings` converts every chunk's `content` to a 384-dimensional vector using `all-MiniLM-L6-v2`. The resulting matrix (shape `[n_chunks, 384]`) is inserted into a persistent `ChromaDBVectorStore`.\n",
    "\n",
    "**On subsequent runs**, leave `reset=False` (the default) to skip re-embedding, it takes time and the store on disk is already correct. Pass `reset=True` only when the corpus or chunking strategy changes.\n",
    "\n",
    "> **Why 384 dimensions?** `all-MiniLM-L6-v2` is a distilled model: small enough to run on CPU in seconds but good enough for retrieval on short technical texts. OpenAI's `text-embedding-3-small` produces 1536-dimensional vectors with higher quality at the cost of an API call per chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "p0-step2-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = SentenceTransformerEmbeddings(model_name=EMBEDDING_MODEL)\n",
    "print(f\"Embedding model: {EMBEDDING_MODEL}\")\n",
    "\n",
    "# Set reset=True to rebuild the store from scratch\n",
    "vector_store = await build_vector_store(\n",
    "    chunks, embedding_model, db_path=VS_PATH, reset=False\n",
    ")\n",
    "print(\"Vector store ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p0-step2-embed-md",
   "metadata": {},
   "source": [
    "### Similarity in Embedding Space\n",
    "\n",
    "Embeddings that are close in vector space share semantic meaning. The cell below embeds several sentences and measures their cosine similarity: a value between -1 (opposite) and 1 (identical). You can change the sentences to see the impact on cosine similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "p0-step2-embed-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "sentence1 = \"carbon footprint of a pallet\"\n",
    "sentence2 = \"GWP value for the Logypal 1\"\n",
    "sentence3 = \"PFAS-free tape declaration\"\n",
    "sentence4 = \"the annual report of a software firm\"\n",
    "\n",
    "\n",
    "async def cosine_similarity(a: str, b: str) -> float:\n",
    "    vecs = await embedding_model.get_embeddings([a, b])\n",
    "    return float(\n",
    "        np.dot(vecs[0], vecs[1]) / (np.linalg.norm(vecs[0]) * np.linalg.norm(vecs[1]))\n",
    "    )\n",
    "\n",
    "\n",
    "pairs = [\n",
    "    (sentence1, sentence2),\n",
    "    (sentence1, sentence3),\n",
    "    (sentence1, sentence4),\n",
    "]\n",
    "\n",
    "print(\"Cosine similarities:\")\n",
    "for a, b in pairs:\n",
    "    sim = await cosine_similarity(a, b)\n",
    "    print(f\"{sim:.3f}  -->  {a!r}  vs  {b!r}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p0-step3-md",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 3: Inspect Retrieval (Before the LLM Sees Anything)\n",
    "\n",
    "This is the **most important diagnostic step** in the whole pipeline:\n",
    "\n",
    "> If the retrieved chunks are wrong, the final answer will be wrong regardless of how good the LLM is.\n",
    "\n",
    "`inspect_retrieval()` runs the query through the embedding model, fetches the top-k most similar chunks from ChromaDB, and prints them with scores. Use this to verify that relevant documents are in the index, tune `top_k`, compare different query phrasings, and identify retrieval gaps before blaming the LLM.\n",
    "\n",
    "The **similarity score** is the L2 distance, range [0,4], lower = more similar. L2 distance is used becuase it works for any vectors, normalised or not. Cosine similarity only makes sense for direction (magnitude doesn't matter), so it requires that vectors be unit-length to be meaningful. L2 makes no such assumption, making it the safer general default. ChromaDB defaults to L2 because it's simpler to compute and works even if vector magnitudes vary. Since our embedding model always produces equal-length vectors, we get cosine-equivalent ranking. The score numbers look different, but the top-5 results would be identical either way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "p0-step3-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY = \"What materials is the Logypal 1 pallet made from?\"\n",
    "\n",
    "results = await inspect_retrieval(QUERY, vector_store, embedding_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p0-step3-fail-md",
   "metadata": {},
   "source": [
    "### Retrieval for a Product Outside the Portfolio\n",
    "\n",
    "The PrimePack AG product catalog defines the portfolio boundary. The **Lara Pallet** is not in the catalog, it does not exist. Watch which chunks are returned and what scores they have. A **higher** minimum score (large L2 distance) signals *weaker semantic match*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "p0-step3-fail-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_OOK = \"What materials is the Lara pallet made from?\"\n",
    "\n",
    "results_ook = await inspect_retrieval(QUERY_OOK, vector_store, embedding_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p0-step3-fail-obs-md",
   "metadata": {},
   "source": [
    "> **Observation:** The retriever always returns the *closest* chunks it can find, it has no concept of \"no match\". For an unknown product the L2 distances are **higher** (the closest chunks are still about other pallets), but without a score-threshold guard the LLM receives those chunks anyway and may silently answer about the wrong product.\n",
    "> **Feature Track 3** shows how to combat this issue."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p0-step4-md",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 4: Build the RAG Agent\n",
    "\n",
    "`build_agent()` assembles the three components:\n",
    "\n",
    "```\n",
    "VectorStoreRetriever\n",
    "    â””â”€ ChromaDBVectorStore (on disk, persists across runs)\n",
    "    â””â”€ SentenceTransformerEmbeddings\n",
    "\n",
    "RAG Agent\n",
    "    â”œâ”€ LLM (Ollama / OpenAI)\n",
    "    â”œâ”€ Retriever\n",
    "    â””â”€ System prompt\n",
    "```\n",
    "\n",
    "### The System Prompt\n",
    "\n",
    "The system prompt is the primary lever for controlling LLM behaviour. It is prepended to every conversation and defines the rules the model must follow:\n",
    "\n",
    "```\n",
    "You are a helpful AI assistant specialised in sustainability and product compliance\n",
    "for PrimePack AG.\n",
    "\n",
    "You will receive document excerpts relevant to the user's question.\n",
    "Produce the best possible answer using only the information in those excerpts.\n",
    "\n",
    "Rules:\n",
    "- Use the provided excerpts as your only source of truth. Do not rely on outside knowledge.\n",
    "- Use all relevant excerpts when forming your answer.\n",
    "- If the answer cannot be found in the excerpts, clearly say that you do not know.\n",
    "- Always cite the source document for any claim you make.\n",
    "- If excerpts contain conflicting information, report both values and flag the conflict.\n",
    "- Distinguish between third-party verified claims (EPDs) and self-declared supplier claims.\n",
    "```\n",
    "\n",
    "Each rule addresses a concrete failure mode:\n",
    "\n",
    "| Rule | Failure mode it prevents |\n",
    "|---|---|\n",
    "| Only use provided excerpts | Hallucination from general knowledge |\n",
    "| Say \"I don't know\" if not found | Confident wrong answer about missing products |\n",
    "| Cite the source document | Untraceable claims |\n",
    "| Flag conflicting information | Silently picking the wrong figure when sources disagree |\n",
    "| Distinguish EPD vs. self-declared | Presenting unverified marketing claims as verified facts |\n",
    "\n",
    "> **Note:** Stronger models (especially GPT-4o) follow these rules reliably even with a minimal prompt. Smaller local models (Ollama `mistral-nemo:12b`) benefit significantly from explicit, detailed rules. Section 4 probes these failure modes â€” if you are using OpenAI you may need to use a weaker model or a more leading query to observe them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "p0-step4-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = build_llm(backend=BACKEND)\n",
    "agent = build_agent(\n",
    "    vector_store=vector_store,\n",
    "    embedding_model=embedding_model,\n",
    "    llm=llm,\n",
    "    top_k=RETRIEVER_TOP_K,\n",
    "    number_query_expansion=0,  # 0 = no expansion; see Feature Track 4 for more\n",
    ")\n",
    "print(\"RAG agent assembled.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p0-step5-md",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 5: Ask a Question\n",
    "\n",
    "`ask()` sends the query to the agent and returns the answer string. The internal flow is:\n",
    "\n",
    "1. Embed the query\n",
    "2. Retrieve top-k chunks\n",
    "3. Build the prompt: `<system>` + `<sources>` XML block + user question\n",
    "4. Generate the answer with the LLM\n",
    "5. Return the answer and a list of cited source chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "p0-step5-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY = \"What materials is the Logypal 1 pallet made from?\"\n",
    "\n",
    "answer = await ask(agent, QUERY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fx3o0zxxg5v",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 5b: Query Expansion\n",
    "\n",
    "A single query embedding is a single point in vector space. If the user's phrasing differs from the document's phrasing, the best matching chunk may not rank in the top-k.\n",
    "\n",
    "**Query expansion** generates several alternative formulations of the same question, retrieves independently for each, and merges the results. The agent already supports this via the `number_query_expansion` parameter.\n",
    "\n",
    "```\n",
    "Original query: \"What is the carbon footprint of the Logypal 1?\"\n",
    "        â”‚\n",
    "        â–¼  LLM generates N variants\n",
    "        â”œâ”€â”€ \"GWP of the Logypal 1 pallet in kg COâ‚‚e\"\n",
    "        â”œâ”€â”€ \"Environmental impact, global warming potential, Logypal 1\"\n",
    "        â””â”€â”€ \"Relicyc pallet carbon emissions A1-A3\"\n",
    "        â”‚\n",
    "        â–¼  Retrieve top-k for each variant â†’ deduplicate â†’ merge\n",
    "        â””â”€â”€ Combined result set â†’ LLM generates final answer\n",
    "```\n",
    "\n",
    "The cost is one extra LLM call per expansion (to generate the variants), but retrieval recall typically improves substantially for domain-specific vocabulary differences.\n",
    "\n",
    "> **Feature Track 4** explores this and other advanced retrieval strategies (hybrid search, re-ranking) in depth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08vv0bu8v0sm",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare retrieval with and without query expansion\n",
    "\n",
    "QUERY_EXPANSION = \"What is the carbon footprint of the Logypal 1?\"\n",
    "\n",
    "# Without expansion â€” single query, top-5 chunks\n",
    "results_no_exp = await inspect_retrieval(QUERY_EXPANSION, vector_store, embedding_model)\n",
    "\n",
    "print(\"\\n--- With 3 expanded queries ---\\n\")\n",
    "\n",
    "# With expansion â€” 3 variants generated, deduplicated results\n",
    "agent_with_expansion = build_agent(\n",
    "    vector_store=vector_store,\n",
    "    embedding_model=embedding_model,\n",
    "    llm=llm,\n",
    "    top_k=RETRIEVER_TOP_K,\n",
    "    number_query_expansion=3,\n",
    ")\n",
    "answer_expanded = await ask(agent_with_expansion, QUERY_EXPANSION)\n",
    "\n",
    "print(f\"\\nChunks retrieved without expansion : {len(results_no_exp)}\")\n",
    "print(\"(with expansion the agent internally deduplicates across all variant queries)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p0-queries-md",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Probing Failure Modes\n",
    "\n",
    "The corpus was designed with three deliberate challenges. Run the queries below and observe the answers.\n",
    "\n",
    "### 4a: Out-of-Portfolio Query\n",
    "\n",
    "The **Lara Pallet** does not exist. A good RAG must say so instead of describing a different pallet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "p0-query-ook-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_ook = await ask(agent, \"What materials is the Lara pallet made from?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p0-query-gap-md",
   "metadata": {},
   "source": [
    "### 4b: Missing Data (LogyLight Pallet)\n",
    "\n",
    "The LogyLight datasheet marks all LCA fields as *\"not yet available\"*. The correct answer is that we don't have the data, not a fabricated figure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "p0-query-gap-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_gap = await ask(agent, \"What is the GWP of the LogyLight pallet?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p0-query-conflict-md",
   "metadata": {},
   "source": [
    "### 4c: Conflicting Evidence (Relicyc GWP Figures)\n",
    "\n",
    "The 2021 Relicyc datasheet reports **4.1 kg COâ‚‚e** per pallet. The 2023 EPD (third-party verified) reports a different, more recent figure. The RAG should flag the conflict and prefer the verified, more recent source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "p0-query-conflict-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_conflict = await ask(\n",
    "    agent, \"What is the GWP of the Logypal 1 pallet, and how reliable is the figure?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p0-query-unverified-md",
   "metadata": {},
   "source": [
    "### 4d: Unverified Supplier Claim (Tesa ECO Tape)\n",
    "\n",
    "The tesa supplier brochure claims **68% COâ‚‚ reduction** compared to conventional tape. This is a self-declared marketing claim, there is no independent EPD. The RAG should report the claim but flag that it is unverified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "p0-query-unverified-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_claim = await ask(\n",
    "    agent,\n",
    "    \"How much lower is the carbon footprint of tesa ECO tape compared to standard tape?\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5948f40",
   "metadata": {},
   "source": [
    "> ðŸ’¬ **Discuss with your group:** Which of the four failure modes above concerns you most in a real deployment? What would the consequence be if a colleague trusted a wrong answer about a sustainability claim?\n",
    ">\n",
    "> Can you think of other ways the system might fail that aren't shown here?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p0-multiturn-md",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Multi-Turn Conversation\n",
    "\n",
    "The `ask()` function accepts a `history` argument, a list of prior `LLMMessage` objects. When history is provided the agent first **rewrites the query** to be self-contained (*\"it\"* becomes the actual product name) before retrieval.\n",
    "\n",
    "This prevents the retriever from embedding vague pronouns that match nothing in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "p0-multiturn-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "from conversational_toolkit.llms.base import LLMMessage, Roles\n",
    "\n",
    "history: list[LLMMessage] = []\n",
    "\n",
    "\n",
    "async def conversation_turn(query: str) -> str:\n",
    "    global history\n",
    "    answer = await agent.answer(QueryWithContext(query=query, history=history))\n",
    "    history.append(LLMMessage(role=Roles.USER, content=query))\n",
    "    history.append(LLMMessage(role=Roles.ASSISTANT, content=answer.content))\n",
    "    return answer.content\n",
    "\n",
    "\n",
    "# Turn 1: ask about a specific product\n",
    "reply1 = await conversation_turn(\n",
    "    \"Which pallets in our portfolio have a third-party verified EPD?\"\n",
    ")\n",
    "print(\"User: Which pallets in our portfolio have a third-party verified EPD?\")\n",
    "print(f\"Assistant: {reply1}\\n\")\n",
    "\n",
    "# Turn 2: follow-up using a pronoun â€” the agent should resolve \"it\" before retrieval\n",
    "reply2 = await conversation_turn(\n",
    "    \"What is the GWP figure reported in it for the Logypal 1?\"\n",
    ")\n",
    "print(\"User: What is the GWP figure reported in it for the Logypal 1?\")\n",
    "print(f\"Assistant: {reply2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p0-arch-md",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Running the Full Pipeline in One Call\n",
    "\n",
    "The `run_pipeline()` convenience function executes all five steps end-to-end. It is also\n",
    "what the `__main__` entry point calls.\n",
    "\n",
    "Use it for quick one-shot queries. Use the individual step functions above when you need\n",
    "to inspect intermediate results or iterate on a specific stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "p0-arch-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sme_kt_zh_collaboration_rag.feature0_baseline_rag import run_pipeline\n",
    "\n",
    "answer = await run_pipeline(\n",
    "    backend=BACKEND,\n",
    "    query=\"What sustainability certifications do the pallets in the portfolio have?\",\n",
    "    reset_vs=False,\n",
    ")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p0-backends-md",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Switching LLM Backends\n",
    "\n",
    "The pipeline abstracts the LLM behind a common interface. Only `build_llm()` needs to change.\n",
    "\n",
    "| Backend | `BACKEND=` | Prerequisite |\n",
    "|---|---|---|\n",
    "| Ollama (local) | `\"ollama\"` | `ollama serve` + `ollama pull mistral-nemo:12b` |\n",
    "| OpenAI | `\"openai\"` | `OPENAI_API_KEY` env variable |\n",
    "\n",
    "You can also override the model within a backend:\n",
    "\n",
    "```python\n",
    "llm = build_llm(backend=\"openai\", model_name=\"gpt-4o\") # stronger model\n",
    "llm = build_llm(backend=\"ollama\", model_name=\"llama3.2\") # smaller local model\n",
    "```\n",
    "\n",
    "The RAG pipeline is **backend-agnostic**, the retrieval step is identical regardless of which LLM is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "p0-backends-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test openai\n",
    "\n",
    "QUERY = \"What materials is the Lara pallet made from?\"\n",
    "\n",
    "llm_openai = build_llm(backend=\"openai\", model_name=\"gpt-4o-mini\")\n",
    "agent_openai = build_agent(vector_store, embedding_model, llm_openai)\n",
    "answer_openai = await ask(agent_openai, QUERY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p0-exercises-md",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Tasks & Discussion\n",
    "\n",
    "Work through these as a group. You don't need to do them all â€” pick what interests you or what matches your background. Mix and match freely.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ’¬ Explore & Discuss\n",
    "\n",
    "**A. Build a test set together**\n",
    "Go through the data files (run the scratch cell below to see what's there) and write down five questions where the answer *is* in the documents, two questions about products that don't exist or data that isn't there, and one question where you suspect conflicting information. Run them through the system. Does it pass your test?\n",
    "\n",
    "**B. Evaluate the outputs â€” would you trust them?**\n",
    "Look back at the failure mode answers in Section 4. For each one: would you have trusted this answer without knowing it might be wrong? What would a *good* response look like? What would need to change in the system to prevent this failure?\n",
    "\n",
    "**C. Think about your own context**\n",
    "If you were to deploy this in your organisation, what documents would go into the knowledge base? What questions should it answer well? Who would use it, and what are the consequences of a wrong answer?\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”§ Code Experiments\n",
    "\n",
    "**1. Retrieval inspection**\n",
    "Call `inspect_retrieval()` with several different queries. Look at the L2 scores â€” at roughly what value do retrieved chunks stop being relevant? Could you use this as a confidence threshold?\n",
    "\n",
    "**2. Top-k sensitivity**\n",
    "Change `top_k` from 5 to 1 and re-run a query from the failure mode section. Does the answer change? Try 10. Is more context always better, or does noise creep in?\n",
    "\n",
    "**3. System prompt ablation**\n",
    "In `feature0_baseline_rag.py`, find `SYSTEM_PROMPT`. Remove the rule about flagging conflicting information. Rebuild the agent and re-run the Relicyc GWP query (Section 4c). Does the answer change?\n",
    "\n",
    "**4. Query phrasing experiment**\n",
    "Try these three phrasings for the same underlying question: `\"COâ‚‚ footprint Logypal 1\"`, `\"carbon emissions recycled pallet\"`, and `\"GWP A1-A3 EPD pallet\"`. Do the retrieved chunks differ? Which phrasing finds the most relevant result?\n",
    "\n",
    "**5. Query expansion**\n",
    "Set `number_query_expansion=3` (Step 5b) and repeat Task 4. Does expansion reduce sensitivity to phrasing?\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸš€ Possible Extensions\n",
    "\n",
    "| Extension | Effort | What it enables |\n",
    "|---|---|---|\n",
    "| Load the full corpus (`max_files=None`) | Minimal | More documents, better coverage |\n",
    "| Try `gpt-4o` instead of `gpt-4o-mini` | Minimal | Stronger failure mode handling |\n",
    "| Add a new document to `data/` | Easy | Extend the knowledge base with your own materials |\n",
    "| Multi-lingual queries | Easy to test | Ask questions in German / French |\n",
    "| Score threshold filter | Medium | Detect when a query falls outside the known portfolio |\n",
    "| Structured outputs (Feature Track 3) | Medium | Every answer includes: product, figure, source, verified? |\n",
    "| Query expansion + re-ranking (Feature Track 4) | Medium | Better recall for domain-specific vocabulary |\n",
    "| Chat frontend (Feature Track 6) | Medium | Share the prototype with colleagues via a browser interface |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "p0-exercises-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scratch cell â€” run your experiments here\n",
    "async def quick_retrieve(query: str, top_k: int = 5):\n",
    "    retriever = VectorStoreRetriever(embedding_model, vector_store, top_k=top_k)\n",
    "    results = await retriever.retrieve(query)\n",
    "    print(f\"Query: {query!r}  (top_k={top_k})\")\n",
    "    for r in results:\n",
    "        src = r.metadata.get(\"source_file\", \"?\")\n",
    "        print(f\"  score={r.score:.4f}  {src}  {r.title!r}\")\n",
    "\n",
    "\n",
    "await quick_retrieve(\"PFAS-free tape declaration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p0-summary-md",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "| Step | Function |\n",
    "|---|---|\n",
    "| 1. Load & chunk | `load_chunks(max_files)` |\n",
    "| 2. Embed & index | `build_vector_store(chunks, emb, reset)` |\n",
    "| 3. Inspect retrieval | `inspect_retrieval(query, vs, emb)` |\n",
    "| 4. Build agent | `build_agent(vs, emb, llm, top_k, number_query_expansion)` |\n",
    "| 5. Generate answer | `ask(agent, query, history)` |\n",
    "\n",
    "### Three Core Failure Modes\n",
    "\n",
    "| Failure mode | Symptom | Root cause | Addressed in |\n",
    "|---|---|---|---|\n",
    "| **Wrong entity** | RAG answers about the wrong product | Retriever returns high-scoring chunks for a product that doesn't exist in corpus; LLM picks the closest match | Feature Track 3 (structured output + confidence filtering) |\n",
    "| **Missing data** | LLM invents a figure that isn't in any document | Retriever returns vaguely related chunks; LLM fills the gap with general knowledge | Feature Track 3 (strict source-only rules) |\n",
    "| **Low recall** | Correct chunk exists but isn't retrieved | Query phrasing doesn't match document vocabulary; top-k too small | Feature Track 4 (query expansion, hybrid search) |\n",
    "\n",
    "> **With OpenAI models these failures are often subtle** â€” GPT-4o follows the \"say I don't know\" rule reliably. They become more pronounced with local models (Ollama) or with adversarial query phrasing. The experiments in the Tasks section above give guidance on how to observe them.\n",
    "\n",
    "**Next â€” Feature Track 1:** Explore chunking strategies and understand how chunk size and strategy affect retrieval quality."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_venv (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
