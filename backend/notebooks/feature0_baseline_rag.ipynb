{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "p0-title-md",
   "metadata": {},
   "source": [
    "# The Baseline RAG Pipeline\n",
    "\n",
    "**RAG Prototyping Workshop**\n",
    "\n",
    "---\n",
    "\n",
    "## What You Will Learn\n",
    "\n",
    "This notebook is the starting point for the workshop. It introduces the **key concepts** behind Retrieval-Augmented Generation (RAG) and walks through every step of the **baseline pipeline** that the later phases build upon.\n",
    "\n",
    "After working through this notebook you will be able to:\n",
    "- Explain why a standalone LLM is insufficient for grounded enterprise Q&A\n",
    "- Describe the five stages of a RAG pipeline (chunk -> embed -> store -> retrieve -> generate)\n",
    "- Run the full baseline pipeline against the PrimePack AG corpus\n",
    "- Use the retrieval inspection step as the primary debugging tool\n",
    "- Identify the three main failure modes this workshop addresses\n",
    "\n",
    "**Workshop Phases at a Glance**\n",
    "| Notebook | Focus |\n",
    "|---|---|\n",
    "| **Baseline (this notebook)** | Key concepts + end-to-end baseline |\n",
    "| Feature Track 1 | Chunking strategies & document ingestion |\n",
    "| Feature Track 2 | Evaluation metrics (retrieval + generation) |\n",
    "| Feature Track 3 | Reliable & structured outputs |\n",
    "| Feature Track 4 | Advanced retrieval |\n",
    "| Feature Track 5 | Multi-step agent workflows |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p0-why-rag-md",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Why RAG? The Problem with a Standalone LLM\n",
    "\n",
    "### The Scenario\n",
    "**PrimePack AG** buys packaging materials (pallets, cardboard boxes, tape) from multiple suppliers. Sustainability claims are increasingly scrutinised by customers and regulators. Employees need to answer questions like:\n",
    "> *\"What is the GWP of the Logypal 1 pallet, and is the figure verified?\"*  \n",
    "> *\"Can we tell a customer that the tesa tape is PFAS-free?\"*  \n",
    "> *\"Which of our suppliers have a certified EPD?\"*\n",
    "\n",
    "### Why Not Just Ask ChatGPT?\n",
    "A general-purpose LLM has three fundamental problems for this task:\n",
    "\n",
    "| Problem | Why It Matters |\n",
    "|---|---|\n",
    "| **No product knowledge** | LLMs know nothing about Logypal 1, Andrea Packaging's specific portfolio, or the individual supplier documents. |\n",
    "| **Hallucination** | When asked about unknown products the LLM invents plausible-sounding but false figures. |\n",
    "| **No evidence trail** | Even when correct, a raw LLM answer cannot be traced back to a source document. |\n",
    "\n",
    "### The RAG Solution\n",
    "RAG adds a **retrieval step** between the user's question and the LLM:\n",
    "\n",
    "```\n",
    " Documents ──► Chunker ──► Embedder ──► Vector DB\n",
    "                                              │\n",
    " User query ─────────────────► Embedder ─────►  Retriever ──► Top-k Chunks\n",
    "                                                                      │\n",
    "                                                               LLM + Prompt\n",
    "                                                                      │\n",
    "                                                               Answer + Sources\n",
    "```\n",
    "\n",
    "The LLM only sees documents that are **actually in the corpus**. The answer can be traced to specific source chunks. If the corpus does not contain the answer, the LLM is instructed to say so.\n",
    "\n",
    "### What RAG Does *Not* Fix\n",
    "RAG shifts the problem from hallucination to **retrieval quality**. If the right chunk is not retrieved, the answer will still be wrong (or absent). The later phases of this workshop address exactly this: better chunking, better retrieval, and better output structure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p0-concepts-md",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Core Concepts\n",
    "\n",
    "### Chunks\n",
    "\n",
    "A **chunk** is a short excerpt from a source document, a section of a PDF, one sheet of a spreadsheet, or one heading-delimited paragraph of a Markdown file. Chunks are the unit of indexing and retrieval.\n",
    "\n",
    "```python\n",
    "@dataclass\n",
    "class Chunk:\n",
    "    id: str           # unique identifier\n",
    "    title: str        # e.g. section heading\n",
    "    content: str      # the text that gets embedded\n",
    "    metadata: dict    # source_file, page, ...\n",
    "```\n",
    "\n",
    "### Embeddings\n",
    "An **embedding** converts text to a dense numeric vector (e.g. 384 dimensions). Semantically similar texts produce similar vectors. Here we use `all-MiniLM-L6-v2`, a compact local model that runs without an API key.\n",
    "\n",
    "### Vector Store (ChromaDB)\n",
    "A **vector store** persists chunk embeddings on disk and supports approximate nearest-neighbour search. Given a query embedding, it returns the `top_k` most similar chunks in milliseconds.\n",
    "\n",
    "### Retriever\n",
    "A **retriever** wraps a vector store and exposes a single `retrieve(query)` method. The baseline uses a `VectorStoreRetriever` with `top_k=5`.\n",
    "\n",
    "### RAG Agent\n",
    "The **RAG agent** combines a retriever and an LLM. Its `answer()` method:\n",
    "1. Embeds the query\n",
    "2. Retrieves the top-k chunks\n",
    "3. Formats chunks as XML `<source>` tags in the prompt\n",
    "4. Calls the LLM and returns the answer + cited sources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p0-setup-md",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Setup\n",
    "\n",
    "**Prerequisites:**\n",
    "- `conversational-toolkit` installed in editable mode (`pip install -e conversational-toolkit/`) (already done on Renku)\n",
    "- `backend` installed in editable mode (`pip install -e backend/`) (already done on Renku)\n",
    "- For the **Ollama** backend (default): `ollama serve` running + `ollama pull mistral-nemo:12b`\n",
    "- For the **OpenAI** backend: `OPENAI_API_KEY` set in the environment (already done on Renku)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "p0-setup-code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consider using the pymupdf_layout package for a greatly improved page layout analysis.\n",
      "Project root : /Users/pkoerner/Desktop/Kanton_Zurich/sme-kt-zh-collaboration-rag\n",
      "Data dir     : /Users/pkoerner/Desktop/Kanton_Zurich/sme-kt-zh-collaboration-rag/data\n",
      "Vector store : /Users/pkoerner/Desktop/Kanton_Zurich/sme-kt-zh-collaboration-rag/backend/data_vs.db\n",
      "LLM backend  : ollama\n"
     ]
    }
   ],
   "source": [
    "# imports and confiurations\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "from conversational_toolkit.agents.base import QueryWithContext\n",
    "from conversational_toolkit.embeddings.sentence_transformer import (\n",
    "    SentenceTransformerEmbeddings,\n",
    ")\n",
    "from conversational_toolkit.retriever.vectorstore_retriever import VectorStoreRetriever\n",
    "\n",
    "from sme_kt_zh_collaboration_rag.baseline_rag import (\n",
    "    load_chunks,\n",
    "    inspect_chunks,\n",
    "    build_vector_store,\n",
    "    inspect_retrieval,\n",
    "    build_agent,\n",
    "    build_llm,\n",
    "    ask,\n",
    "    DATA_DIR,\n",
    "    VS_PATH,\n",
    "    EMBEDDING_MODEL,\n",
    "    RETRIEVER_TOP_K,\n",
    ")\n",
    "\n",
    "BACKEND = \"ollama\"  # \"ollama\" (local) or \"openai\" (requires OPENAI_API_KEY)\n",
    "\n",
    "ROOT = Path().resolve().parents[1]  # backend/notebooks/ → project root\n",
    "print(f\"Project root : {ROOT}\")\n",
    "print(f\"Data dir     : {DATA_DIR}\")\n",
    "print(f\"Vector store : {VS_PATH}\")\n",
    "print(f\"LLM backend  : {BACKEND}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p0-step1-md",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 1: Load and Chunk Documents\n",
    "\n",
    "The `load_chunks()` function walks `data/` and dispatches each file to the right chunker:\n",
    "\n",
    "| Extension | Chunker | Strategy |\n",
    "|---|---|---|\n",
    "| `.pdf` | `PDFChunker` | Convert to Markdown via `pymupdf4llm`, split on `#` headings |\n",
    "| `.xlsx`, `.xls` | `ExcelChunker` | One chunk per sheet, serialised as a Markdown table |\n",
    "| `.md`, `.txt` | `MarkdownChunker` | Split on `#` headings |\n",
    "\n",
    "The result is a flat `list[Chunk]`, the same structure regardless of the original format.\n",
    "\n",
    "We use `max_files=5` here for speed. Remove the limit (or set `None`) to load the full corpus.\n",
    "\n",
    "> **Feature Track 1** explores alternative chunking strategies in depth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "p0-step1-code",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-02-20 15:41:10.821\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msme_kt_zh_collaboration_rag.baseline_rag\u001b[0m:\u001b[36mload_chunks\u001b[0m:\u001b[36m178\u001b[0m - \u001b[1mChunking 5 files from /Users/pkoerner/Desktop/Kanton_Zurich/sme-kt-zh-collaboration-rag/data\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-02-20 15:41:19.315\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msme_kt_zh_collaboration_rag.baseline_rag\u001b[0m:\u001b[36mload_chunks\u001b[0m:\u001b[36m187\u001b[0m - \u001b[34m\u001b[1m  1_Product-Life-Cycle-Accounting-Reporting-Standard_041613.pdf: 32 chunks\u001b[0m\n",
      "\u001b[32m2026-02-20 15:41:20.152\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msme_kt_zh_collaboration_rag.baseline_rag\u001b[0m:\u001b[36mload_chunks\u001b[0m:\u001b[36m187\u001b[0m - \u001b[34m\u001b[1m  2_EPD_pallet_CPR.pdf: 11 chunks\u001b[0m\n",
      "\u001b[32m2026-02-20 15:41:21.541\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msme_kt_zh_collaboration_rag.baseline_rag\u001b[0m:\u001b[36mload_chunks\u001b[0m:\u001b[36m187\u001b[0m - \u001b[34m\u001b[1m  3_EPD_pallet_relicyc.pdf: 17 chunks\u001b[0m\n",
      "\u001b[32m2026-02-20 15:41:22.869\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msme_kt_zh_collaboration_rag.baseline_rag\u001b[0m:\u001b[36mload_chunks\u001b[0m:\u001b[36m187\u001b[0m - \u001b[34m\u001b[1m  4_EPD_pallet_Stabilplastik.pdf: 2 chunks\u001b[0m\n",
      "\u001b[32m2026-02-20 15:41:22.870\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msme_kt_zh_collaboration_rag.baseline_rag\u001b[0m:\u001b[36mload_chunks\u001b[0m:\u001b[36m187\u001b[0m - \u001b[34m\u001b[1m  ART_customer_inquiry_frische_felder.md: 6 chunks\u001b[0m\n",
      "\u001b[32m2026-02-20 15:41:22.870\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msme_kt_zh_collaboration_rag.baseline_rag\u001b[0m:\u001b[36mload_chunks\u001b[0m:\u001b[36m191\u001b[0m - \u001b[1mDone, 68 chunks total\u001b[0m\n",
      "\u001b[32m2026-02-20 15:41:22.871\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msme_kt_zh_collaboration_rag.baseline_rag\u001b[0m:\u001b[36minspect_chunks\u001b[0m:\u001b[36m203\u001b[0m - \u001b[1m------ Chunk inspection -------\u001b[0m\n",
      "\u001b[32m2026-02-20 15:41:22.871\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msme_kt_zh_collaboration_rag.baseline_rag\u001b[0m:\u001b[36minspect_chunks\u001b[0m:\u001b[36m204\u001b[0m - \u001b[1mTotal chunks: 68; Source files: 5\u001b[0m\n",
      "\u001b[32m2026-02-20 15:41:22.871\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msme_kt_zh_collaboration_rag.baseline_rag\u001b[0m:\u001b[36minspect_chunks\u001b[0m:\u001b[36m206\u001b[0m - \u001b[1m1_Product-Life-Cycle-Accounting-Reporting-Standard_041613.pdf: 32 chunks\u001b[0m\n",
      "\u001b[32m2026-02-20 15:41:22.871\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msme_kt_zh_collaboration_rag.baseline_rag\u001b[0m:\u001b[36minspect_chunks\u001b[0m:\u001b[36m206\u001b[0m - \u001b[1m2_EPD_pallet_CPR.pdf: 11 chunks\u001b[0m\n",
      "\u001b[32m2026-02-20 15:41:22.871\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msme_kt_zh_collaboration_rag.baseline_rag\u001b[0m:\u001b[36minspect_chunks\u001b[0m:\u001b[36m206\u001b[0m - \u001b[1m3_EPD_pallet_relicyc.pdf: 17 chunks\u001b[0m\n",
      "\u001b[32m2026-02-20 15:41:22.872\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msme_kt_zh_collaboration_rag.baseline_rag\u001b[0m:\u001b[36minspect_chunks\u001b[0m:\u001b[36m206\u001b[0m - \u001b[1m4_EPD_pallet_Stabilplastik.pdf: 2 chunks\u001b[0m\n",
      "\u001b[32m2026-02-20 15:41:22.872\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msme_kt_zh_collaboration_rag.baseline_rag\u001b[0m:\u001b[36minspect_chunks\u001b[0m:\u001b[36m206\u001b[0m - \u001b[1mART_customer_inquiry_frische_felder.md: 6 chunks\u001b[0m\n",
      "\u001b[32m2026-02-20 15:41:22.872\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msme_kt_zh_collaboration_rag.baseline_rag\u001b[0m:\u001b[36minspect_chunks\u001b[0m:\u001b[36m207\u001b[0m - \u001b[1mSample (first 5):\u001b[0m\n",
      "\u001b[32m2026-02-20 15:41:22.872\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msme_kt_zh_collaboration_rag.baseline_rag\u001b[0m:\u001b[36minspect_chunks\u001b[0m:\u001b[36m210\u001b[0m - \u001b[1mSource and title: [1_Product-Life-Cycle-Accounting-Reporting-Standard_041613.pdf] '###### **_01 Introduction_**'\u001b[0m\n",
      "\u001b[32m2026-02-20 15:41:22.872\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msme_kt_zh_collaboration_rag.baseline_rag\u001b[0m:\u001b[36minspect_chunks\u001b[0m:\u001b[36m211\u001b[0m - \u001b[1mChunk content: '###### **_01 Introduction_**'\u001b[0m\n",
      "\u001b[32m2026-02-20 15:41:22.873\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msme_kt_zh_collaboration_rag.baseline_rag\u001b[0m:\u001b[36minspect_chunks\u001b[0m:\u001b[36m210\u001b[0m - \u001b[1mSource and title: [1_Product-Life-Cycle-Accounting-Reporting-Standard_041613.pdf] '# **_E_**'\u001b[0m\n",
      "\u001b[32m2026-02-20 15:41:22.873\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msme_kt_zh_collaboration_rag.baseline_rag\u001b[0m:\u001b[36minspect_chunks\u001b[0m:\u001b[36m211\u001b[0m - \u001b[1mChunk content: '# **_E_**\\n\\n\\n\\n_**missions of the anthropogenic greenhouse gases (GHG) that drive climate change**_\\n\\n_**and its impacts around the world are growing. According to climate scientists,**_\\n\\n_**global carbo'\u001b[0m\n",
      "\u001b[32m2026-02-20 15:41:22.873\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msme_kt_zh_collaboration_rag.baseline_rag\u001b[0m:\u001b[36minspect_chunks\u001b[0m:\u001b[36m210\u001b[0m - \u001b[1mSource and title: [1_Product-Life-Cycle-Accounting-Reporting-Standard_041613.pdf] '###### **_02 Defining Business Goals_**'\u001b[0m\n",
      "\u001b[32m2026-02-20 15:41:22.873\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msme_kt_zh_collaboration_rag.baseline_rag\u001b[0m:\u001b[36minspect_chunks\u001b[0m:\u001b[36m211\u001b[0m - \u001b[1mChunk content: '###### **_02 Defining Business Goals_**'\u001b[0m\n",
      "\u001b[32m2026-02-20 15:41:22.873\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msme_kt_zh_collaboration_rag.baseline_rag\u001b[0m:\u001b[36minspect_chunks\u001b[0m:\u001b[36m210\u001b[0m - \u001b[1mSource and title: [1_Product-Life-Cycle-Accounting-Reporting-Standard_041613.pdf] '# **_C_**'\u001b[0m\n",
      "\u001b[32m2026-02-20 15:41:22.874\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msme_kt_zh_collaboration_rag.baseline_rag\u001b[0m:\u001b[36minspect_chunks\u001b[0m:\u001b[36m211\u001b[0m - \u001b[1mChunk content: '# **_C_**\\n\\n\\n\\n_**ompanies should first identify their business goals before conducting product**_\\n\\n_**GHG inventories. Doing so can bring clarity and assist in selecting the appropriate**_\\n\\n_**methodol'\u001b[0m\n",
      "\u001b[32m2026-02-20 15:41:22.874\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msme_kt_zh_collaboration_rag.baseline_rag\u001b[0m:\u001b[36minspect_chunks\u001b[0m:\u001b[36m210\u001b[0m - \u001b[1mSource and title: [1_Product-Life-Cycle-Accounting-Reporting-Standard_041613.pdf] '###### **_03 Summary of Steps and Requirements_**'\u001b[0m\n",
      "\u001b[32m2026-02-20 15:41:22.874\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msme_kt_zh_collaboration_rag.baseline_rag\u001b[0m:\u001b[36minspect_chunks\u001b[0m:\u001b[36m211\u001b[0m - \u001b[1mChunk content: '###### **_03 Summary of Steps and Requirements_**\\n\\n [12] _**Product Life Cycle Accounting and Reporting Standard**_'\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Chunks total       : 68\n",
      "Mean length (chars): 5304\n",
      "Over 1024-char limit (≈256 tok embedding limit): 34 / 68\n"
     ]
    }
   ],
   "source": [
    "chunks = load_chunks(max_files=5)\n",
    "inspect_chunks(chunks)\n",
    "\n",
    "# Quick size distribution\n",
    "char_lengths = [len(c.content) for c in chunks]\n",
    "over_limit = sum(1 for n in char_lengths if n > 1024)\n",
    "print(f\"\\nChunks total       : {len(chunks)}\")\n",
    "print(f\"Mean length (chars): {sum(char_lengths) // len(char_lengths)}\")\n",
    "print(f\"Over 1024-char limit (≈256 tok embedding limit): {over_limit} / {len(chunks)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p0-step1-inspect-md",
   "metadata": {},
   "source": [
    "### What a Chunk Looks Like\n",
    "\n",
    "Each chunk carries a `title` (the heading), the raw text `content`, and a `metadata` dict\n",
    "with the source file name. This metadata is returned alongside the answer so the user can\n",
    "trace every claim back to its origin document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "p0-step1-inspect-code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- [1_Product-Life-Cycle-Accounting-Reporting-Standard_041613.pdf] ---\n",
      "Title  : '###### **_01 Introduction_**'\n",
      "Length : 31 chars\n",
      "Preview: '###### **_01 Introduction_**'\n",
      "\n",
      "--- [1_Product-Life-Cycle-Accounting-Reporting-Standard_041613.pdf] ---\n",
      "Title  : '# **_E_**'\n",
      "Length : 15995 chars\n",
      "Preview: '# **_E_**\\n\\n\\n\\n_**missions of the anthropogenic greenhouse gases (GHG) that drive climate change**_\\n\\n_**and its impacts around the world are growing. According to climate scientists,**_\\n\\n_**global carbon dioxide emissions must be cut by as much as 85 p'\n",
      "\n",
      "--- [1_Product-Life-Cycle-Accounting-Reporting-Standard_041613.pdf] ---\n",
      "Title  : '###### **_02 Defining Business Goals_**'\n",
      "Length : 42 chars\n",
      "Preview: '###### **_02 Defining Business Goals_**'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print 3 representative chunks\n",
    "for c in chunks[:3]:\n",
    "    print(f\"--- [{c.metadata.get('source_file', '?')}] ---\")\n",
    "    print(f\"Title  : {c.title!r}\")\n",
    "    print(f\"Length : {len(c.content)} chars\")\n",
    "    print(f\"Preview: {c.content[:250].strip()!r}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p0-step2-md",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 2: Embed Chunks and Build the Vector Store\n",
    "\n",
    "`SentenceTransformerEmbeddings` converts every chunk's `content` to a 384-dimensional vector using `all-MiniLM-L6-v2`. The resulting matrix (shape `[n_chunks, 384]`) is inserted into a persistent `ChromaDBVectorStore`.\n",
    "\n",
    "**On subsequent runs**, leave `reset=False` (the default) to skip re-embedding, it takes time and the store on disk is already correct. Pass `reset=True` only when the corpus or chunking strategy changes.\n",
    "\n",
    "> **Why 384 dimensions?** `all-MiniLM-L6-v2` is a distilled model: small enough to run on CPU in seconds but good enough for retrieval on short technical texts. OpenAI's `text-embedding-3-small` produces 1536-dimensional vectors with higher quality at the cost of an API call per chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "p0-step2-code",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-02-20 15:41:24.884\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.embeddings.sentence_transformer\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m57\u001b[0m - \u001b[34m\u001b[1mSentence Transformer embeddings model loaded: sentence-transformers/all-MiniLM-L6-v2 with kwargs: {}\u001b[0m\n",
      "\u001b[32m2026-02-20 15:41:24.972\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msme_kt_zh_collaboration_rag.baseline_rag\u001b[0m:\u001b[36mbuild_vector_store\u001b[0m:\u001b[36m235\u001b[0m - \u001b[1mVector store already contains 78 chunks — skipping embedding.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding model: sentence-transformers/all-MiniLM-L6-v2\n",
      "Vector store ready.\n"
     ]
    }
   ],
   "source": [
    "embedding_model = SentenceTransformerEmbeddings(model_name=EMBEDDING_MODEL)\n",
    "print(f\"Embedding model: {EMBEDDING_MODEL}\")\n",
    "\n",
    "# Set reset=True to rebuild the store from scratch\n",
    "vector_store = await build_vector_store(\n",
    "    chunks, embedding_model, db_path=VS_PATH, reset=False\n",
    ")\n",
    "print(\"Vector store ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p0-step2-embed-md",
   "metadata": {},
   "source": [
    "### Similarity in Embedding Space\n",
    "\n",
    "Embeddings that are close in vector space share semantic meaning. The cell below embeds several sentences and measures their cosine similarity: a value between -1 (opposite) and 1 (identical). You can change the sentences to see the impact on cosine similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "p0-step2-embed-code",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-02-20 15:41:25.082\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.embeddings.sentence_transformer\u001b[0m:\u001b[36mget_embeddings\u001b[0m:\u001b[36m76\u001b[0m - \u001b[34m\u001b[1msentence-transformers/all-MiniLM-L6-v2 embeddings size: (2, 384)\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarities:\n",
      "0.133  -->  'carbon footprint of a pallet'  vs  'GWP value for the Logypal 1'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-02-20 15:41:25.169\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.embeddings.sentence_transformer\u001b[0m:\u001b[36mget_embeddings\u001b[0m:\u001b[36m76\u001b[0m - \u001b[34m\u001b[1msentence-transformers/all-MiniLM-L6-v2 embeddings size: (2, 384)\u001b[0m\n",
      "\u001b[32m2026-02-20 15:41:25.178\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.embeddings.sentence_transformer\u001b[0m:\u001b[36mget_embeddings\u001b[0m:\u001b[36m76\u001b[0m - \u001b[34m\u001b[1msentence-transformers/all-MiniLM-L6-v2 embeddings size: (2, 384)\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.064  -->  'carbon footprint of a pallet'  vs  'PFAS-free tape declaration'\n",
      "0.014  -->  'carbon footprint of a pallet'  vs  'the annual report of a software firm'\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "sentence1 = \"carbon footprint of a pallet\"\n",
    "sentence2 = \"GWP value for the Logypal 1\"\n",
    "sentence3 = \"PFAS-free tape declaration\"\n",
    "sentence4 = \"the annual report of a software firm\"\n",
    "\n",
    "\n",
    "async def cosine_similarity(a: str, b: str) -> float:\n",
    "    vecs = await embedding_model.get_embeddings([a, b])\n",
    "    return float(\n",
    "        np.dot(vecs[0], vecs[1]) / (np.linalg.norm(vecs[0]) * np.linalg.norm(vecs[1]))\n",
    "    )\n",
    "\n",
    "\n",
    "pairs = [\n",
    "    (sentence1, sentence2),\n",
    "    (sentence1, sentence3),\n",
    "    (sentence1, sentence4),\n",
    "]\n",
    "\n",
    "print(\"Cosine similarities:\")\n",
    "for a, b in pairs:\n",
    "    sim = await cosine_similarity(a, b)\n",
    "    print(f\"{sim:.3f}  -->  {a!r}  vs  {b!r}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p0-step3-md",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 3: Inspect Retrieval (Before the LLM Sees Anything)\n",
    "\n",
    "This is the **most important diagnostic step** in the whole pipeline:\n",
    "\n",
    "> If the retrieved chunks are wrong, the final answer will be wrong regardless of how good the LLM is.\n",
    "\n",
    "`inspect_retrieval()` runs the query through the embedding model, fetches the top-k most similar chunks from ChromaDB, and prints them with scores. Use this to:\n",
    "- Verify that relevant documents are in the index\n",
    "- Tune `top_k`\n",
    "- Compare different query phrasings\n",
    "- Identify retrieval gaps before blaming the LLM\n",
    "\n",
    "The **similarity score** is the L2 distance, range [0,4], lower = more similar. L2 distance is used becuase it works for any vectors, normalised or not. Cosine similarity only makes sense for direction (magnitude doesn't matter), so it requires that vectors be unit-length to be meaningful. L2 makes no such assumption, making it the safer general default. ChromaDB defaults to L2 because it's simpler to compute and works even if vector magnitudes vary. Since our embedding model always produces equal-length vectors, we get cosine-equivalent ranking. The score numbers look different, but the top-5 results would be identical either way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "p0-step3-code",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-02-20 15:43:06.665\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.embeddings.sentence_transformer\u001b[0m:\u001b[36mget_embeddings\u001b[0m:\u001b[36m76\u001b[0m - \u001b[34m\u001b[1msentence-transformers/all-MiniLM-L6-v2 embeddings size: (1, 384)\u001b[0m\n",
      "\u001b[32m2026-02-20 15:43:06.671\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msme_kt_zh_collaboration_rag.baseline_rag\u001b[0m:\u001b[36minspect_retrieval\u001b[0m:\u001b[36m274\u001b[0m - \u001b[1mRetrieval for query: 'What materials is the Logypal 1 pallet made from?'\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top-5 retrieved chunks (returned=5; showing a maximum of 1000 content characters):\n",
      "  [1] score=0.8810  file='3_EPD_pallet_relicyc.pdf'  title='# PRODUCT INFORMATION'\n",
      "       '# PRODUCT INFORMATION\\n\\nThis Environmental Product Declaration concerns the environmental\\n\\nimpacts associated with a model of recycled polypropylene pallet:\\n\\n Logypal 1 [®]\\n\\n All these pallets are produced with secondary raw materials\\n\\n(a mix of polypropylene and high density polyethylene).\\n\\nThese new plastic pallets are the real alternative to the ISPM-15\\n\\ntreated wooden pallet (HT standard phytosanitary treatment that\\n\\ncertifies the suitability of the material to the international regulations\\n\\ndrawn up by the IPPC), having a comparable cost, but without the\\n\\nbureaucracy and mandatory certifications for purchase.\\n\\nThese products are also light, resistant, washable and resistant to\\n\\nmold and humidity.\\n\\nThe main characteristics of the model of pallet under study are shown\\n\\nin the following table:\\n\\n\\n\\n_Table 1: Main characteristics of the studied pallet model_\\n\\n The packaging under study are intended to handling and transport\\n\\nof various kind of goods; for this reason falls into the catego'\n",
      "  [2] score=0.8873  file='3_EPD_pallet_relicyc.pdf'  title='# CONTENT DECLARATION'\n",
      "       '# CONTENT DECLARATION\\n\\nLogypal 1, classified as distribution packaging, is mainly composed (> 99%) of polyolefins and other\\n\\ntrace materials. The product under this study has a recycled plastic content of 100% and recycled\\n\\nmaterials are post-consumer plastic waste.\\n\\n\\n\\n\\n\\n_Table 3: Content declaration of pallet Logypal 1_\\n\\n The reported recycled content has been taken from certificates issued by the Kiwa certification\\n\\nbody (Accr. N.069B). For the product covered by the EPD, the reference certificate is Accr.\\n\\nNo. 021/2020 (the certificate dated 26-03-2023 shows a value of 100%, while the certificate dated\\n\\n04-02-2020 shows a value of 95%).\\n\\n |Col1|Col2|Col3|Col4|Col5|Col6|Col7| |---|---|---|---|---|---|---| |||||||| |||||||| |||||||| |||||||| |||||||| |||||||| |||||||15|'\n",
      "  [3] score=1.0462  file='2_EPD_pallet_CPR.pdf'  title='#### 3. Product information'\n",
      "       \"#### 3. Product information\\n\\nUN CPC code\\n\\n 36490 Other articles for the conveyance or packing of goods, of plastics; stoppers, lids, caps and other closures, of plastics.\\n\\n Product description\\n\\n The Noè pallet, identified by code PR12, is a product developed and patented by CPR SYSTEM and produced by Newpal S.p.A. It is used by the CPR SYSTEM Group as a support for the plastic folding crates managed within the company's pooling system.\\n\\n This four-way, non-reversible pallet consists of a platform and crosspieces connected by a male-female locking system on the blocks. It is equipped with a Smart Label traceability tag that includes an RFID TAG, an EAN128 barcode, and a Data Matrix, allowing monitoring throughout the entire logistics chain. Additionally, the product is recyclable at the end of its life, easily washable and sanitizable, resistant to rust, and stable in determining its tare weight.\\n\\n The pallet has been designed with containment edges and specific technical features to mi\"\n",
      "  [4] score=1.0733  file='2_EPD_pallet_CPR.pdf'  title='#### 5. Content declaration'\n",
      "       '#### 5. Content declaration\\n\\n MATERIAL % WEIGHT [kg]\\n\\n Polyolefins 65,3 15\\n\\n Micronized aluminium 7,3 1,7\\n\\n Cellulose 1,5 0,3\\n\\n Resin 15,4 3,5\\n\\n Glass fibres 9,4 2,2\\n\\n Additives and pigments 1,1 0,3\\n\\n Total 100 23\\n\\n The product is made up of 99% recycled material derived from pre-consumer and post-consumer materials.\\n\\n The product is free from any hazardous chemical substances as classified under Regulation (EC) No 1907/2006 (REACH) and Regulation (EC) No 1272/2008 (CLP).\\n\\n No packaging is used for the finished product.\\n\\n 8'\n",
      "  [5] score=1.2778  file='3_EPD_pallet_relicyc.pdf'  title='# 40 YEARS OF SUSTAINABLE INNOVATION'\n",
      "       '# 40 YEARS OF SUSTAINABLE INNOVATION\\n\\nRelicyc has a long history in managing end-of-life plastic\\n\\nand wooden pallets: from recovery to reintroduction into the\\n\\nmarketplace, it gives the material a new lease on life. Over 40\\n\\nyears of experience has led the company to become a prominent\\n\\nplayer in the field and a partner that today’s environmental efficient customers can rely on.\\n\\nThe need for sustainability is what drives our model, whose focus\\n\\nis on re-using resources at the end of their life and routing them\\n\\nproperly for recycling so they can find new uses while bringing the\\n\\nbusinesses involved new value.'\n"
     ]
    }
   ],
   "source": [
    "QUERY = \"What materials is the Logypal 1 pallet made from?\"\n",
    "\n",
    "results = await inspect_retrieval(QUERY, vector_store, embedding_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p0-step3-fail-md",
   "metadata": {},
   "source": [
    "### Retrieval for a Product Outside the Portfolio\n",
    "\n",
    "The PrimePack AG product catalog defines the portfolio boundary. The **Lara Pallet** is not in the catalog, it does not exist. Watch which chunks are returned and what scores they have. A **higher** minimum score (large L2 distance) signals *weaker semantic match*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "p0-step3-fail-code",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-02-20 15:49:19.544\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.embeddings.sentence_transformer\u001b[0m:\u001b[36mget_embeddings\u001b[0m:\u001b[36m76\u001b[0m - \u001b[34m\u001b[1msentence-transformers/all-MiniLM-L6-v2 embeddings size: (1, 384)\u001b[0m\n",
      "\u001b[32m2026-02-20 15:49:19.545\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msme_kt_zh_collaboration_rag.baseline_rag\u001b[0m:\u001b[36minspect_retrieval\u001b[0m:\u001b[36m274\u001b[0m - \u001b[1mRetrieval for query: 'What materials is the Lara pallet made from?'\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top-5 retrieved chunks (returned=5; showing a maximum of 1000 content characters):\n",
      "  [1] score=1.0161  file='3_EPD_pallet_relicyc.pdf'  title='# PRODUCT INFORMATION'\n",
      "       '# PRODUCT INFORMATION\\n\\nThis Environmental Product Declaration concerns the environmental\\n\\nimpacts associated with a model of recycled polypropylene pallet:\\n\\n Logypal 1 [®]\\n\\n All these pallets are produced with secondary raw materials\\n\\n(a mix of polypropylene and high density polyethylene).\\n\\nThese new plastic pallets are the real alternative to the ISPM-15\\n\\ntreated wooden pallet (HT standard phytosanitary treatment that\\n\\ncertifies the suitability of the material to the international regulations\\n\\ndrawn up by the IPPC), having a comparable cost, but without the\\n\\nbureaucracy and mandatory certifications for purchase.\\n\\nThese products are also light, resistant, washable and resistant to\\n\\nmold and humidity.\\n\\nThe main characteristics of the model of pallet under study are shown\\n\\nin the following table:\\n\\n\\n\\n_Table 1: Main characteristics of the studied pallet model_\\n\\n The packaging under study are intended to handling and transport\\n\\nof various kind of goods; for this reason falls into the catego'\n",
      "  [2] score=1.1324  file='3_EPD_pallet_relicyc.pdf'  title='# CONTENT DECLARATION'\n",
      "       '# CONTENT DECLARATION\\n\\nLogypal 1, classified as distribution packaging, is mainly composed (> 99%) of polyolefins and other\\n\\ntrace materials. The product under this study has a recycled plastic content of 100% and recycled\\n\\nmaterials are post-consumer plastic waste.\\n\\n\\n\\n\\n\\n_Table 3: Content declaration of pallet Logypal 1_\\n\\n The reported recycled content has been taken from certificates issued by the Kiwa certification\\n\\nbody (Accr. N.069B). For the product covered by the EPD, the reference certificate is Accr.\\n\\nNo. 021/2020 (the certificate dated 26-03-2023 shows a value of 100%, while the certificate dated\\n\\n04-02-2020 shows a value of 95%).\\n\\n |Col1|Col2|Col3|Col4|Col5|Col6|Col7| |---|---|---|---|---|---|---| |||||||| |||||||| |||||||| |||||||| |||||||| |||||||| |||||||15|'\n",
      "  [3] score=1.1894  file='2_EPD_pallet_CPR.pdf'  title='#### 5. Content declaration'\n",
      "       '#### 5. Content declaration\\n\\n MATERIAL % WEIGHT [kg]\\n\\n Polyolefins 65,3 15\\n\\n Micronized aluminium 7,3 1,7\\n\\n Cellulose 1,5 0,3\\n\\n Resin 15,4 3,5\\n\\n Glass fibres 9,4 2,2\\n\\n Additives and pigments 1,1 0,3\\n\\n Total 100 23\\n\\n The product is made up of 99% recycled material derived from pre-consumer and post-consumer materials.\\n\\n The product is free from any hazardous chemical substances as classified under Regulation (EC) No 1907/2006 (REACH) and Regulation (EC) No 1272/2008 (CLP).\\n\\n No packaging is used for the finished product.\\n\\n 8'\n",
      "  [4] score=1.2014  file='2_EPD_pallet_CPR.pdf'  title='#### 3. Product information'\n",
      "       \"#### 3. Product information\\n\\nUN CPC code\\n\\n 36490 Other articles for the conveyance or packing of goods, of plastics; stoppers, lids, caps and other closures, of plastics.\\n\\n Product description\\n\\n The Noè pallet, identified by code PR12, is a product developed and patented by CPR SYSTEM and produced by Newpal S.p.A. It is used by the CPR SYSTEM Group as a support for the plastic folding crates managed within the company's pooling system.\\n\\n This four-way, non-reversible pallet consists of a platform and crosspieces connected by a male-female locking system on the blocks. It is equipped with a Smart Label traceability tag that includes an RFID TAG, an EAN128 barcode, and a Data Matrix, allowing monitoring throughout the entire logistics chain. Additionally, the product is recyclable at the end of its life, easily washable and sanitizable, resistant to rust, and stable in determining its tare weight.\\n\\n The pallet has been designed with containment edges and specific technical features to mi\"\n",
      "  [5] score=1.3325  file='3_EPD_pallet_relicyc.pdf'  title='# 40 YEARS OF SUSTAINABLE INNOVATION'\n",
      "       '# 40 YEARS OF SUSTAINABLE INNOVATION\\n\\nRelicyc has a long history in managing end-of-life plastic\\n\\nand wooden pallets: from recovery to reintroduction into the\\n\\nmarketplace, it gives the material a new lease on life. Over 40\\n\\nyears of experience has led the company to become a prominent\\n\\nplayer in the field and a partner that today’s environmental efficient customers can rely on.\\n\\nThe need for sustainability is what drives our model, whose focus\\n\\nis on re-using resources at the end of their life and routing them\\n\\nproperly for recycling so they can find new uses while bringing the\\n\\nbusinesses involved new value.'\n"
     ]
    }
   ],
   "source": [
    "QUERY_OOK = \"What materials is the Lara pallet made from?\"\n",
    "\n",
    "results_ook = await inspect_retrieval(QUERY_OOK, vector_store, embedding_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p0-step3-fail-obs-md",
   "metadata": {},
   "source": [
    "> **Observation:** The retriever always returns the *closest* chunks it can find, it has no concept of \"no match\". For an unknown product the L2 distances are **higher** (the closest chunks are still about other pallets), but without a score-threshold guard the LLM receives those chunks anyway and may silently answer about the wrong product.\n",
    "> **Phase 3** shows how to combat this issue."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p0-step4-md",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 4: Build the RAG Agent\n",
    "\n",
    "`build_agent()` assembles the three components:\n",
    "\n",
    "```\n",
    "VectorStoreRetriever\n",
    "    └─ ChromaDBVectorStore (on disk, persists across runs)\n",
    "    └─ SentenceTransformerEmbeddings\n",
    "\n",
    "RAG Agent\n",
    "    ├─ LLM (Ollama / OpenAI / SDSC Qwen)\n",
    "    ├─ Retriever\n",
    "    └─ System prompt\n",
    "```\n",
    "\n",
    "### The System Prompt\n",
    "\n",
    "The system prompt is a very powerful lever for controlling LLM behaviour:\n",
    "\n",
    "```\n",
    "You are a helpful AI assistant specialised in sustainability and product compliance. Answer questions using the provided sources. If the information is not in the sources, say so clearly.\n",
    "```\n",
    "\n",
    "The instruction *\"If the information is not in the sources, say so clearly\"* should prevents hallucination about missing products and unverified claims."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "p0-step4-code",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-02-20 15:54:39.250\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msme_kt_zh_collaboration_rag.baseline_rag\u001b[0m:\u001b[36mbuild_llm\u001b[0m:\u001b[36m129\u001b[0m - \u001b[1mLLM backend: Ollama (mistral-nemo:12b)\u001b[0m\n",
      "\u001b[32m2026-02-20 15:54:39.277\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m60\u001b[0m - \u001b[34m\u001b[1mOllama LLM loaded: mistral-nemo:12b; temperature: 0.3; seed: 42; tools: None; response_format: None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:54:39.279\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msme_kt_zh_collaboration_rag.baseline_rag\u001b[0m:\u001b[36mbuild_agent\u001b[0m:\u001b[36m306\u001b[0m - \u001b[1mRAG agent ready (top_k=5  query_expansion=0)\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG agent assembled.\n"
     ]
    }
   ],
   "source": [
    "llm = build_llm(backend=BACKEND)\n",
    "agent = build_agent(\n",
    "    vector_store=vector_store,\n",
    "    embedding_model=embedding_model,\n",
    "    llm=llm,\n",
    "    top_k=RETRIEVER_TOP_K,\n",
    "    number_query_expansion=0,  # 0 = no expansion; will look at this more in feautre track 4\n",
    ")\n",
    "print(\"RAG agent assembled.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p0-step5-md",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 5: Ask a Question\n",
    "\n",
    "`ask()` sends the query to the agent and returns the answer string. The internal flow is:\n",
    "\n",
    "1. Embed the query\n",
    "2. Retrieve top-k chunks\n",
    "3. Build the prompt: `<system>` + `<sources>` XML block + user question\n",
    "4. Generate the answer with the LLM\n",
    "5. Return the answer and a list of cited source chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "p0-step5-code",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-02-20 15:55:02.149\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msme_kt_zh_collaboration_rag.baseline_rag\u001b[0m:\u001b[36mask\u001b[0m:\u001b[36m323\u001b[0m - \u001b[1mQuery: 'What materials is the Logypal 1 pallet made from?'\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:02.396\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.embeddings.sentence_transformer\u001b[0m:\u001b[36mget_embeddings\u001b[0m:\u001b[36m76\u001b[0m - \u001b[34m\u001b[1msentence-transformers/all-MiniLM-L6-v2 embeddings size: (1, 384)\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:27.865\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:27.844893Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='Based', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:27.919\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:27.915926Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' on', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:27.995\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:27.994616Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' the', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:28.067\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:28.067512Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' provided', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:28.140\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:28.14046Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' sources', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:28.212\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:28.212184Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=':\\n\\n', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:28.284\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:28.283785Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='1', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:28.355\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:28.354827Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='.', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:28.430\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:28.429781Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' **', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:28.507\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:28.507029Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='Main', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:28.588\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:28.58791Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' Material', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:28.662\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:28.662462Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='**:', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:28.736\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:28.735616Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' The', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:28.807\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:28.806642Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' Log', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:28.879\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:28.878912Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='yp', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:28.949\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:28.948814Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='al', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:29.023\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:29.023Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' ', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:29.099\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:29.098713Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='1', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:29.175\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:29.175023Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' pal', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:29.250\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:29.250067Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='let', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:29.321\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:29.320891Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' is', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:29.391\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:29.391339Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' primarily', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:29.463\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:29.462999Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' made', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:29.534\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:29.533861Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' from', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:29.604\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:29.604303Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' recycled', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:29.675\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:29.674591Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' polypropylene', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:29.746\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:29.746354Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' (', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:29.817\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:29.816555Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='PP', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:29.888\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:29.887656Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='),', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:29.958\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:29.957889Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' with', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:30.028\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:30.027941Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' some', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:30.099\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:30.099037Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' high', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:30.170\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:30.16972Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='-density', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:30.241\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:30.240851Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' polyethylene', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:30.312\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:30.311669Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' (', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:30.384\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:30.38453Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='HD', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:30.461\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:30.461009Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='PE', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:30.538\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:30.538708Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=').', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:30.620\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:30.620423Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' This', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:30.703\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:30.703419Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' is', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:30.780\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:30.78074Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' confirmed', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:30.860\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:30.859595Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' by', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:30.936\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:30.936611Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' both', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:31.013\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:31.012811Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' Source', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:31.089\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:31.088971Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' ', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:31.175\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:31.175075Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='1', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:31.257\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:31.257596Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' and', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:31.337\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:31.337455Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' Source', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:31.416\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:31.416436Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' ', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:31.493\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:31.492843Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='2', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:31.568\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:31.56806Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='.\\n', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:31.640\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:31.640553Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='  ', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:31.711\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:31.710887Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' -', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:31.796\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:31.795157Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' Source', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:31.871\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:31.871673Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' ', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:31.945\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:31.944768Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='1', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:32.015\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:32.015304Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=':', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:32.085\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:32.085208Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' \"', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:32.156\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:32.156348Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='All', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:32.229\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:32.228517Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' these', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:32.304\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:32.303933Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' pal', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:32.378\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:32.377964Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='lets', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:32.453\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:32.452883Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' are', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:32.526\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:32.5266Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' produced', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:32.597\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:32.597396Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' with', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:32.668\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:32.668073Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' secondary', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:32.741\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:32.74121Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' raw', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:32.816\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:32.816524Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' materials', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:32.891\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:32.891321Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' (', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:32.965\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:32.964587Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='a', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:33.036\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:33.035743Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' mix', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:33.111\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:33.111391Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' of', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:33.186\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:33.185957Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' polypropylene', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:33.258\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:33.258286Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' and', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:33.329\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:33.329079Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' high', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:33.400\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:33.399695Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' density', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:33.470\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:33.470362Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' polyethylene', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:33.541\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:33.541007Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=').', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:33.611\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:33.611248Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='\"\\n', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:33.682\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:33.68153Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='  ', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:33.752\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:33.751585Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' -', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:33.822\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:33.821802Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' Source', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:33.892\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:33.892082Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' ', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:33.962\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:33.962313Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='2', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:34.033\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:34.03286Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=':', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:34.103\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:34.10296Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' \"', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:34.173\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:34.173236Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='The', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:34.245\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:34.244654Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' product', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:34.320\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:34.320483Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' under', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:34.393\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:34.392725Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' this', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:34.462\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:34.462546Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' study', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:34.536\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:34.535717Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' has', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:34.612\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:34.61184Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' a', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:34.686\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:34.686506Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' recycled', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:34.764\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:34.763984Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' plastic', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:34.840\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:34.839846Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' content', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:34.911\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:34.911643Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' of', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:34.982\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:34.981905Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' ', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:35.053\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:35.053191Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='1', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:35.124\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:35.12415Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='0', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:35.195\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:35.194651Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='0', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:35.265\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:35.265248Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='%', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:35.349\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:35.335843Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' [...]', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:35.413\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:35.412877Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' mainly', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:35.486\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:35.486064Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' composed', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:35.562\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:35.562492Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' (>', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:35.638\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:35.638585Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' ', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:35.715\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:35.715317Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='9', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:35.790\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:35.790615Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='9', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:35.867\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:35.867244Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='%)', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:35.943\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:35.943622Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' of', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:36.014\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:36.014173Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' poly', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:36.086\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:36.085985Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='ole', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:36.157\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:36.157316Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='f', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:36.229\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:36.228521Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='ins', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:36.299\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:36.299043Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='.\"\\n\\n', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:36.370\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:36.370415Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='2', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:36.441\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:36.441004Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='.', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:36.512\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:36.511679Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' **', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:36.583\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:36.582672Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='Rec', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:36.654\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:36.654295Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='ycled', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:36.725\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:36.725439Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' Content', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:36.799\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:36.799654Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='**:', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:36.875\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:36.875591Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' Both', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:36.954\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:36.953773Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' sources', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:37.030\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:37.029812Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' mention', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:37.102\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:37.102283Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' that', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:37.173\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:37.17272Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' the', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:37.247\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:37.247088Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' Log', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:37.324\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:37.324182Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='yp', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:37.404\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:37.403814Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='al', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:37.484\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:37.483694Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' ', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:37.562\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:37.561768Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='1', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:37.636\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:37.635833Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' pal', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:37.707\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:37.707629Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='let', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:37.780\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:37.779945Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' is', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:37.854\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:37.854179Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' made', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:37.930\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:37.929655Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' from', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:38.011\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:38.011048Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' recycled', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:38.086\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:38.085942Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' materials', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:38.160\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:38.160434Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='.\\n', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:38.234\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:38.233941Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='  ', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:38.305\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:38.304764Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' -', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:38.376\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:38.375756Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' Source', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:38.446\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:38.446452Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' ', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:38.519\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:38.519083Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='1', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:38.590\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:38.59013Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=':', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:38.662\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:38.661625Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' \"', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:38.733\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:38.732967Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='These', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:38.807\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:38.806927Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' products', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:38.881\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:38.880871Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' are', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:38.954\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:38.954066Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' also', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:39.026\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:39.025822Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' fully', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:39.097\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:39.09731Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' rec', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:39.169\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:39.169584Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='ycl', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:39.245\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:39.245168Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='able', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:39.324\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:39.323999Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' packaging', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:39.397\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:39.397232Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' [...', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:39.470\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:39.470176Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='].', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:39.543\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:39.543253Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='\"\\n', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:39.616\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:39.616481Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='  ', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:39.688\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:39.688486Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' -', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:39.766\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:39.765933Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' Source', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:39.839\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:39.839162Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' ', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:39.913\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:39.913374Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='2', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:39.984\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:39.984512Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=':', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:40.057\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:40.05772Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' \"', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:40.129\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:40.129581Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='The', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:40.201\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:40.201528Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' product', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:40.274\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:40.274244Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' under', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:40.348\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:40.34822Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' this', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:40.421\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:40.421561Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' study', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:40.496\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:40.495847Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' has', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:40.568\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:40.567692Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' a', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:40.642\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:40.642219Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' recycled', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:40.714\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:40.714555Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' plastic', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:40.787\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:40.78745Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' content', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:40.859\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:40.859534Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' of', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:40.932\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:40.931699Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' ', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:41.004\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:41.004616Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='1', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:41.078\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:41.078341Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='0', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:41.152\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:41.151905Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='0', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:41.227\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:41.226297Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='%', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:41.304\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:41.304321Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' and', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:41.377\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:41.377263Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' recycled', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:41.450\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:41.450073Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' materials', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:41.523\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:41.522936Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' are', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:41.611\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:41.61117Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' post', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:41.700\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:41.699904Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='-consum', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:41.780\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:41.779707Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='er', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:41.852\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:41.852301Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' plastic', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:41.925\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:41.924997Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' waste', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:41.996\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:41.996441Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='.\"\\n\\n', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:42.068\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:42.067948Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='3', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:42.140\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:42.139952Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='.', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:42.213\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:42.212778Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' **', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:42.285\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:42.285436Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='Other', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:42.359\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:42.359041Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' Materials', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:42.432\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:42.431599Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='**:', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:42.505\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:42.505262Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' While', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:42.579\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:42.578808Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' not', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:42.651\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:42.650835Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' the', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:42.723\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:42.722915Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' main', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:42.795\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:42.795703Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' components', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:42.869\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:42.869109Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=',', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:42.942\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:42.94176Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' other', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:43.014\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:43.014013Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' materials', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:43.085\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:43.085242Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' may', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:43.157\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:43.15772Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' include', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:43.230\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:43.230631Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' micron', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:43.302\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:43.302605Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='ized', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:43.374\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:43.374549Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' aluminum', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:43.447\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:43.447271Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=',', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:43.519\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:43.519276Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' cellulose', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:43.592\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:43.592556Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=',', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:43.664\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:43.664659Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' resin', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:43.737\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:43.736975Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=',', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:43.812\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:43.81219Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' glass', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:43.885\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:43.885413Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' fibers', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:43.958\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:43.958623Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=',', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:44.030\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:44.029832Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' additives', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:44.102\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:44.101653Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=',', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:44.173\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:44.173516Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' and', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:44.251\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:44.250845Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' pigments', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:44.329\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:44.329201Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' (', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:44.401\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:44.401336Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='Source', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:44.475\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:44.475552Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' ', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:44.547\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:44.547377Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='5', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:44.619\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:44.619208Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=').', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:44.692\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:44.691627Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' However', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:44.764\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:44.763976Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=',', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:44.836\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:44.836075Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' these', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:44.908\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:44.907977Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' make', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:44.980\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:44.980137Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' up', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:45.051\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:45.051614Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' less', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:45.124\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:45.124473Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' than', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:45.196\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:45.196291Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' ', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:45.268\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:45.26757Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='2', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:45.340\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:45.340676Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='0', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:45.415\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:45.414821Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='%', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:45.487\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:45.487043Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' of', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:45.558\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:45.558413Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' the', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:45.630\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:45.630107Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' total', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:45.702\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:45.702451Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' weight', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:45.775\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:45.775576Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='.\\n\\n', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:45.847\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:45.8468Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='4', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:45.918\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:45.917856Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='.', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:45.989\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:45.989196Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' **', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:46.060\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:46.060249Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='Man', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:46.132\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:46.132373Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='ufacturer', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:46.205\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:46.204787Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='**:', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:46.277\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:46.27684Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' The', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:46.348\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:46.348097Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' Log', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:46.421\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:46.420774Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='yp', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:46.492\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:46.492359Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='al', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:46.564\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:46.564668Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' ', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:46.637\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:46.637062Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='1', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:46.709\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:46.709151Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' pal', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:46.781\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:46.781209Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='let', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:46.852\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:46.852681Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' is', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:46.924\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:46.92417Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' produced', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:46.996\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:46.995895Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' by', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:47.068\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:47.068198Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' Rel', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:47.140\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:47.140305Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='icy', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:47.217\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:47.217563Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='c', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:47.289\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:47.289374Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' (', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:47.363\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:47.362893Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='Source', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:47.434\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:47.434446Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' ', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:47.506\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:47.506203Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='5', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:47.580\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:47.580557Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=').\\n\\n', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:47.655\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:47.654974Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='Sources', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:47.729\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:47.728947Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=':\\n', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:47.802\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:47.801799Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='-', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:47.874\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:47.873692Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' Source', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:47.945\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:47.945318Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' ', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:48.016\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:48.016334Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='1', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:48.088\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:48.088152Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=':', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:48.160\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:48.16044Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' <', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:48.233\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:48.233216Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='https', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:48.305\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:48.304808Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='://', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:48.377\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:48.376747Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='www', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:48.448\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:48.448646Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='.e', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:48.521\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:48.520805Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='com', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:48.593\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:48.592751Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='at', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:48.664\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:48.664526Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='ters', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:48.735\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:48.73562Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='.nl', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:48.809\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:48.808897Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='/wp', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:48.882\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:48.882328Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='-content', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:48.956\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:48.955726Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='/uploads', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:49.028\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:49.027932Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='/', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:49.099\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:49.099378Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='2', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:49.171\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:49.170903Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='0', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:49.243\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:49.242714Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='2', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:49.314\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:49.314523Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='1', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:49.386\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:49.386173Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='/', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:49.458\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:49.457924Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='0', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:49.531\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:49.530807Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='3', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:49.602\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:49.602508Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='/E', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:49.676\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:49.676138Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='PD', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:49.748\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:49.748276Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='_', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:49.826\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:49.825804Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='Log', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:49.898\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:49.898064Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='yp', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:49.970\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:49.96976Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='al', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:50.041\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:50.040944Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='_', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:50.112\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:50.112601Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='1', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:50.185\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:50.184951Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='.pdf', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:50.258\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:50.25849Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='>\\n', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:50.332\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:50.33204Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='-', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:50.403\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:50.403251Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' Source', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:50.475\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:50.474803Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' ', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:50.547\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:50.547003Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='2', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:50.619\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:50.619027Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=':', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:50.691\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:50.690583Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' <', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:50.763\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:50.763273Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='http', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:50.836\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:50.83648Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='://', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:50.908\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:50.907971Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='ki', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:50.980\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:50.980576Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='wa', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:51.052\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:51.05262Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='.co', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:51.125\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:51.12517Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='.uk', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:51.197\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:51.197545Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='/c', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:51.269\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:51.269481Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='ert', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:51.342\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:51.341701Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='ification', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:51.413\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:51.413238Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='/', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:51.486\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:51.486329Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='environment', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:51.559\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:51.558928Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='al', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:51.630\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:51.630336Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='-product', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:51.702\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:51.702588Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='-de', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:51.775\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:51.774858Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='claration', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:51.848\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:51.848172Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='-', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:51.920\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:51.91995Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='ep', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:51.996\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:51.996379Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='d', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:52.070\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:52.070663Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='/>\\n', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:52.144\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:52.143886Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='-', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:52.215\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:52.215226Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' Source', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:52.292\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:52.292505Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' ', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:52.364\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:52.364089Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='5', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:52.435\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:52.435099Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=':', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:52.506\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:52.506367Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' <', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:52.578\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:52.577749Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='https', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:52.649\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:52.649064Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='://', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:52.720\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:52.720282Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='www', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:52.793\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:52.792802Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='.c', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:52.865\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:52.864821Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='pr', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:52.936\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:52.936667Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='system', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:53.009\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:53.009075Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='.com', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:53.081\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:53.081255Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='/en', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:53.154\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:53.153874Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='/products', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:53.226\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:53.22624Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='/no', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:53.299\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:53.298594Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='e', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:53.371\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:53.370675Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='-p', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:53.444\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:53.444542Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='allet', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:53.517\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:53.517587Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='/>', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:53.593\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T14:55:53.592664Z' done=True done_reason='stop' total_duration=51165994000 load_duration=5999646250 prompt_eval_count=1783 prompt_eval_duration=19382753458 eval_count=353 eval_duration=25648904114 message=Message(role='assistant', content='', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:53.598\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msme_kt_zh_collaboration_rag.baseline_rag\u001b[0m:\u001b[36mask\u001b[0m:\u001b[36m326\u001b[0m - \u001b[1mAnswer:\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:53.599\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msme_kt_zh_collaboration_rag.baseline_rag\u001b[0m:\u001b[36mask\u001b[0m:\u001b[36m327\u001b[0m - \u001b[1m  Based on the provided sources:\n",
      "\n",
      "1. **Main Material**: The Logypal 1 pallet is primarily made from recycled polypropylene (PP), with some high-density polyethylene (HDPE). This is confirmed by both Source 1 and Source 2.\n",
      "   - Source 1: \"All these pallets are produced with secondary raw materials (a mix of polypropylene and high density polyethylene).\"\n",
      "   - Source 2: \"The product under this study has a recycled plastic content of 100% [...] mainly composed (> 99%) of polyolefins.\"\n",
      "\n",
      "2. **Recycled Content**: Both sources mention that the Logypal 1 pallet is made from recycled materials.\n",
      "   - Source 1: \"These products are also fully recyclable packaging [...].\"\n",
      "   - Source 2: \"The product under this study has a recycled plastic content of 100% and recycled materials are post-consumer plastic waste.\"\n",
      "\n",
      "3. **Other Materials**: While not the main components, other materials may include micronized aluminum, cellulose, resin, glass fibers, additives, and pigments (Source 5). However, these make up less than 20% of the total weight.\n",
      "\n",
      "4. **Manufacturer**: The Logypal 1 pallet is produced by Relicyc (Source 5).\n",
      "\n",
      "Sources:\n",
      "- Source 1: <https://www.ecomatters.nl/wp-content/uploads/2021/03/EPD_Logypal_1.pdf>\n",
      "- Source 2: <http://kiwa.co.uk/certification/environmental-product-declaration-epd/>\n",
      "- Source 5: <https://www.cprsystem.com/en/products/noe-pallet/>\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:53.599\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msme_kt_zh_collaboration_rag.baseline_rag\u001b[0m:\u001b[36mask\u001b[0m:\u001b[36m328\u001b[0m - \u001b[1mSources (5):\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:53.599\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msme_kt_zh_collaboration_rag.baseline_rag\u001b[0m:\u001b[36mask\u001b[0m:\u001b[36m331\u001b[0m - \u001b[1m  '3_EPD_pallet_relicyc.pdf'  |  '# PRODUCT INFORMATION'\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:53.600\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msme_kt_zh_collaboration_rag.baseline_rag\u001b[0m:\u001b[36mask\u001b[0m:\u001b[36m331\u001b[0m - \u001b[1m  '3_EPD_pallet_relicyc.pdf'  |  '# CONTENT DECLARATION'\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:53.600\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msme_kt_zh_collaboration_rag.baseline_rag\u001b[0m:\u001b[36mask\u001b[0m:\u001b[36m331\u001b[0m - \u001b[1m  '2_EPD_pallet_CPR.pdf'  |  '#### 3. Product information'\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:53.600\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msme_kt_zh_collaboration_rag.baseline_rag\u001b[0m:\u001b[36mask\u001b[0m:\u001b[36m331\u001b[0m - \u001b[1m  '2_EPD_pallet_CPR.pdf'  |  '#### 5. Content declaration'\u001b[0m\n",
      "\u001b[32m2026-02-20 15:55:53.601\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msme_kt_zh_collaboration_rag.baseline_rag\u001b[0m:\u001b[36mask\u001b[0m:\u001b[36m331\u001b[0m - \u001b[1m  '3_EPD_pallet_relicyc.pdf'  |  '# 40 YEARS OF SUSTAINABLE INNOVATION'\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Answer ===\n",
      "Based on the provided sources:\n",
      "\n",
      "1. **Main Material**: The Logypal 1 pallet is primarily made from recycled polypropylene (PP), with some high-density polyethylene (HDPE). This is confirmed by both Source 1 and Source 2.\n",
      "   - Source 1: \"All these pallets are produced with secondary raw materials (a mix of polypropylene and high density polyethylene).\"\n",
      "   - Source 2: \"The product under this study has a recycled plastic content of 100% [...] mainly composed (> 99%) of polyolefins.\"\n",
      "\n",
      "2. **Recycled Content**: Both sources mention that the Logypal 1 pallet is made from recycled materials.\n",
      "   - Source 1: \"These products are also fully recyclable packaging [...].\"\n",
      "   - Source 2: \"The product under this study has a recycled plastic content of 100% and recycled materials are post-consumer plastic waste.\"\n",
      "\n",
      "3. **Other Materials**: While not the main components, other materials may include micronized aluminum, cellulose, resin, glass fibers, additives, and pigments (Source 5). However, these make up less than 20% of the total weight.\n",
      "\n",
      "4. **Manufacturer**: The Logypal 1 pallet is produced by Relicyc (Source 5).\n",
      "\n",
      "Sources:\n",
      "- Source 1: <https://www.ecomatters.nl/wp-content/uploads/2021/03/EPD_Logypal_1.pdf>\n",
      "- Source 2: <http://kiwa.co.uk/certification/environmental-product-declaration-epd/>\n",
      "- Source 5: <https://www.cprsystem.com/en/products/noe-pallet/>\n"
     ]
    }
   ],
   "source": [
    "QUERY = \"What materials is the Logypal 1 pallet made from?\"\n",
    "\n",
    "answer = await ask(agent, QUERY)\n",
    "print(\"\\n--------------- Answer --------------- \")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p0-queries-md",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Probing Failure Modes\n",
    "\n",
    "The corpus was designed with three deliberate challenges. Run the queries below and observe the answers.\n",
    "\n",
    "### 4a: Out-of-Portfolio Query\n",
    "\n",
    "The **Lara Pallet** does not exist. A good RAG must say so instead of describing a different pallet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "p0-query-ook-code",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-02-20 16:02:52.260\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msme_kt_zh_collaboration_rag.baseline_rag\u001b[0m:\u001b[36mask\u001b[0m:\u001b[36m323\u001b[0m - \u001b[1mQuery: 'What materials is the Lara pallet made from?'\u001b[0m\n",
      "\u001b[32m2026-02-20 16:02:52.411\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.embeddings.sentence_transformer\u001b[0m:\u001b[36mget_embeddings\u001b[0m:\u001b[36m76\u001b[0m - \u001b[34m\u001b[1msentence-transformers/all-MiniLM-L6-v2 embeddings size: (1, 384)\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:16.585\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:16.579046Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='Based', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:16.649\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:16.649162Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' on', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:16.721\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:16.720988Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' the', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:16.794\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:16.794248Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' sources', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:16.867\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:16.866962Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' provided', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:16.939\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:16.938981Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=',', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:17.013\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:17.012854Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' here', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:17.086\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:17.086153Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=\"'s\", thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:17.160\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:17.15984Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' what', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:17.233\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:17.23293Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' we', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:17.305\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:17.305145Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' know', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:17.383\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:17.382689Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' about', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:17.458\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:17.458272Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' the', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:17.545\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:17.545562Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' materials', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:17.630\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:17.630081Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' used', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:17.715\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:17.714767Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' to', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:17.791\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:17.791092Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' make', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:17.864\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:17.864375Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' the', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:17.937\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:17.937332Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' Lara', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:18.012\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:18.012411Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' pal', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:18.086\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:18.086369Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='let', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:18.163\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:18.16303Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=':\\n\\n', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:18.237\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:18.23691Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='1', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:18.311\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:18.311122Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='.', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:18.386\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:18.386619Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' **', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:18.461\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:18.461461Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='Primary', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:18.536\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:18.536645Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' Material', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:18.611\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:18.611623Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='**:\\n', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:18.685\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:18.685376Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='  ', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:18.757\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:18.756739Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' -', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:18.829\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:18.828646Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' The', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:18.901\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:18.90074Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' Lara', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:18.973\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:18.972857Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' pal', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:19.044\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:19.043852Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='let', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:19.115\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:19.114767Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' is', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:19.186\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:19.18611Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' made', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:19.257\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:19.257383Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' primarily', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:19.331\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:19.331108Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' from', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:19.405\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:19.404878Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' plastic', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:19.476\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:19.476372Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='.\\n', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:19.549\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:19.549439Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='  ', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:19.622\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:19.62174Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' -', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:19.692\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:19.692596Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' It', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:19.764\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:19.764305Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' is', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:19.835\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:19.835278Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' composed', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:19.907\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:19.906757Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' of', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:19.978\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:19.97799Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' greater', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:20.049\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:20.04907Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' than', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:20.121\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:20.12108Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' ', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:20.196\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:20.196416Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='9', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:20.270\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:20.27042Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='9', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:20.344\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:20.34374Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='%', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:20.415\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:20.414777Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' poly', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:20.486\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:20.486081Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='ole', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:20.559\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:20.558933Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='f', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:20.631\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:20.63127Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='ins', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:20.703\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:20.703079Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' and', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:20.784\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:20.782846Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' other', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:20.861\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:20.860841Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' trace', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:20.933\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:20.932709Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' materials', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:21.005\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:21.004884Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' (', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:21.079\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:21.079094Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='Source', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:21.152\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:21.151989Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=':', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:21.224\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:21.224031Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=\" '\", thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:21.295\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:21.29488Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='a', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:21.366\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:21.36592Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='7', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:21.437\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:21.436832Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='3', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:21.508\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:21.507879Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='3', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:21.582\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:21.581738Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='b', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:21.653\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:21.652867Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='8', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:21.723\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:21.723589Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='4', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:21.796\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:21.796424Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='8', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:21.867\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:21.867565Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='-', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:21.939\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:21.939125Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='ef', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:22.016\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:22.016384Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='4', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:22.090\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:22.090526Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='7', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:22.163\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:22.163198Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='-', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:22.234\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:22.233836Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='4', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:22.305\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:22.30469Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='6', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:22.377\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:22.377232Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='9', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:22.448\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:22.448372Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='0', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:22.520\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:22.519759Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='-b', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:22.591\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:22.59104Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='d', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:22.662\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:22.661947Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='0', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:22.738\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:22.738643Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='c', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:22.810\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:22.81062Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='-c', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:22.885\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:22.885597Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='f', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:22.959\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:22.958835Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='4', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:23.030\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:23.030059Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='1', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:23.101\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:23.100895Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='a', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:23.172\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:23.172582Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='1', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:23.243\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:23.24357Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='3', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:23.315\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:23.314628Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='4', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:23.387\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:23.387571Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='abc', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:23.460\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:23.460438Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='3', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:23.532\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:23.531721Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=\"'\", thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:23.603\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:23.603236Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=').\\n\\n', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:23.674\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:23.674545Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='2', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:23.746\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:23.745859Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='.', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:23.817\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:23.816929Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' **', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:23.888\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:23.88792Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='Rec', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:23.960\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:23.959961Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='ycled', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:24.032\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:24.031712Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' Content', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:24.103\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:24.103047Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='**:\\n', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:24.176\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:24.176206Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='  ', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:24.247\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:24.247137Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' -', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:24.318\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:24.318235Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' The', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:24.392\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:24.391802Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' pal', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:24.463\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:24.463212Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='let', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:24.534\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:24.534379Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' has', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:24.606\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:24.60579Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' a', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:24.684\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:24.684082Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' recycled', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:24.755\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:24.755177Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' plastic', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:24.826\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:24.826115Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' content', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:24.897\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:24.897039Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' of', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:24.968\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:24.968531Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' ', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:25.040\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:25.039632Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='1', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:25.111\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:25.111213Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='0', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:25.183\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:25.183376Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='0', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:25.254\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:25.253824Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='%', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:25.327\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:25.327193Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='.\\n', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:25.405\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:25.405165Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='  ', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:25.478\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:25.478473Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' -', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:25.555\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:25.55513Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' It', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:25.631\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:25.631308Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' is', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:25.707\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:25.706854Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' made', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:25.779\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:25.779707Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' up', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:25.851\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:25.850676Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' of', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:25.921\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:25.921504Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' post', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:25.993\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:25.993082Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='-consum', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:26.064\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:26.064517Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='er', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:26.137\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:26.136955Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' plastic', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:26.208\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:26.20786Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' waste', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:26.279\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:26.278918Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' (', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:26.351\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:26.350907Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='Source', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:26.423\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:26.423695Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=':', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:26.496\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:26.496068Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=\" '\", thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:26.568\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:26.568378Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='a', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:26.641\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:26.640945Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='7', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:26.713\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:26.713224Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='3', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:26.786\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:26.785726Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='3', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:26.859\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:26.859422Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='b', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:26.932\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:26.931589Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='8', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:27.005\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:27.004739Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='4', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:27.078\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:27.078595Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='8', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:27.151\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:27.151555Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='-', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:27.223\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:27.223352Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='ef', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:27.296\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:27.295703Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='4', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:27.368\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:27.368251Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='7', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:27.441\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:27.440794Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='-', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:27.515\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:27.51533Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='4', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:27.592\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:27.591729Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='6', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:27.670\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:27.669401Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='9', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:27.757\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:27.757207Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='0', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:27.841\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:27.841529Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='-b', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:27.922\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:27.922347Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='d', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:28.001\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:28.000708Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='0', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:28.075\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:28.075028Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='c', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:28.149\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:28.149146Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='-c', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:28.224\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:28.223795Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='f', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:28.298\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:28.298265Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='4', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:28.373\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:28.373381Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='1', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:28.450\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:28.450221Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='a', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:28.525\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:28.524848Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='1', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:28.601\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:28.601132Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='3', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:28.675\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:28.674958Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='4', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:28.754\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:28.754524Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='abc', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:28.833\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:28.833021Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='3', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:28.909\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:28.909111Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=\"'\", thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:28.986\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:28.986522Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=').\\n', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:29.063\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:29.062898Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='  ', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:29.138\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:29.137992Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' -', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:29.212\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:29.211729Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' The', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:29.283\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:29.283102Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' pal', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:29.355\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:29.355192Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='let', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:29.428\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:29.427951Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' is', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:29.504\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:29.503621Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' free', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:29.589\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:29.588799Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' from', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:29.670\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:29.670461Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' hazardous', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:29.748\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:29.748428Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' chemical', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:29.820\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:29.820542Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' substances', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:29.895\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:29.895568Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' as', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:29.968\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:29.968599Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' classified', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:30.041\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:30.040619Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' under', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:30.115\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:30.114834Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' RE', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:30.187\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:30.186918Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='ACH', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:30.259\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:30.25942Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' and', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:30.333\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:30.332972Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' CL', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:30.409\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:30.408648Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='P', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:30.481\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:30.481248Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' regulations', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:30.553\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:30.553283Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' (', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:30.628\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:30.628342Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='Source', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:30.701\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:30.700913Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=':', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:30.773\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:30.773283Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=\" '\", thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:30.846\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:30.845728Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='6', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:30.917\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:30.917451Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='b', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:30.990\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:30.990046Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='2', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:31.061\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:31.061515Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='8', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:31.133\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:31.132902Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='b', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:31.205\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:31.205255Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='9', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:31.278\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:31.277781Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='fa', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:31.351\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:31.350657Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='-d', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:31.422\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:31.422469Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='7', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:31.495\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:31.494786Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='8', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:31.568\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:31.568347Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='d', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:31.640\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:31.640148Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='-', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:31.711\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:31.711535Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='4', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:31.783\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:31.783173Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='9', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:31.856\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:31.856055Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='f', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:31.932\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:31.932185Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='0', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:32.006\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:32.005986Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='-a', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:32.079\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:32.079362Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='0', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:32.151\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:32.150778Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='5', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:32.222\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:32.222458Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='6', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:32.294\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:32.294318Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='-', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:32.370\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:32.369522Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='6', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:32.445\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:32.445516Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='d', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:32.520\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:32.520227Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='1', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:32.593\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:32.593013Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='1', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:32.665\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:32.665125Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='7', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:32.737\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:32.73736Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='4', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:32.810\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:32.81032Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='8', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:32.882\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:32.881837Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='a', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:32.953\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:32.953184Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='6', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:33.024\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:33.024496Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='ed', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:33.096\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:33.096143Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='0', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:33.167\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:33.167511Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=\"'\", thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:33.240\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:33.240214Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=').\\n\\n', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:33.312\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:33.311967Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='3', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:33.384\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:33.383659Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='.', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:33.455\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:33.455482Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' **', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:33.529\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:33.529213Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='Additional', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:33.600\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:33.600737Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' Materials', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:33.673\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:33.672707Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='**:\\n', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:33.744\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:33.743889Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='  ', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:33.815\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:33.815171Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' -', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:33.887\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:33.88704Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' Some', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:33.958\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:33.958032Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' sources', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:34.029\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:34.02935Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' mention', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:34.103\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:34.102944Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' additional', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:34.176\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:34.176019Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' materials', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:34.249\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:34.249026Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' used', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:34.321\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:34.320799Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' in', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:34.394\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:34.394073Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' the', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:34.468\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:34.467783Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' production', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:34.540\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:34.539667Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' of', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:34.615\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:34.614669Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' similar', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:34.693\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:34.693125Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' pal', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:34.765\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:34.765466Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='lets', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:34.837\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:34.837455Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=',', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:34.909\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:34.909281Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' such', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:34.982\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:34.982515Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' as', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:35.054\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:35.05409Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=':\\n', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:35.126\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:35.125739Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='    ', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:35.197\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:35.197392Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' -', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:35.269\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:35.26951Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' Mic', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:35.341\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:35.341148Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='ron', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:35.415\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:35.415556Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='ized', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:35.488\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:35.488659Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' aluminum', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:35.560\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:35.560406Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' and', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:35.632\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:35.631925Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' cellulose', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:35.705\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:35.704763Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' (', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:35.776\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:35.776528Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='Source', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:35.848\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:35.847859Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=':', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:35.919\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:35.919272Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=\" '\", thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:35.991\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:35.990908Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='6', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:36.063\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:36.062593Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='b', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:36.134\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:36.133931Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='2', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:36.205\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:36.20545Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='8', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:36.277\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:36.277372Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='b', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:36.351\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:36.35153Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='9', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:36.428\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:36.428232Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='fa', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:36.502\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:36.502434Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='-d', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:36.576\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:36.576092Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='7', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:36.649\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:36.649243Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='8', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:36.721\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:36.721497Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='d', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:36.793\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:36.792902Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='-', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:36.864\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:36.864177Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='4', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:36.935\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:36.935368Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='9', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:37.007\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:37.006841Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='f', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:37.078\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:37.078254Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='0', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:37.151\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:37.150755Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='-a', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:37.222\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:37.222431Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='0', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:37.293\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:37.293585Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='5', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:37.365\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:37.365038Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='6', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:37.438\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:37.438009Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='-', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:37.511\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:37.51088Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='6', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:37.583\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:37.582953Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='d', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:37.654\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:37.654291Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='1', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:37.726\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:37.726086Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='1', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:37.798\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:37.797683Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='7', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:37.870\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:37.87041Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='4', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:37.942\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:37.942494Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='8', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:38.014\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:38.014187Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='a', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:38.086\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:38.085633Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='6', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:38.157\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:38.157138Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='ed', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:38.229\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:38.228871Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='0', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:38.300\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:38.300233Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=\"'\", thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:38.372\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:38.371661Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=').\\n', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:38.445\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:38.444878Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='    ', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:38.518\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:38.518166Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' -', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:38.593\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:38.592961Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' Glass', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:38.666\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:38.66633Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' fibers', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:38.738\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:38.737959Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' and', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:38.809\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:38.809589Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' additives', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:38.881\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:38.881159Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='/p', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:38.953\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:38.953417Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='ig', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:39.026\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:39.026013Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='ments', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:39.099\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:39.098898Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' (', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:39.177\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:39.176739Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='Source', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:39.248\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:39.248505Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=':', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:39.321\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:39.320979Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=\" '\", thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:39.393\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:39.392754Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='6', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:39.464\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:39.464193Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='b', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:39.535\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:39.535586Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='2', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:39.607\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:39.60691Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='8', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:39.678\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:39.67815Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='b', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:39.749\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:39.749647Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='9', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:39.821\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:39.820982Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='fa', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:39.892\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:39.892368Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='-d', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:39.964\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:39.964214Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='7', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:40.037\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:40.03683Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='8', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:40.108\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:40.108182Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='d', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:40.180\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:40.17962Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='-', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:40.251\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:40.251179Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='4', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:40.322\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:40.322452Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='9', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:40.394\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:40.394368Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='f', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:40.466\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:40.466512Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='0', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:40.538\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:40.538307Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='-a', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:40.610\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:40.610013Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='0', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:40.681\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:40.681404Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='5', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:40.754\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:40.754537Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='6', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:40.826\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:40.82616Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='-', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:40.897\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:40.897501Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='6', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:40.969\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:40.969127Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='d', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:41.043\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:41.043373Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='1', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:41.115\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:41.115519Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='1', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:41.187\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:41.187099Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='7', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:41.259\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:41.258813Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='4', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:41.330\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:41.330205Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='8', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:41.402\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:41.402479Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='a', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:41.475\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:41.475163Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='6', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:41.547\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:41.54737Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='ed', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:41.635\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:41.634581Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='0', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:41.722\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:41.721009Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=\"'\", thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:41.800\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:41.799647Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=').\\n\\n', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:41.871\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:41.871496Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='However', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:41.946\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:41.946258Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=',', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:42.021\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:42.020659Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' it', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:42.092\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:42.092085Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=\"'s\", thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:42.164\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:42.164237Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' important', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:42.238\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:42.237741Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' to', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:42.309\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:42.308976Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' note', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:42.381\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:42.381051Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' that', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:42.458\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:42.457011Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' the', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:42.539\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:42.538092Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' primary', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:42.615\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:42.614617Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' material', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:42.690\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:42.689645Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' is', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:42.768\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:42.768032Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' recycled', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:42.843\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:42.843543Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' polypropylene', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:42.919\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:42.918803Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' or', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:42.997\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:42.996994Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' high', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:43.077\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:43.077185Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='-density', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:43.151\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:43.151073Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' polyethylene', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:43.226\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:43.225709Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=',', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:43.298\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:43.298142Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' as', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:43.375\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:43.375048Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' mentioned', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:43.458\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:43.457666Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' in', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:43.530\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:43.530405Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' the', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:43.604\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:43.604145Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' first', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:43.686\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:43.685841Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' source', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:43.764\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:43.764544Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=\" ('\", thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:43.839\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:43.839338Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='1', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:43.915\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:43.915608Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='f', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:43.991\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:43.991134Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='0', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:44.066\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:44.065857Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='5', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:44.141\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:44.141196Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='bf', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:44.221\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:44.220799Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='8', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:44.302\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:44.302108Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='6', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:44.381\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:44.381055Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='-', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:44.465\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:44.464921Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='9', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:44.547\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:44.546838Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='a', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:44.624\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:44.624022Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='9', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:44.695\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:44.695691Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='3', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:44.767\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:44.767387Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='-', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:44.840\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:44.840479Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='4', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:44.912\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:44.912072Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='3', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:44.983\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:44.983396Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='8', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:45.056\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:45.056534Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='4', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:45.129\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:45.12888Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='-b', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:45.202\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:45.20238Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='1', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:45.274\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:45.27375Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='d', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:45.347\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:45.346895Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='7', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:45.422\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:45.422048Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='-c', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:45.498\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:45.498451Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='0', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:45.570\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:45.570321Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='2', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:45.642\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:45.642373Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='1', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:45.714\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:45.714278Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='b', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:45.786\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:45.785735Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='3', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:45.859\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:45.859402Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='3', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:45.945\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:45.943131Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='9', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:46.025\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:46.025437Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='b', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:46.099\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:46.099128Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='7', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:46.170\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:46.170496Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='4', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:46.242\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:46.242006Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='a', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:46.314\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:46.313914Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=\"').\", thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:46.387\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:46.386846Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' The', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:46.459\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:46.459097Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' additional', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:46.533\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:46.532886Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' materials', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:46.606\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:46.60617Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' might', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:46.678\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:46.678316Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' be', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:46.751\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:46.750743Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' used', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:46.829\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:46.828725Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' in', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:46.901\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:46.901041Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' different', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:46.974\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:46.973867Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' models', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:47.047\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:47.047134Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' or', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:47.119\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:47.119359Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' variations', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:47.193\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:47.193375Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' of', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:47.265\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:47.265365Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' the', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:47.338\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:47.337922Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' pal', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:47.410\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:47.409996Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='let', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:47.482\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:47.482535Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='.', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:47.559\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:47.559478Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' To', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:47.632\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:47.632739Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' get', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:47.707\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:47.706947Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' a', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:47.779\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:47.779095Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' definitive', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:47.854\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:47.854634Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' answer', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:47.928\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:47.928163Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' about', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:48.000\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:48.000079Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' the', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:48.072\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:48.071871Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' materials', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:48.143\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:48.143309Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' used', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:48.215\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:48.214862Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' specifically', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:48.287\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:48.28705Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' in', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:48.359\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:48.358762Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' the', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:48.431\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:48.431492Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' Lara', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:48.503\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:48.503316Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' pal', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:48.575\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:48.575137Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='let', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:48.646\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:48.646384Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=',', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:48.718\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:48.717755Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' it', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:48.789\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:48.7893Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' would', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:48.862\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:48.861954Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' be', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:48.933\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:48.933539Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' best', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:49.008\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:49.008321Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' to', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:49.080\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:49.079998Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' contact', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:49.151\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:49.151539Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' the', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:49.223\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:49.22316Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' manufacturer', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:49.297\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:49.296687Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' directly', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:49.370\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:49.369678Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='.', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:49.445\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:03:49.443957Z' done=True done_reason='stop' total_duration=57005580834 load_duration=4667154417 prompt_eval_count=1779 prompt_eval_duration=19432114459 eval_count=449 eval_duration=32732550781 message=Message(role='assistant', content='', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:49.447\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msme_kt_zh_collaboration_rag.baseline_rag\u001b[0m:\u001b[36mask\u001b[0m:\u001b[36m326\u001b[0m - \u001b[1mAnswer:\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:49.447\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msme_kt_zh_collaboration_rag.baseline_rag\u001b[0m:\u001b[36mask\u001b[0m:\u001b[36m327\u001b[0m - \u001b[1m  Based on the sources provided, here's what we know about the materials used to make the Lara pallet:\n",
      "\n",
      "1. **Primary Material**:\n",
      "   - The Lara pallet is made primarily from plastic.\n",
      "   - It is composed of greater than 99% polyolefins and other trace materials (Source: 'a733b848-ef47-4690-bd0c-cf41a134abc3').\n",
      "\n",
      "2. **Recycled Content**:\n",
      "   - The pallet has a recycled plastic content of 100%.\n",
      "   - It is made up of post-consumer plastic waste (Source: 'a733b848-ef47-4690-bd0c-cf41a134abc3').\n",
      "   - The pallet is free from hazardous chemical substances as classified under REACH and CLP regulations (Source: '6b28b9fa-d78d-49f0-a056-6d11748a6ed0').\n",
      "\n",
      "3. **Additional Materials**:\n",
      "   - Some sources mention additional materials used in the production of similar pallets, such as:\n",
      "     - Micronized aluminum and cellulose (Source: '6b28b9fa-d78d-49f0-a056-6d11748a6ed0').\n",
      "     - Glass fibers and additives/pigments (Source: '6b28b9fa-d78d-49f0-a056-6d11748a6ed0').\n",
      "\n",
      "However, it's important to note that the primary material is recycled polypropylene or high-density polyethylene, as mentioned in the first source ('1f05bf86-9a93-4384-b1d7-c021b339b74a'). The additional materials might be used in different models or variations of the pallet. To get a definitive answer about the materials used specifically in the Lara pallet, it would be best to contact the manufacturer directly.\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:49.447\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msme_kt_zh_collaboration_rag.baseline_rag\u001b[0m:\u001b[36mask\u001b[0m:\u001b[36m328\u001b[0m - \u001b[1mSources (5):\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:49.448\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msme_kt_zh_collaboration_rag.baseline_rag\u001b[0m:\u001b[36mask\u001b[0m:\u001b[36m331\u001b[0m - \u001b[1m  '3_EPD_pallet_relicyc.pdf'  |  '# PRODUCT INFORMATION'\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:49.448\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msme_kt_zh_collaboration_rag.baseline_rag\u001b[0m:\u001b[36mask\u001b[0m:\u001b[36m331\u001b[0m - \u001b[1m  '3_EPD_pallet_relicyc.pdf'  |  '# CONTENT DECLARATION'\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:49.448\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msme_kt_zh_collaboration_rag.baseline_rag\u001b[0m:\u001b[36mask\u001b[0m:\u001b[36m331\u001b[0m - \u001b[1m  '2_EPD_pallet_CPR.pdf'  |  '#### 5. Content declaration'\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:49.448\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msme_kt_zh_collaboration_rag.baseline_rag\u001b[0m:\u001b[36mask\u001b[0m:\u001b[36m331\u001b[0m - \u001b[1m  '2_EPD_pallet_CPR.pdf'  |  '#### 3. Product information'\u001b[0m\n",
      "\u001b[32m2026-02-20 16:03:49.449\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msme_kt_zh_collaboration_rag.baseline_rag\u001b[0m:\u001b[36mask\u001b[0m:\u001b[36m331\u001b[0m - \u001b[1m  '3_EPD_pallet_relicyc.pdf'  |  '# 40 YEARS OF SUSTAINABLE INNOVATION'\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the sources provided, here's what we know about the materials used to make the Lara pallet:\n",
      "\n",
      "1. **Primary Material**:\n",
      "   - The Lara pallet is made primarily from plastic.\n",
      "   - It is composed of greater than 99% polyolefins and other trace materials (Source: 'a733b848-ef47-4690-bd0c-cf41a134abc3').\n",
      "\n",
      "2. **Recycled Content**:\n",
      "   - The pallet has a recycled plastic content of 100%.\n",
      "   - It is made up of post-consumer plastic waste (Source: 'a733b848-ef47-4690-bd0c-cf41a134abc3').\n",
      "   - The pallet is free from hazardous chemical substances as classified under REACH and CLP regulations (Source: '6b28b9fa-d78d-49f0-a056-6d11748a6ed0').\n",
      "\n",
      "3. **Additional Materials**:\n",
      "   - Some sources mention additional materials used in the production of similar pallets, such as:\n",
      "     - Micronized aluminum and cellulose (Source: '6b28b9fa-d78d-49f0-a056-6d11748a6ed0').\n",
      "     - Glass fibers and additives/pigments (Source: '6b28b9fa-d78d-49f0-a056-6d11748a6ed0').\n",
      "\n",
      "However, it's important to note that the primary material is recycled polypropylene or high-density polyethylene, as mentioned in the first source ('1f05bf86-9a93-4384-b1d7-c021b339b74a'). The additional materials might be used in different models or variations of the pallet. To get a definitive answer about the materials used specifically in the Lara pallet, it would be best to contact the manufacturer directly.\n"
     ]
    }
   ],
   "source": [
    "answer_ook = await ask(agent, \"What materials is the Lara pallet made from?\")\n",
    "print(answer_ook)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p0-query-gap-md",
   "metadata": {},
   "source": [
    "### 4b: Missing Data (LogyLight Pallet)\n",
    "\n",
    "The LogyLight datasheet marks all LCA fields as *\"not yet available\"*. The correct answer is that we don't have the data, not a fabricated figure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "p0-query-gap-code",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-02-20 16:04:38.952\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msme_kt_zh_collaboration_rag.baseline_rag\u001b[0m:\u001b[36mask\u001b[0m:\u001b[36m323\u001b[0m - \u001b[1mQuery: 'What is the GWP of the LogyLight pallet?'\u001b[0m\n",
      "\u001b[32m2026-02-20 16:04:39.412\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.embeddings.sentence_transformer\u001b[0m:\u001b[36mget_embeddings\u001b[0m:\u001b[36m76\u001b[0m - \u001b[34m\u001b[1msentence-transformers/all-MiniLM-L6-v2 embeddings size: (1, 384)\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:15.519\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:15.513433Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='Based', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:15.648\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:15.648236Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' on', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:15.722\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:15.721962Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' the', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:15.796\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:15.796429Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' provided', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:15.871\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:15.871223Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' information', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:15.945\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:15.945103Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=',', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:16.019\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:16.018771Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' here', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:16.094\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:16.093956Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=\"'s\", thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:16.189\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:16.188278Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' a', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:16.271\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:16.271079Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' summary', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:16.344\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:16.344084Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' of', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:16.418\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:16.418142Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' the', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:16.495\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:16.49486Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' environmental', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:16.571\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:16.570735Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' impact', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:16.647\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:16.64675Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' assessment', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:16.721\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:16.720905Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' for', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:16.796\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:16.796336Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' the', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:16.869\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:16.868983Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' plastic', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:16.943\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:16.942801Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' pal', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:17.021\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:17.020947Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='let', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:17.100\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:17.099494Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' using', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:17.178\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:17.17854Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' an', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:17.272\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:17.271902Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' alternative', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:17.355\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:17.35515Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' functional', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:17.441\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:17.441352Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' unit', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:17.522\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:17.522632Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' of', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:17.596\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:17.596519Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' transporting', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:17.670\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:17.670652Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' ', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:17.743\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:17.743547Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='1', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:17.816\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:17.816387Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' m', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:17.887\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:17.887173Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='³', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:17.960\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:17.96012Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' under', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:18.031\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:18.030912Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' specific', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:18.104\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:18.103797Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' conditions', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:18.177\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:18.177088Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=':\\n\\n', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:18.248\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:18.248341Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='**', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:18.321\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:18.321001Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='Global', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:18.403\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:18.403421Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' W', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:18.478\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:18.478064Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='arming', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:18.550\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:18.549914Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' Potential', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:18.624\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:18.624343Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' (', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:18.696\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:18.69595Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='G', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:18.769\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:18.769666Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='WP', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:18.845\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:18.84494Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='):', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:18.917\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:18.916942Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='**\\n', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:18.990\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:18.990101Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='-', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:19.062\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:19.061869Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' Up', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:19.134\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:19.134061Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='stream', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:19.207\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:19.20669Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=':', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:19.279\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:19.278845Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' ', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:19.350\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:19.350466Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='5', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:19.424\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:19.423649Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='.', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:19.498\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:19.498037Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='5', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:19.574\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:19.57395Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='0', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:19.646\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:19.646298Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='E', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:19.718\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:19.71821Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='-', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:19.790\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:19.790213Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='0', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:19.862\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:19.862044Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='4', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:19.934\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:19.933797Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' kg', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:20.006\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:20.00589Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' CO', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:20.079\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:20.079013Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='₂', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:20.154\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:20.154026Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' eq', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:20.231\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:20.231214Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='.\\n', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:20.311\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:20.311045Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='-', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:20.388\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:20.388589Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' Core', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:20.465\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:20.46485Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=':', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:20.540\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:20.539863Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' ', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:20.614\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:20.614607Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='2', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:20.687\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:20.687605Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='.', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:20.759\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:20.759663Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='3', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:20.833\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:20.833102Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='1', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:20.905\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:20.905649Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='E', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:20.977\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:20.977431Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='+', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:21.049\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:21.049133Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='0', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:21.122\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:21.122338Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='0', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:21.193\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:21.19344Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' kg', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:21.266\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:21.266128Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' CO', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:21.341\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:21.34155Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='₂', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:21.415\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:21.414844Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' eq', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:21.488\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:21.488527Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='.\\n', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:21.561\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:21.56064Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='-', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:21.633\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:21.633582Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' Down', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:21.706\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:21.705752Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='stream', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:21.777\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:21.777196Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=':', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:21.849\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:21.849154Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' ', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:21.921\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:21.920799Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='2', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:21.995\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:21.995479Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='.', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:22.071\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:22.070926Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='0', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:22.143\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:22.142737Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='5', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:22.215\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:22.214845Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='E', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:22.286\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:22.286307Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='+', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:22.359\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:22.359377Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='0', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:22.431\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:22.431139Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='0', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:22.503\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:22.50312Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' kg', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:22.575\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:22.574813Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' CO', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:22.647\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:22.646963Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='₂', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:22.719\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:22.719036Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' eq', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:22.791\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:22.791141Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='.\\n', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:22.866\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:22.865705Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='-', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:22.938\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:22.938387Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' Total', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:23.016\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:23.015808Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=':', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:23.095\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:23.094556Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' ', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:23.170\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:23.170034Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='4', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:23.262\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:23.262391Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='.', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:23.340\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:23.340347Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='3', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:23.418\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:23.417722Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='6', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:23.493\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:23.493503Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='E', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:23.568\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:23.567733Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='+', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:23.641\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:23.641478Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='0', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:23.716\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:23.715941Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='0', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:23.792\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:23.791696Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' kg', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:23.868\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:23.867819Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' CO', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:23.940\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:23.93968Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='₂', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:24.012\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:24.012046Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' eq', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:24.084\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:24.083726Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='.\\n\\n', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:24.157\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:24.157037Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='**', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:24.229\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:24.229208Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='Ac', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:24.301\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:24.300848Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='id', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:24.373\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:24.373399Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='ification', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:24.449\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:24.449379Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' Potential', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:24.525\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:24.525474Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' (', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:24.605\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:24.605012Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='AP', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:24.681\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:24.681005Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='):', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:24.756\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:24.755665Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='**\\n', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:24.834\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:24.833973Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='-', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:24.913\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:24.91328Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' Total', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:24.992\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:24.991707Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=':', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:25.064\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:25.064558Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' ', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:25.137\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:25.137276Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='1', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:25.211\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:25.211136Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='.', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:25.284\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:25.284074Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='8', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:25.356\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:25.356429Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='6', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:25.429\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:25.428651Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='E', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:25.501\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:25.50096Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='-', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:25.573\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:25.573189Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='0', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:25.647\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:25.646987Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='2', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:25.721\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:25.720969Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' kg', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:25.794\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:25.794167Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' mol', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:25.866\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:25.866524Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' H', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:26.011\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:26.010563Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='⁺', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:26.083\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:26.083471Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' eq', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:26.155\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:26.155537Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='.\\n\\n', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:26.228\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:26.227713Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='**', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:26.300\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:26.299811Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='E', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:26.372\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:26.372018Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='ut', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:26.444\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:26.443918Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='roph', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:26.517\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:26.5167Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='ication', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:26.603\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:26.602859Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' Potential', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:26.675\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:26.675295Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' (', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:26.747\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:26.74699Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='EP', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:26.820\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:26.819893Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='):', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:26.892\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:26.892036Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='**\\n', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:26.964\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:26.963859Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='-', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:27.035\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:27.035516Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' Aqu', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:27.109\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:27.108746Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='atic', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:27.181\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:27.181368Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' freshwater', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:27.255\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:27.254858Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=':', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:27.328\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:27.328184Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' ', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:27.400\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:27.400078Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='4', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:27.474\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:27.474031Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='.', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:27.546\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:27.545838Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='7', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:27.618\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:27.617735Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='6', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:27.689\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:27.689375Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='E', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:27.761\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:27.761183Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='-', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:27.834\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:27.834183Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='0', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:27.906\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:27.905866Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='4', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:27.977\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:27.977636Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' kg', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:28.050\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:28.050594Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' P', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:28.122\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:28.122302Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' eq', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:28.194\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:28.193859Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='.\\n', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:28.267\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:28.266798Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='-', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:28.341\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:28.341452Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' Aqu', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:28.415\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:28.4154Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='atic', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:28.493\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:28.492954Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' marine', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:28.569\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:28.569588Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=':', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:28.643\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:28.642965Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' ', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:28.717\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:28.71677Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='4', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:28.791\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:28.791218Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='.', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:28.864\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:28.864087Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='7', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:28.937\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:28.936759Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='7', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:29.011\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:29.011405Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='E', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:29.084\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:29.084245Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='-', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:29.156\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:29.156601Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='0', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:29.229\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:29.229174Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='3', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:29.301\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:29.30115Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' kg', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:29.372\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:29.372571Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' N', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:29.444\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:29.444079Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' eq', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:29.517\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:29.517036Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='.\\n', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:29.588\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:29.588366Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='-', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:29.660\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:29.659907Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' Aqu', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:29.731\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:29.731403Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='atic', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:29.803\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:29.802981Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' terrestrial', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:29.875\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:29.87545Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=':', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:29.948\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:29.947929Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' ', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:30.020\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:30.020422Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='5', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:30.095\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:30.095111Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='.', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:30.169\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:30.168705Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='0', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:30.248\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:30.248528Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='8', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:30.321\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:30.321628Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='E', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:30.396\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:30.396481Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='-', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:30.475\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:30.475086Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='0', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:30.550\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:30.549853Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='2', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:30.627\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:30.626828Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' mol', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:30.701\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:30.701559Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' N', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:30.775\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:30.774768Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' eq', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:30.847\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:30.847406Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='.\\n\\n', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:30.933\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:30.927665Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='**', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:31.004\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:31.003918Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='Ph', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:31.075\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:31.075514Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='ot', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:31.148\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:31.148529Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='ochemical', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:31.222\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:31.222026Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' Oxid', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:31.295\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:31.295209Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='ant', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:31.369\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:31.369633Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' Creation', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:31.443\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:31.44305Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' Potential', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:31.520\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:31.520577Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=':**\\n', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:31.594\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:31.593905Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='-', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:31.667\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:31.667362Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' Total', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:31.744\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:31.744012Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=':', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:31.817\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:31.816962Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' ', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:31.889\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:31.888834Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='1', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:31.961\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:31.961304Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='.', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:32.034\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:32.034413Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='3', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:32.108\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:32.108246Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='3', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:32.180\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:32.179868Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='E', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:32.252\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:32.252412Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='-', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:32.327\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:32.326604Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='0', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:32.400\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:32.400321Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='2', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:32.472\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:32.472324Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' kg', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:32.545\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:32.544743Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' NM', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:32.616\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:32.616483Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='V', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:32.688\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:32.688372Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='OC', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:32.761\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:32.760812Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' eq', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:32.833\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:32.832818Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='.\\n\\n', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:32.904\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:32.904495Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='**', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:32.977\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:32.976836Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='O', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:33.050\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:33.050674Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='zone', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:33.122\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:33.122547Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' Layer', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:33.195\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:33.195078Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' De', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:33.269\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:33.268727Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='pletion', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:33.340\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:33.340546Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=':**\\n', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:33.412\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:33.4124Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='-', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:33.485\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:33.484899Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' Total', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:33.556\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:33.55665Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=':', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:33.628\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:33.628517Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' ', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:33.700\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:33.700609Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='8', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:33.774\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:33.773761Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='.', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:33.845\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:33.845675Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='4', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:33.917\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:33.917326Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='9', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:33.989\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:33.989471Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='E', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:34.061\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:34.061553Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='-', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:34.133\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:34.133534Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='0', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:34.205\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:34.205303Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='7', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:34.279\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:34.278766Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' kg', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:34.350\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:34.350722Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' C', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:34.423\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:34.423125Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='FC', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:34.497\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:34.496795Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' ', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:34.569\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:34.569177Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='1', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:34.641\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:34.640775Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='1', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:34.713\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:34.712842Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' eq', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:34.785\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:34.784889Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='.\\n\\n', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:34.859\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:34.859318Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='**', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:34.937\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:34.936722Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='A', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:35.009\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:35.009555Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='biotic', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:35.082\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:35.081972Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' De', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:35.153\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:35.153396Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='pletion', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:35.227\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:35.227406Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' Potential', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:35.300\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:35.30013Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' (', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:35.373\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:35.373547Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='AD', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:35.445\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:35.445227Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='P', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:35.517\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:35.517378Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='):', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:35.590\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:35.589838Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='**\\n', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:35.662\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:35.661988Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='-', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:35.734\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:35.733886Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' Met', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:35.808\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:35.808316Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='als', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:35.881\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:35.881305Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' and', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:35.954\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:35.9546Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' minerals', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:36.027\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:36.027198Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=':', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:36.099\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:36.099529Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' ', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:36.171\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:36.171259Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='1', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:36.246\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:36.245926Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='.', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:36.320\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:36.320272Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='4', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:36.398\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:36.397854Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='8', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:36.474\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:36.473826Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='E', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:36.548\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:36.547653Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='-', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:36.620\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:36.620383Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='0', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:36.696\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:36.696321Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='5', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:36.769\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:36.769069Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' kg', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:36.842\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:36.842199Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' Sb', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:36.918\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:36.918095Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' eq', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:36.992\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:36.991979Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='.\\n', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:37.065\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:37.064675Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='-', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:37.138\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:37.138201Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' Foss', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:37.210\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:37.210405Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='il', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:37.282\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:37.282642Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' resources', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:37.355\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:37.354621Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=':', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:37.428\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:37.428069Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' ', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:37.500\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:37.500699Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='6', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:37.573\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:37.573291Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='.', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:37.645\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:37.645391Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='7', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:37.718\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:37.717739Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='0', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:37.790\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:37.790564Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='E', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:37.867\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:37.867577Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='+', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:37.939\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:37.939385Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='0', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:38.011\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:38.011313Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='1', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:38.084\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:38.084292Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' MJ', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:38.158\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:38.158294Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='\\n\\n', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:38.231\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:38.230758Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='**', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:38.308\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:38.308669Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='Water', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:38.380\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:38.380511Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' De', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:38.452\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:38.452636Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='pr', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:38.525\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:38.525381Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='ivation', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:38.598\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:38.597858Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' Potential', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:38.670\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:38.66994Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' (', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:38.742\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:38.742015Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='W', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:38.814\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:38.814025Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='DP', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:38.887\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:38.887448Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='):', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:38.960\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:38.959878Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='**\\n', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:39.037\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:39.03764Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='-', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:39.113\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:39.112709Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' Total', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:39.186\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:39.186235Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=':', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:39.260\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:39.2599Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' ', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:39.334\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:39.334413Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='1', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:39.408\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:39.408191Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='.', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:39.482\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:39.481966Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='7', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:39.557\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:39.556777Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='1', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:39.631\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:39.631117Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='E', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:39.705\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:39.705339Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='+', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:39.781\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:39.780691Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='0', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:39.854\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:39.853718Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='0', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:39.935\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:39.935486Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' m', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:40.013\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:40.012876Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='³', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:40.090\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:40.089954Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' depr', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:40.163\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:40.163214Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='iv', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:40.237\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:40.236791Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='.\\n\\n', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:40.310\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:40.310448Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='**', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:40.382\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:40.382203Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='Resources', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:40.456\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:40.456747Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' Use', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:40.530\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:40.529716Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=':**\\n', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:40.605\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:40.605463Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='-', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:40.680\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:40.679995Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' Renew', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:40.754\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:40.753736Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='able', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:40.835\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:40.835104Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' materials', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:40.909\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:40.908829Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' used', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:40.983\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:40.983194Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' as', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:41.060\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:41.059802Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' energy', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:41.133\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:41.133587Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' carrier', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:41.213\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:41.213129Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=':', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:41.290\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:41.290255Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' ', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:41.363\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:41.363533Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='4', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:41.436\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:41.436693Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='.', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:41.510\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:41.510253Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='8', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:41.591\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:41.59013Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='5', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:41.686\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:41.685591Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='E', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:41.772\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:41.771879Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='-', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:41.854\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:41.853618Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='0', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:41.936\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:41.936224Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='1', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:42.014\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:42.014257Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' MJ', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:42.087\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:42.087663Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=',', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:42.163\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:42.163234Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' net', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:42.235\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:42.235565Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' calor', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:42.308\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:42.30861Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='ific', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:42.382\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:42.382172Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' value', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:42.462\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:42.46187Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='\\n', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:42.536\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:42.536311Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='-', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:42.623\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:42.623197Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' Non', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:42.696\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:42.696431Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='-', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:42.770\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:42.769788Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='renew', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:42.847\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:42.847032Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='able', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:42.919\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:42.91933Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' materials', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:42.993\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:42.993472Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' used', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:43.066\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:43.065917Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' as', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:43.138\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:43.138113Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' energy', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:43.210\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:43.210148Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' carrier', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:43.285\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:43.284894Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=':', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:43.358\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:43.358231Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' ', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:43.430\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:43.430717Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='6', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:43.503\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:43.503219Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='.', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:43.578\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:43.577724Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='7', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:43.650\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:43.650598Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='0', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:43.724\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:43.72394Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='E', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:43.796\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:43.796309Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='+', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:43.871\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:43.870718Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='0', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:43.944\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:43.943624Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='1', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:44.017\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:44.016652Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' MJ', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:44.089\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:44.089384Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='\\n', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:44.163\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:44.163352Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='-', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:44.238\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:44.237845Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' Total', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:44.311\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:44.310655Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' non', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:44.383\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:44.383188Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='-', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:44.457\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:44.457119Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='renew', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:44.530\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:44.529933Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='able', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:44.602\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:44.602627Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' resources', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:44.675\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:44.675281Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' use', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:44.749\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:44.749153Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=':', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:44.823\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:44.82314Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' ', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:44.895\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:44.895501Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='6', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:44.968\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:44.968332Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='.', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:45.041\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:45.04095Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='7', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:45.116\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:45.116005Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='2', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:45.188\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:45.188326Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='E', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:45.261\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:45.261185Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='+', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:45.336\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:45.336422Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='0', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:45.412\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:45.412187Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='1', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:45.485\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:45.485203Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' MJ', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:45.558\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:45.557742Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='\\n\\n', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:45.630\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:45.63001Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='**', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:45.702\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:45.702451Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='Additional', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:45.778\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:45.778307Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' Information', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:45.851\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:45.850743Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=':**\\n', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:45.924\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:45.924375Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='-', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:45.997\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:45.996988Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' The', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:46.069\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:46.06928Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' pal', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:46.141\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:46.141399Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='let', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:46.214\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:46.213944Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' is', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:46.292\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:46.292066Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' designed', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:46.366\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:46.366366Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' for', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:46.440\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:46.440609Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' ', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:46.517\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:46.516908Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='4', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:46.592\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:46.592096Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='1', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:46.665\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:46.665197Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' uses', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:46.739\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:46.739239Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' in', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:46.812\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:46.811786Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' its', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:46.887\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:46.886499Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' lifetime', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:46.966\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:46.966573Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='.\\n', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:47.039\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:47.039274Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='-', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:47.112\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:47.111748Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' The', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:47.184\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:47.18392Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' occupied', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:47.257\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:47.257377Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' volume', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:47.332\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:47.332267Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' considered', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:47.405\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:47.404731Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' for', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:47.479\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:47.47884Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' the', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:47.551\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:47.550997Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' alternative', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:47.623\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:47.623397Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' functional', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:47.696\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:47.695785Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' unit', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:47.768\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:47.768332Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' is', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:47.840\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:47.840455Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' ', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:47.913\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:47.912786Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='0', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:47.986\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:47.98658Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='.', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:48.059\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:48.059226Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='9', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:48.131\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:48.131373Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='6', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:48.204\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:48.203994Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' m', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:48.277\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:48.276699Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='³', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:48.351\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:48.350701Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=',', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:48.423\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:48.423354Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' with', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:48.500\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:48.499778Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' a', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:48.572\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:48.572535Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' load', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:48.646\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:48.646101Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' density', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:48.720\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:48.719827Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' of', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:48.792\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:48.792258Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' ', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:48.864\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:48.864569Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='8', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:48.937\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:48.936774Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='3', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:49.011\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:49.0107Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='3', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:49.083\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:49.083152Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' kg', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:49.155\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:49.155462Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='/m', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:49.228\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:49.228421Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='³', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:49.301\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:49.301155Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' to', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:49.373\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:49.373578Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' reach', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:49.446\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:49.446031Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' the', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:49.520\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:49.520016Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' maximum', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:49.594\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:49.593801Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' dynamic', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:49.667\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:49.667635Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' load', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:49.740\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:49.739984Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' of', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:49.812\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:49.812619Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' ', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:49.885\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:49.88494Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='8', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:49.957\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:49.957304Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='0', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:50.031\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:50.031167Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='0', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:50.104\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:50.103887Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' kg', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:50.176\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:50.176347Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='.\\n\\n', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:50.252\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:50.251816Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='These', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:50.328\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:50.32842Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' results', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:50.402\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:50.402009Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' provide', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:50.475\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:50.474993Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' insight', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:50.559\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:50.559604Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' into', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:50.632\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:50.632118Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' the', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:50.705\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:50.704743Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' environmental', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:50.777\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:50.777439Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' impact', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:50.850\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:50.85018Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' of', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:50.923\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:50.922919Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' transporting', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:51.002\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:51.002469Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' ', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:51.084\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:51.084285Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='1', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:51.159\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:51.158941Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' m', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:51.234\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:51.23371Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='³', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:51.308\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:51.30773Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' using', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:51.381\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:51.381468Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' this', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:51.457\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:51.456785Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' plastic', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:51.535\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:51.534804Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' pal', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:51.608\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:51.608289Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='let', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:51.682\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:51.681975Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' over', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:51.756\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:51.756714Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' its', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:51.831\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:51.831181Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' entire', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:51.906\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:51.905825Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' life', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:51.984\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:51.98388Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' cycle', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:52.061\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:52.06075Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='.', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:52.137\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:05:52.136288Z' done=True done_reason='stop' total_duration=72683616833 load_duration=89029875 prompt_eval_count=4096 prompt_eval_duration=35920618999 eval_count=495 eval_duration=36459150719 message=Message(role='assistant', content='', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:52.140\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msme_kt_zh_collaboration_rag.baseline_rag\u001b[0m:\u001b[36mask\u001b[0m:\u001b[36m326\u001b[0m - \u001b[1mAnswer:\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:52.140\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msme_kt_zh_collaboration_rag.baseline_rag\u001b[0m:\u001b[36mask\u001b[0m:\u001b[36m327\u001b[0m - \u001b[1m  Based on the provided information, here's a summary of the environmental impact assessment for the plastic pallet using an alternative functional unit of transporting 1 m³ under specific conditions:\n",
      "\n",
      "**Global Warming Potential (GWP):**\n",
      "- Upstream: 5.50E-04 kg CO₂ eq.\n",
      "- Core: 2.31E+00 kg CO₂ eq.\n",
      "- Downstream: 2.05E+00 kg CO₂ eq.\n",
      "- Total: 4.36E+00 kg CO₂ eq.\n",
      "\n",
      "**Acidification Potential (AP):**\n",
      "- Total: 1.86E-02 kg mol H⁺ eq.\n",
      "\n",
      "**Eutrophication Potential (EP):**\n",
      "- Aquatic freshwater: 4.76E-04 kg P eq.\n",
      "- Aquatic marine: 4.77E-03 kg N eq.\n",
      "- Aquatic terrestrial: 5.08E-02 mol N eq.\n",
      "\n",
      "**Photochemical Oxidant Creation Potential:**\n",
      "- Total: 1.33E-02 kg NMVOC eq.\n",
      "\n",
      "**Ozone Layer Depletion:**\n",
      "- Total: 8.49E-07 kg CFC 11 eq.\n",
      "\n",
      "**Abiotic Depletion Potential (ADP):**\n",
      "- Metals and minerals: 1.48E-05 kg Sb eq.\n",
      "- Fossil resources: 6.70E+01 MJ\n",
      "\n",
      "**Water Deprivation Potential (WDP):**\n",
      "- Total: 1.71E+00 m³ depriv.\n",
      "\n",
      "**Resources Use:**\n",
      "- Renewable materials used as energy carrier: 4.85E-01 MJ, net calorific value\n",
      "- Non-renewable materials used as energy carrier: 6.70E+01 MJ\n",
      "- Total non-renewable resources use: 6.72E+01 MJ\n",
      "\n",
      "**Additional Information:**\n",
      "- The pallet is designed for 41 uses in its lifetime.\n",
      "- The occupied volume considered for the alternative functional unit is 0.96 m³, with a load density of 833 kg/m³ to reach the maximum dynamic load of 800 kg.\n",
      "\n",
      "These results provide insight into the environmental impact of transporting 1 m³ using this plastic pallet over its entire life cycle.\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:52.141\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msme_kt_zh_collaboration_rag.baseline_rag\u001b[0m:\u001b[36mask\u001b[0m:\u001b[36m328\u001b[0m - \u001b[1mSources (5):\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:52.141\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msme_kt_zh_collaboration_rag.baseline_rag\u001b[0m:\u001b[36mask\u001b[0m:\u001b[36m331\u001b[0m - \u001b[1m  '3_EPD_pallet_relicyc.pdf'  |  '# PRODUCT INFORMATION'\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:52.141\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msme_kt_zh_collaboration_rag.baseline_rag\u001b[0m:\u001b[36mask\u001b[0m:\u001b[36m331\u001b[0m - \u001b[1m  '3_EPD_pallet_relicyc.pdf'  |  '# CONTENT DECLARATION'\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:52.141\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msme_kt_zh_collaboration_rag.baseline_rag\u001b[0m:\u001b[36mask\u001b[0m:\u001b[36m331\u001b[0m - \u001b[1m  '1_Product-Life-Cycle-Accounting-Reporting-Standard_041613.pdf'  |  '# **_T_**'\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:52.142\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msme_kt_zh_collaboration_rag.baseline_rag\u001b[0m:\u001b[36mask\u001b[0m:\u001b[36m331\u001b[0m - \u001b[1m  '2_EPD_pallet_CPR.pdf'  |  '#### 3. Product information'\u001b[0m\n",
      "\u001b[32m2026-02-20 16:05:52.142\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msme_kt_zh_collaboration_rag.baseline_rag\u001b[0m:\u001b[36mask\u001b[0m:\u001b[36m331\u001b[0m - \u001b[1m  '3_EPD_pallet_relicyc.pdf'  |  '# ADDITIONAL INFORMATION'\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided information, here's a summary of the environmental impact assessment for the plastic pallet using an alternative functional unit of transporting 1 m³ under specific conditions:\n",
      "\n",
      "**Global Warming Potential (GWP):**\n",
      "- Upstream: 5.50E-04 kg CO₂ eq.\n",
      "- Core: 2.31E+00 kg CO₂ eq.\n",
      "- Downstream: 2.05E+00 kg CO₂ eq.\n",
      "- Total: 4.36E+00 kg CO₂ eq.\n",
      "\n",
      "**Acidification Potential (AP):**\n",
      "- Total: 1.86E-02 kg mol H⁺ eq.\n",
      "\n",
      "**Eutrophication Potential (EP):**\n",
      "- Aquatic freshwater: 4.76E-04 kg P eq.\n",
      "- Aquatic marine: 4.77E-03 kg N eq.\n",
      "- Aquatic terrestrial: 5.08E-02 mol N eq.\n",
      "\n",
      "**Photochemical Oxidant Creation Potential:**\n",
      "- Total: 1.33E-02 kg NMVOC eq.\n",
      "\n",
      "**Ozone Layer Depletion:**\n",
      "- Total: 8.49E-07 kg CFC 11 eq.\n",
      "\n",
      "**Abiotic Depletion Potential (ADP):**\n",
      "- Metals and minerals: 1.48E-05 kg Sb eq.\n",
      "- Fossil resources: 6.70E+01 MJ\n",
      "\n",
      "**Water Deprivation Potential (WDP):**\n",
      "- Total: 1.71E+00 m³ depriv.\n",
      "\n",
      "**Resources Use:**\n",
      "- Renewable materials used as energy carrier: 4.85E-01 MJ, net calorific value\n",
      "- Non-renewable materials used as energy carrier: 6.70E+01 MJ\n",
      "- Total non-renewable resources use: 6.72E+01 MJ\n",
      "\n",
      "**Additional Information:**\n",
      "- The pallet is designed for 41 uses in its lifetime.\n",
      "- The occupied volume considered for the alternative functional unit is 0.96 m³, with a load density of 833 kg/m³ to reach the maximum dynamic load of 800 kg.\n",
      "\n",
      "These results provide insight into the environmental impact of transporting 1 m³ using this plastic pallet over its entire life cycle.\n"
     ]
    }
   ],
   "source": [
    "answer_gap = await ask(agent, \"What is the GWP of the LogyLight pallet?\")\n",
    "print(answer_gap)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p0-query-conflict-md",
   "metadata": {},
   "source": [
    "### 4c: Conflicting Evidence (Relicyc GWP Figures)\n",
    "\n",
    "The 2021 Relicyc datasheet reports **4.1 kg CO₂e** per pallet. The 2023 EPD (third-party verified) reports a different, more recent figure. The RAG should flag the conflict and prefer the verified, more recent source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "p0-query-conflict-code",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-02-20 16:06:51.244\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msme_kt_zh_collaboration_rag.baseline_rag\u001b[0m:\u001b[36mask\u001b[0m:\u001b[36m323\u001b[0m - \u001b[1mQuery: 'What is the GWP of the Logypal 1 pallet, and how reliable is the figure?'\u001b[0m\n",
      "\u001b[32m2026-02-20 16:06:51.597\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.embeddings.sentence_transformer\u001b[0m:\u001b[36mget_embeddings\u001b[0m:\u001b[36m76\u001b[0m - \u001b[34m\u001b[1msentence-transformers/all-MiniLM-L6-v2 embeddings size: (1, 384)\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:27.854\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:27.83971Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='Based', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:27.926\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:27.926103Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' on', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:28.001\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:28.001606Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' the', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:28.076\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:28.076325Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' provided', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:28.150\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:28.149785Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' source', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:28.223\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:28.223467Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=',', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:28.297\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:28.297268Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' here', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:28.371\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:28.371604Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=\"'s\", thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:28.446\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:28.44532Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' a', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:28.522\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:28.521813Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' summary', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:28.596\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:28.59583Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' of', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:28.671\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:28.671101Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' how', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:28.744\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:28.744619Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' to', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:28.817\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:28.817478Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' calculate', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:28.891\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:28.891452Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' and', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:28.965\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:28.965351Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' report', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:29.038\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:29.038372Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' greenhouse', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:29.113\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:29.113034Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' gas', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:29.190\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:29.19013Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' (', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:29.267\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:29.26703Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='GH', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:29.344\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:29.343807Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='G', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:29.419\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:29.419192Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=')', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:29.494\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:29.494207Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' inventory', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:29.569\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:29.56941Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' results', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:29.642\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:29.642396Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' for', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:29.715\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:29.715028Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' a', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:29.788\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:29.787906Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' product', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:29.861\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:29.860777Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=':\\n\\n', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:29.933\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:29.933292Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='**', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:30.006\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:30.006329Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='Steps', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:30.078\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:30.078315Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' to', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:30.152\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:30.150835Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' Calculate', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:30.224\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:30.224113Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' Total', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:30.302\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:30.301839Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' Inventory', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:30.380\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:30.380635Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' Results', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:30.462\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:30.462175Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=':**\\n\\n', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:30.539\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:30.53913Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='1', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:30.620\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:30.620656Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='.', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:30.697\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:30.697602Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' **', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:30.774\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:30.774401Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='Calculate', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:30.854\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:30.854252Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' Em', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:30.932\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:30.932239Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='issions', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:31.014\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:31.014134Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' and', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:31.088\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:31.088606Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' Rem', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:31.160\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:31.160211Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='ov', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:31.232\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:31.231908Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='als', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:31.308\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:31.307741Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' per', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:31.383\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:31.383192Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' GH', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:31.456\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:31.456286Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='G', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:31.529\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:31.529605Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=':**\\n', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:31.603\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:31.603142Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='  ', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:31.680\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:31.680123Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' -', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:31.762\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:31.7623Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' Sum', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:31.846\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:31.846173Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' emissions', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:31.920\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:31.920063Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' and', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:32.004\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:32.004219Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' remov', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:32.084\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:32.083804Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='als', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:32.158\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:32.158573Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' for', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:32.238\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:32.237852Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' each', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:32.317\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:32.317024Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' GH', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:32.396\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:32.39616Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='G', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:32.472\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:32.472258Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' (', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:32.547\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:32.547421Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='CO', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:32.623\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:32.623636Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='2', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:32.700\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:32.699987Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=',', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:32.780\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:32.780287Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' CH', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:32.856\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:32.856513Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='4', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:32.930\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:32.9305Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=',', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:33.005\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:33.004765Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' N', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:33.078\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:33.078535Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='2', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:33.152\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:33.152234Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='O', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:33.224\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:33.224654Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=').\\n', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:33.300\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:33.300211Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='  ', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:33.372\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:33.372598Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' -', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:33.444\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:33.443941Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' Use', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:33.515\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:33.515493Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' the', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:33.587\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:33.586966Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' global', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:33.662\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:33.66193Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' warming', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:33.739\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:33.739211Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' potential', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:33.811\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:33.810786Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' (', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:33.884\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:33.884017Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='G', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:33.955\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:33.95494Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='WP', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:34.027\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:34.027665Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=')', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:34.110\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:34.110368Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' factors', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:34.183\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:34.183356Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' to', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:34.263\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:34.263094Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' convert', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:34.346\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:34.34594Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' them', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:34.419\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:34.418504Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' into', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:34.489\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:34.489431Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' CO', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:34.564\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:34.563809Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='2', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:34.637\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:34.636902Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' equivalents', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:34.708\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:34.708668Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='.\\n\\n', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:34.787\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:34.78682Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='2', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:34.863\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:34.862803Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='.', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:34.936\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:34.936047Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' **', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:35.009\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:35.008913Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='Sum', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:35.083\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:35.08329Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' Em', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:35.156\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:35.155984Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='issions', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:35.229\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:35.228762Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' and', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:35.301\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:35.301581Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' Rem', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:35.374\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:35.373914Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='ov', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:35.446\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:35.446109Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='als', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:35.521\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:35.520795Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' on', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:35.594\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:35.594186Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' Reference', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:35.667\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:35.667569Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' Flow', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:35.740\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:35.740591Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' Basis', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:35.815\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:35.814998Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=':**\\n', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:35.889\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:35.889629Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='  ', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:35.964\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:35.964581Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' -', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:36.037\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:36.037584Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' Ensure', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:36.110\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:36.11056Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' all', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:36.186\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:36.186474Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' results', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:36.260\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:36.260156Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' are', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:36.332\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:36.33247Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' on', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:36.404\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:36.403961Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' the', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:36.475\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:36.475084Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' same', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:36.548\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:36.548285Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' reference', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:36.627\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:36.627317Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' flow', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:36.701\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:36.70071Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' basis', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:36.772\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:36.772589Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='.\\n', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:36.847\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:36.847209Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='  ', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:36.920\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:36.920265Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' -', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:36.993\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:36.99319Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' Sum', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:37.065\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:37.065522Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' emissions', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:37.138\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:37.138265Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' and', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:37.210\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:37.21005Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' remov', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:37.282\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:37.282289Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='als', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:37.357\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:37.35695Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' per', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:37.429\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:37.428921Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' GH', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:37.502\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:37.50244Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='G', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:37.575\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:37.575614Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' to', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:37.647\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:37.647001Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' get', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:37.720\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:37.720234Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' total', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:37.792\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:37.7919Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' CO', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:37.876\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:37.875638Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='2', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:37.951\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:37.951464Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='e', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:38.023\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:38.022989Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' emissions', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:38.096\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:38.09616Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' and', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:38.167\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:38.167526Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' remov', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:38.239\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:38.238968Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='als', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:38.314\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:38.31412Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' per', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:38.390\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:38.390641Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' reference', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:38.465\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:38.465528Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' flow', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:38.539\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:38.5393Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='.\\n\\n', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:38.613\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:38.613237Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='3', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:38.686\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:38.685852Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='.', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:38.758\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:38.758341Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' **', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:38.831\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:38.831264Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='Include', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:38.905\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:38.904932Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' Land', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:38.976\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:38.976288Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='-', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:39.048\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:39.047735Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='Use', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:39.124\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:39.124281Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' Change', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:39.197\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:39.196774Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' Imp', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:39.269\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:39.269285Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='acts', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:39.340\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:39.340534Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=':**\\n', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:39.412\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:39.412039Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='  ', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:39.483\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:39.483605Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' -', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:39.558\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:39.558564Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' If', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:39.639\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:39.638742Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' applicable', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:39.720\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:39.720074Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=',', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:39.798\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:39.797957Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' include', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:39.878\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:39.878013Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' land', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:39.971\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:39.971516Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='-use', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:40.050\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:40.050061Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' change', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:40.125\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:40.125305Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' impacts', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:40.203\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:40.203359Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' in', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:40.275\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:40.274962Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' the', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:40.347\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:40.347198Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' total', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:40.420\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:40.41939Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' inventory', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:40.492\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:40.491067Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' results', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:40.566\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:40.566136Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='.\\n\\n', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:40.642\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:40.641831Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='4', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:40.715\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:40.714571Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='.', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:40.790\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:40.789857Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' **', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:40.863\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:40.863041Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='Calculate', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:40.934\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:40.934379Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' Total', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:41.006\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:41.005862Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' Inventory', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:41.081\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:41.080822Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' Results', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:41.153\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:41.15281Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' (', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:41.224\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:41.224078Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='CO', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:41.306\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:41.305296Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='2', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:41.380\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:41.380439Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='e', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:41.451\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:41.451286Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='/', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:41.523\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:41.522829Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='Unit', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:41.594\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:41.594326Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' of', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:41.675\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:41.675566Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' Analysis', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:41.763\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:41.763223Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='):', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:41.844\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:41.844246Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='**\\n', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:41.923\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:41.922846Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='  ', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:41.995\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:41.995161Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' -', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:42.068\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:42.067965Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' Sum', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:42.149\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:42.149509Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' emissions', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:42.224\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:42.223596Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' and', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:42.297\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:42.297586Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' remov', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:42.374\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:42.373766Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='als', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:42.450\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:42.449557Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' on', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:42.521\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:42.520758Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' the', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:42.594\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:42.593615Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' reference', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:42.672\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:42.672323Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' flow', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:42.745\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:42.745401Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' basis', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:42.817\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:42.817212Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' to', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:42.892\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:42.891469Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' get', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:42.970\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:42.96978Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' the', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:43.049\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:43.048988Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' total', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:43.128\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:43.127839Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' CO', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:43.203\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:43.203027Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='2', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:43.277\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:43.276516Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='e', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:43.354\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:43.354018Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' per', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:43.426\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:43.426032Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' unit', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:43.499\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:43.498793Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' of', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:43.576\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:43.576008Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' analysis', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:43.653\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:43.653032Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='.\\n\\n', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:43.726\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:43.725485Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='**', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:43.798\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:43.797534Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='Report', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:43.871\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:43.870509Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='ing', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:43.944\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:43.943611Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' Inventory', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:44.016\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:44.015557Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' Results', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:44.094\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:44.093609Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=':**\\n\\n', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:44.169\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:44.168857Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='-', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:44.246\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:44.245264Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' Report', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:44.325\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:44.32557Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' total', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:44.399\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:44.399137Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' inventory', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:44.472\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:44.471635Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' results', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:44.543\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:44.543226Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' as', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:44.633\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:44.632936Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' the', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:44.706\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:44.706255Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' sum', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:44.777\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:44.777267Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' of', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:44.849\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:44.848973Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' bi', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:44.921\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:44.920454Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='ogenic', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:44.992\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:44.992064Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' emissions', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:45.064\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:45.063754Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=',', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:45.136\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:45.135587Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' non', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:45.207\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:45.207216Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='-b', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:45.279\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:45.278668Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='i', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:45.351\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:45.350606Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='ogenic', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:45.426\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:45.425616Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' emissions', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:45.500\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:45.499553Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=',', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:45.571\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:45.571364Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' bi', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:45.643\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:45.643103Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='ogenic', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:45.717\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:45.716568Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' remov', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:45.788\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:45.788102Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='als', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:45.861\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:45.860472Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=',', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:45.935\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:45.934742Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' non', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:46.007\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:46.006596Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='-b', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:46.079\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:46.078894Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='i', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:46.156\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:46.156194Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='ogenic', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:46.227\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:46.22742Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' remov', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:46.299\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:46.299034Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='als', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:46.371\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:46.370866Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=',', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:46.443\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:46.442724Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' and', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:46.515\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:46.514578Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' land', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:46.590\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:46.590498Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='-use', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:46.673\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:46.673173Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' change', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:46.750\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:46.750422Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' impacts', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:46.827\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:46.827317Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='.\\n', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:46.903\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:46.903089Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='-', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:46.980\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:46.98062Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' Calculate', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:47.053\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:47.05264Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' and', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:47.125\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:47.12476Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' report', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:47.199\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:47.199471Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' percentages', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:47.271\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:47.271395Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' for', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:47.343\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:47.34304Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' each', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:47.416\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:47.416133Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' life', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:47.488\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:47.488088Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' cycle', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:47.561\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:47.560954Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' stage', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:47.634\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:47.634217Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' (', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:47.707\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:47.706752Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='material', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:47.783\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:47.783548Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' acquisition', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:47.861\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:47.861342Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='/', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:47.940\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:47.939949Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='processing', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:48.018\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:48.017577Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=',', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:48.094\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:48.094424Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' production', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:48.166\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:48.166245Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=',', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:48.238\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:48.238093Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' use', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:48.312\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:48.311877Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=',', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:48.384\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:48.383509Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' end', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:48.456\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:48.455697Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='-of', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:48.528\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:48.528215Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='-life', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:48.602\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:48.601883Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=').\\n', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:48.674\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:48.673896Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='-', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:48.746\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:48.746394Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' Separ', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:48.818\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:48.817954Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='ately', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:48.891\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:48.891279Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' report', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:48.963\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:48.963053Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' bi', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:49.035\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:49.035359Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='ogenic', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:49.107\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:49.107244Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' and', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:49.180\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:49.180276Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' non', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:49.252\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:49.252297Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='-b', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:49.324\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:49.324265Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='i', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:49.400\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:49.400109Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='ogenic', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:49.481\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:49.481479Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' emissions', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:49.566\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:49.566604Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' and', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:49.646\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:49.646201Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' remov', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:49.722\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:49.722251Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='als', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:49.798\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:49.797731Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=',', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:49.869\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:49.869281Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' and', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:49.941\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:49.941478Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' land', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:50.015\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:50.015075Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='-use', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:50.087\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:50.087054Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' change', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:50.159\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:50.158974Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' impacts', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:50.233\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:50.232583Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' if', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:50.306\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:50.305957Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' applicable', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:50.382\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:50.381545Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='.\\n\\n', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:50.455\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:50.454749Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='**', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:50.527\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:50.527027Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='Offset', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:50.601\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:50.601456Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='s', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:50.683\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:50.681336Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' and', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:50.762\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:50.761594Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' A', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:50.835\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:50.834767Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='vo', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:50.907\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:50.906846Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='ided', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:50.979\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:50.978834Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' Em', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:51.051\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:51.051Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='issions', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:51.123\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:51.122821Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=':**\\n\\n', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:51.195\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:51.194865Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='-', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:51.267\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:51.26673Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' Off', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:51.341\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:51.341219Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='sets', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:51.413\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:51.413197Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' can', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:51.485\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:51.48516Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' be', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:51.558\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:51.55744Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' reported', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:51.630\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:51.629464Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' separately', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:51.702\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:51.701472Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' if', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:51.773\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:51.773438Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' purchased', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:51.847\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:51.846681Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' according', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:51.920\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:51.920114Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' to', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:51.992\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:51.992357Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' the', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:52.067\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:52.067407Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' GH', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:52.141\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:52.141108Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='G', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:52.214\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:52.214098Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' Protocol', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:52.287\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:52.287247Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' Project', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:52.359\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:52.359044Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' Protocol', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:52.431\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:52.430817Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' or', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:52.503\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:52.502639Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' similar', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:52.575\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:52.5747Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' methodologies', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:52.647\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:52.646932Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='.\\n', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:52.719\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:52.718814Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='-', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:52.791\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:52.790639Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' A', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:52.863\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:52.862633Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='vo', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:52.936\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:52.936465Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='ided', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:53.024\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:53.023611Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' emissions', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:53.109\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:53.108943Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' cannot', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:53.188\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:53.187817Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' be', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:53.265\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:53.265219Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' subtracted', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:53.338\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:53.338084Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' from', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:53.415\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:53.414839Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' total', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:53.488\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:53.488087Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' inventory', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:53.563\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:53.562579Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' results', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:53.641\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:53.641324Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' but', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:53.724\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:53.724252Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' can', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:53.802\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:53.802547Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' be', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:53.877\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:53.877312Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' reported', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:53.951\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:53.951203Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' separately', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:54.025\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:54.024861Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' if', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:54.099\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:54.098418Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' calculated', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:54.175\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:54.175084Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' using', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:54.249\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:54.249343Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' appropriate', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:54.339\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:54.338855Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' methods', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:54.424\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:54.423877Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='.', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:54.507\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:54.506494Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' Ind', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:54.584\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:54.583787Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='irect', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:54.657\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:54.657326Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' emissions', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:54.730\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:54.7298Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' increases', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:54.802\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:54.801785Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' should', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:54.875\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:54.87472Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' also', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:54.949\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:54.948745Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' be', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:55.021\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:55.020944Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' considered', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:55.093\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:55.092989Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' and', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:55.165\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:55.165057Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' reported', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:55.239\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:55.23853Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' alongside', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:55.312\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:55.312086Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' avoided', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:55.384\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:55.384084Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' emissions', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:55.456\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:55.45647Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='.\\n\\n', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:55.529\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:55.529204Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='**', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:55.602\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:55.601453Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='End', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:55.682\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:55.682324Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='notes', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:55.755\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:55.755002Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=':**\\n', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:55.831\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:55.831011Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='1', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:55.903\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:55.903239Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='.', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:55.975\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:55.975225Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' Em', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:56.048\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:56.048167Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='issions', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:56.120\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:56.120251Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' increases', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:56.193\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:56.193075Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' indirectly', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:56.266\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:56.26577Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' caused', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:56.338\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:56.337934Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' by', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:56.411\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:56.410917Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' the', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:56.483\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:56.482933Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' studied', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:56.556\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:56.555782Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' product', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:56.628\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:56.628012Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' should', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:56.700\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:56.700215Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' also', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:56.772\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:56.77223Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' be', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:56.844\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:56.844282Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' reported', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:56.918\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:56.917426Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' separately', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:56.991\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:56.991036Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' with', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:57.063\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:57.06324Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' avoided', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:57.137\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:57.136707Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' emissions', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:57.209\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:57.208729Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='.', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:57.285\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T15:07:57.283776Z' done=True done_reason='stop' total_duration=65637609500 load_duration=85491375 prompt_eval_count=4096 prompt_eval_duration=36054716166 eval_count=396 eval_duration=29305482095 message=Message(role='assistant', content='', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:57.292\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msme_kt_zh_collaboration_rag.baseline_rag\u001b[0m:\u001b[36mask\u001b[0m:\u001b[36m326\u001b[0m - \u001b[1mAnswer:\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:57.293\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msme_kt_zh_collaboration_rag.baseline_rag\u001b[0m:\u001b[36mask\u001b[0m:\u001b[36m327\u001b[0m - \u001b[1m  Based on the provided source, here's a summary of how to calculate and report greenhouse gas (GHG) inventory results for a product:\n",
      "\n",
      "**Steps to Calculate Total Inventory Results:**\n",
      "\n",
      "1. **Calculate Emissions and Removals per GHG:**\n",
      "   - Sum emissions and removals for each GHG (CO2, CH4, N2O).\n",
      "   - Use the global warming potential (GWP) factors to convert them into CO2 equivalents.\n",
      "\n",
      "2. **Sum Emissions and Removals on Reference Flow Basis:**\n",
      "   - Ensure all results are on the same reference flow basis.\n",
      "   - Sum emissions and removals per GHG to get total CO2e emissions and removals per reference flow.\n",
      "\n",
      "3. **Include Land-Use Change Impacts:**\n",
      "   - If applicable, include land-use change impacts in the total inventory results.\n",
      "\n",
      "4. **Calculate Total Inventory Results (CO2e/Unit of Analysis):**\n",
      "   - Sum emissions and removals on the reference flow basis to get the total CO2e per unit of analysis.\n",
      "\n",
      "**Reporting Inventory Results:**\n",
      "\n",
      "- Report total inventory results as the sum of biogenic emissions, non-biogenic emissions, biogenic removals, non-biogenic removals, and land-use change impacts.\n",
      "- Calculate and report percentages for each life cycle stage (material acquisition/processing, production, use, end-of-life).\n",
      "- Separately report biogenic and non-biogenic emissions and removals, and land-use change impacts if applicable.\n",
      "\n",
      "**Offsets and Avoided Emissions:**\n",
      "\n",
      "- Offsets can be reported separately if purchased according to the GHG Protocol Project Protocol or similar methodologies.\n",
      "- Avoided emissions cannot be subtracted from total inventory results but can be reported separately if calculated using appropriate methods. Indirect emissions increases should also be considered and reported alongside avoided emissions.\n",
      "\n",
      "**Endnotes:**\n",
      "1. Emissions increases indirectly caused by the studied product should also be reported separately with avoided emissions.\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:57.294\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msme_kt_zh_collaboration_rag.baseline_rag\u001b[0m:\u001b[36mask\u001b[0m:\u001b[36m328\u001b[0m - \u001b[1mSources (5):\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:57.296\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msme_kt_zh_collaboration_rag.baseline_rag\u001b[0m:\u001b[36mask\u001b[0m:\u001b[36m331\u001b[0m - \u001b[1m  '3_EPD_pallet_relicyc.pdf'  |  '# CONTENT DECLARATION'\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:57.298\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msme_kt_zh_collaboration_rag.baseline_rag\u001b[0m:\u001b[36mask\u001b[0m:\u001b[36m331\u001b[0m - \u001b[1m  '3_EPD_pallet_relicyc.pdf'  |  '# PRODUCT INFORMATION'\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:57.300\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msme_kt_zh_collaboration_rag.baseline_rag\u001b[0m:\u001b[36mask\u001b[0m:\u001b[36m331\u001b[0m - \u001b[1m  '3_EPD_pallet_relicyc.pdf'  |  '# ADDITIONAL INFORMATION'\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:57.301\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msme_kt_zh_collaboration_rag.baseline_rag\u001b[0m:\u001b[36mask\u001b[0m:\u001b[36m331\u001b[0m - \u001b[1m  '2_EPD_pallet_CPR.pdf'  |  '#### 3. Product information'\u001b[0m\n",
      "\u001b[32m2026-02-20 16:07:57.303\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msme_kt_zh_collaboration_rag.baseline_rag\u001b[0m:\u001b[36mask\u001b[0m:\u001b[36m331\u001b[0m - \u001b[1m  '1_Product-Life-Cycle-Accounting-Reporting-Standard_041613.pdf'  |  '# **_T_**'\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided source, here's a summary of how to calculate and report greenhouse gas (GHG) inventory results for a product:\n",
      "\n",
      "**Steps to Calculate Total Inventory Results:**\n",
      "\n",
      "1. **Calculate Emissions and Removals per GHG:**\n",
      "   - Sum emissions and removals for each GHG (CO2, CH4, N2O).\n",
      "   - Use the global warming potential (GWP) factors to convert them into CO2 equivalents.\n",
      "\n",
      "2. **Sum Emissions and Removals on Reference Flow Basis:**\n",
      "   - Ensure all results are on the same reference flow basis.\n",
      "   - Sum emissions and removals per GHG to get total CO2e emissions and removals per reference flow.\n",
      "\n",
      "3. **Include Land-Use Change Impacts:**\n",
      "   - If applicable, include land-use change impacts in the total inventory results.\n",
      "\n",
      "4. **Calculate Total Inventory Results (CO2e/Unit of Analysis):**\n",
      "   - Sum emissions and removals on the reference flow basis to get the total CO2e per unit of analysis.\n",
      "\n",
      "**Reporting Inventory Results:**\n",
      "\n",
      "- Report total inventory results as the sum of biogenic emissions, non-biogenic emissions, biogenic removals, non-biogenic removals, and land-use change impacts.\n",
      "- Calculate and report percentages for each life cycle stage (material acquisition/processing, production, use, end-of-life).\n",
      "- Separately report biogenic and non-biogenic emissions and removals, and land-use change impacts if applicable.\n",
      "\n",
      "**Offsets and Avoided Emissions:**\n",
      "\n",
      "- Offsets can be reported separately if purchased according to the GHG Protocol Project Protocol or similar methodologies.\n",
      "- Avoided emissions cannot be subtracted from total inventory results but can be reported separately if calculated using appropriate methods. Indirect emissions increases should also be considered and reported alongside avoided emissions.\n",
      "\n",
      "**Endnotes:**\n",
      "1. Emissions increases indirectly caused by the studied product should also be reported separately with avoided emissions.\n"
     ]
    }
   ],
   "source": [
    "answer_conflict = await ask(\n",
    "    agent, \"What is the GWP of the Logypal 1 pallet, and how reliable is the figure?\"\n",
    ")\n",
    "print(answer_conflict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p0-query-unverified-md",
   "metadata": {},
   "source": [
    "### 4d: Unverified Supplier Claim (Tesa ECO Tape)\n",
    "\n",
    "The tesa supplier brochure claims **68% CO₂ reduction** compared to conventional tape. This is a self-declared marketing claim — there is no independent EPD. The RAG should report the claim but flag that it is unverified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "p0-query-unverified-code",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-02-20 17:05:18.937\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msme_kt_zh_collaboration_rag.baseline_rag\u001b[0m:\u001b[36mask\u001b[0m:\u001b[36m323\u001b[0m - \u001b[1mQuery: 'How much lower is the carbon footprint of tesa ECO tape compared to standard tape?'\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:19.307\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.embeddings.sentence_transformer\u001b[0m:\u001b[36mget_embeddings\u001b[0m:\u001b[36m76\u001b[0m - \u001b[34m\u001b[1msentence-transformers/all-MiniLM-L6-v2 embeddings size: (1, 384)\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:53.184\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:53.176204Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='Here', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:53.259\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:53.259046Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' are', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:53.330\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:53.329523Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' the', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:53.401\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:53.400903Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' environmental', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:53.473\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:53.472689Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' impact', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:53.546\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:53.545703Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' indicators', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:53.617\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:53.616911Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' from', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:53.688\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:53.687665Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' the', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:53.758\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:53.758433Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' provided', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:53.829\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:53.828957Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' sources', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:53.901\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:53.900908Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=',', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:53.972\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:53.971593Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' separated', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:54.043\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:54.042518Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' into', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:54.114\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:54.113784Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' categories', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:54.185\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:54.185161Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=':\\n\\n', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:54.256\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:54.255695Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='**', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:54.326\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:54.326357Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='1', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:54.397\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:54.396946Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='.', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:54.468\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:54.467701Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' Green', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:54.541\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:54.540534Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='house', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:54.613\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:54.612847Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' Gas', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:54.684\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:54.684316Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' Em', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:54.755\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:54.75515Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='issions', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:54.827\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:54.826572Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' (', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:54.898\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:54.897511Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='G', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:54.969\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:54.968992Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='WP', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:55.040\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:55.039584Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='-G', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:55.111\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:55.111223Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='HG', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:55.182\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:55.18189Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='):', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:55.253\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:55.253211Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='**\\n', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:55.325\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:55.324515Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='-', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:55.396\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:55.395548Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' Up', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:55.469\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:55.46924Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='stream', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:55.544\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:55.544209Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=':', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:55.618\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:55.61841Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' ', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:55.692\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:55.69242Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='2', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:55.766\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:55.76614Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='.', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:55.838\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:55.83769Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='5', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:55.909\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:55.908476Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='8', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:55.979\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:55.979312Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='E', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:56.051\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:56.050764Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='+', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:56.124\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:56.123875Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='0', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:56.195\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:56.195074Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='0', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:56.266\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:56.265685Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' kg', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:56.337\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:56.337368Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' CO', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:56.408\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:56.408034Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='2', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:56.479\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:56.479049Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' eq', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:56.553\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:56.553593Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='.\\n', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:56.627\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:56.62715Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='-', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:56.701\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:56.700809Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' Core', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:56.774\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:56.773505Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=':', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:56.844\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:56.844326Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' ', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:56.915\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:56.915408Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='5', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:56.986\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:56.986265Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='.', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:57.059\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:57.058611Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='0', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:57.130\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:57.129533Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='9', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:57.200\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:57.20022Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='E', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:57.271\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:57.27094Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='+', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:57.342\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:57.341549Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='0', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:57.412\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:57.412277Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='0', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:57.483\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:57.483008Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' kg', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:57.554\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:57.554393Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' CO', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:57.626\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:57.625485Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='2', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:57.696\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:57.696261Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' eq', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:57.767\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:57.766863Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='.\\n', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:57.838\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:57.837505Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='-', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:57.909\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:57.909188Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' Down', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:57.980\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:57.980087Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='stream', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:58.051\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:58.050989Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=':', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:58.122\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:58.121955Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' ', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:58.193\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:58.192726Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='1', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:58.265\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:58.26459Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='.', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:58.337\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:58.336671Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='3', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:58.409\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:58.409328Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='3', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:58.481\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:58.480573Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='E', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:58.552\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:58.551601Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='+', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:58.623\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:58.622506Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='0', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:58.693\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:58.693342Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='0', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:58.764\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:58.764083Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' kg', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:58.835\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:58.835079Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' CO', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:58.918\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:58.917669Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='2', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:58.990\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:58.990531Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' eq', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:59.066\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:59.066352Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='.\\n', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:59.138\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:59.137734Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='-', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:59.209\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:59.208817Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' Total', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:59.280\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:59.280174Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=':', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:59.351\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:59.350933Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' ', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:59.422\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:59.421739Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='9', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:59.493\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:59.492658Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='.', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:59.564\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:59.564145Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='0', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:59.635\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:59.635151Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='0', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:59.706\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:59.705847Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='E', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:59.777\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:59.776601Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='+', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:59.848\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:59.847558Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='0', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:59.919\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:59.919261Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='0', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:59.990\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:59.990318Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' kg', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:00.066\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:00.06571Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' CO', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:00.137\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:00.136742Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='2', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:00.208\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:00.207498Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' eq', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:00.278\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:00.278385Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='.\\n\\n', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:00.351\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:00.351257Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='**', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:00.427\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:00.427018Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='2', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:00.514\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:00.514576Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='.', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:00.592\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:00.592453Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' Global', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:00.666\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:00.665693Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' W', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:00.748\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:00.748022Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='arming', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:00.822\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:00.822226Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' Potential', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:00.897\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:00.897072Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' (', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:00.969\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:00.969512Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='G', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:01.046\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:01.046107Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='WP', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:01.128\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:01.128559Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='):', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:01.208\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:01.206924Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='**\\n', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:01.293\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:01.293358Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='-', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:01.393\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:01.392419Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' Up', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:01.481\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:01.481482Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='stream', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:01.568\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:01.567923Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=':\\n', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:01.654\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:01.653772Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' ', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:01.732\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:01.732114Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' -', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:01.806\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:01.805975Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' GH', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:01.881\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:01.881178Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='G', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:01.958\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:01.957975Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=':', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:02.036\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:02.035356Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' ', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:02.119\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:02.11857Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='4', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:02.201\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:02.201282Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='.', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:02.283\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:02.282978Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='5', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:02.367\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:02.367108Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='7', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:02.450\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:02.449795Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='E', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:02.535\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:02.535454Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='-', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:02.620\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:02.62019Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='0', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:02.706\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:02.706136Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='1', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:02.797\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:02.796781Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' kg', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:02.888\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:02.88679Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' CO', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:02.970\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:02.969686Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='2', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:03.048\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:03.047648Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' eq', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:03.136\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:03.135956Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='.\\n', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:03.220\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:03.220346Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' ', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:03.312\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:03.310999Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' -', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:03.398\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:03.397741Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' Non', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:03.529\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:03.522018Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='-G', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:03.690\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:03.690203Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='HG', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:03.779\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:03.77897Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=':', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:03.873\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:03.873055Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' ', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:03.966\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:03.966151Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='3', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:04.065\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:04.063331Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='.', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:04.167\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:04.16322Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='6', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:04.257\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:04.256496Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='3', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:04.340\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:04.339112Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='E', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:04.439\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:04.438376Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='-', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:04.527\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:04.527427Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='0', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:04.611\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:04.611363Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='1', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:04.693\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:04.693279Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' kg', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:04.773\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:04.773123Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' CO', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:04.858\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:04.857658Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='2', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:04.936\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:04.936056Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' eq', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:05.020\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:05.019088Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='.\\n', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:05.098\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:05.098179Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='-', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:05.181\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:05.1817Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' Core', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:05.262\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:05.26175Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=':\\n', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:05.342\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:05.342635Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' ', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:05.425\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:05.424589Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' -', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:05.518\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:05.517565Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' GH', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:05.609\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:05.608833Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='G', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:05.692\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:05.692214Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=':', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:05.772\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:05.772009Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' ', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:05.850\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:05.850165Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='8', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:05.927\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:05.927377Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='.', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:06.002\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:06.002418Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='9', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:06.081\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:06.081294Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='4', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:06.154\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:06.154548Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='E', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:06.227\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:06.226966Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='+', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:06.302\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:06.302296Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='0', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:06.385\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:06.385189Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='0', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:06.464\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:06.464172Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' kg', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:06.541\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:06.54129Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' CO', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:06.620\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:06.61999Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='2', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:06.693\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:06.692634Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' eq', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:06.766\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:06.76567Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='.\\n', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:06.840\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:06.840274Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' ', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:06.917\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:06.917173Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' -', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:06.992\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:06.992293Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' Non', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:07.085\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:07.083847Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='-G', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:07.176\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:07.175859Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='HG', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:07.253\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:07.252815Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=':', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:07.330\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:07.330064Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' ', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:07.403\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:07.402836Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='2', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:07.477\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:07.476759Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='.', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:07.554\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:07.553661Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='1', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:07.631\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:07.630767Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='7', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:07.708\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:07.708197Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='E', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:07.787\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:07.787423Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='+', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:07.865\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:07.865532Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='0', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:07.944\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:07.943895Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='0', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:08.019\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:08.01865Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' kg', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:08.094\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:08.093464Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' CO', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:08.166\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:08.166112Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='2', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:08.241\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:08.240824Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' eq', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:08.314\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:08.313817Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='.\\n', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:08.389\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:08.388999Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='-', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:08.462\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:08.462164Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' Down', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:08.537\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:08.537224Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='stream', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:08.613\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:08.61333Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=':\\n', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:08.688\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:08.688125Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' ', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:08.763\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:08.762948Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' -', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:08.846\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:08.844936Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' GH', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:08.920\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:08.920147Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='G', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:08.994\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:08.994214Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=':', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:09.074\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:09.073598Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' ', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:09.159\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:09.159008Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='2', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:09.243\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:09.243254Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='.', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:09.326\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:09.325916Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='5', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:09.405\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:09.40535Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='6', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:09.484\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:09.483828Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='E', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:09.559\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:09.559034Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='+', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:09.639\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:09.638233Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='0', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:09.719\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:09.719048Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='0', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:09.797\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:09.796038Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' kg', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:09.873\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:09.872784Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' CO', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:09.952\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:09.951659Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='2', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:10.034\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:10.033681Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' eq', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:10.116\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:10.115921Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='.\\n', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:10.192\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:10.192499Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' ', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:10.270\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:10.270022Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' -', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:10.351\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:10.351342Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' Non', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:10.437\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:10.436449Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='-G', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:10.535\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:10.534302Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='HG', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:10.650\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:10.640642Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=':', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:10.738\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:10.737366Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' ', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:10.825\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:10.824637Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='3', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:10.910\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:10.910288Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='.', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:10.994\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:10.994519Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='8', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:11.086\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:11.085708Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='3', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:11.178\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:11.17755Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='E', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:11.275\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:11.272717Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='-', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:11.372\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:11.370266Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='0', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:11.463\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:11.46274Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='1', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:11.554\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:11.554244Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' kg', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:11.644\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:11.643504Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' CO', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:11.731\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:11.731283Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='2', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:11.819\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:11.818891Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' eq', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:11.905\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:11.905115Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='.\\n', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:11.989\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:11.988872Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='-', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:12.073\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:12.073039Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' Total', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:12.163\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:12.162284Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=':\\n', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:12.249\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:12.24858Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' ', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:12.331\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:12.331348Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' -', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:12.409\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:12.409242Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' GH', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:12.486\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:12.485854Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='G', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:12.563\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:12.563186Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=':', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:12.641\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:12.641477Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' ', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:12.717\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:12.716962Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='9', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:12.791\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:12.791322Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='.', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:12.866\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:12.865578Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='4', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:12.945\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:12.94507Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='7', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:13.022\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:13.022313Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='E', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:13.102\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:13.101874Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='+', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:13.178\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:13.178375Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='0', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:13.252\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:13.252556Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='0', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:13.325\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:13.325003Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' kg', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:13.398\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:13.398033Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' CO', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:13.471\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:13.470811Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='2', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:13.549\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:13.54873Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' eq', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:13.623\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:13.623578Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='.\\n', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:13.700\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:13.700612Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' ', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:13.777\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:13.777041Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' -', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:13.850\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:13.850072Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' Non', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:13.925\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:13.925111Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='-G', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:13.998\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:13.998101Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='HG', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:14.075\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:14.074914Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=':', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:14.147\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:14.147379Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' ', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:14.223\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:14.222947Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='2', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:14.295\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:14.295241Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='.', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:14.372\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:14.372142Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='7', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:14.445\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:14.444805Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='5', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:14.520\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:14.519831Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='E', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:14.594\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:14.594539Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='+', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:14.668\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:14.668624Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='0', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:14.742\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:14.742204Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='0', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:14.814\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:14.813983Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' kg', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:14.895\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:14.892383Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' CO', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:14.971\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:14.9709Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='2', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:15.045\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:15.045592Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' eq', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:15.125\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:15.124861Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='.\\n\\n', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:15.203\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:15.203253Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='**', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:15.275\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:15.275124Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='3', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:15.347\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:15.347554Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='.', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:15.422\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:15.421505Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' Part', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:15.500\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:15.499407Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='ic', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:15.575\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:15.575349Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='ulate', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:15.650\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:15.650499Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' Matter', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:15.732\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:15.731615Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' (', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:15.806\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:15.806187Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='PM', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:15.885\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:15.88548Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='):', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:15.958\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:15.958193Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='**\\n', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:16.032\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:16.032086Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='-', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:16.106\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:16.106383Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' Disease', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:16.178\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:16.178546Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' inc', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:16.252\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:16.252713Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='idences', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:16.325\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:16.325081Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=':', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:16.400\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:16.400416Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' ', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:16.475\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:16.474474Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='1', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:16.547\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:16.5475Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='.', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:16.622\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:16.621867Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='4', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:16.694\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:16.694196Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='1', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:16.770\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:16.769707Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='E', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:16.843\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:16.842792Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='-', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:16.934\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:16.93353Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='0', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:17.008\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:17.007852Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='7', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:17.083\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:17.082397Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='\\n\\n', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:17.162\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:17.16183Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='**', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:17.235\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:17.234857Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='4', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:17.307\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:17.307342Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='.', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:17.381\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:17.381626Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' Ion', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:17.455\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:17.454711Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='ising', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:17.528\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:17.527944Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' Radiation', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:17.607\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:17.606965Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=':**\\n', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:17.680\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:17.679622Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='-', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:17.754\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:17.754064Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' Human', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:17.828\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:17.82814Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' health', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:17.904\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:17.903875Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=':', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:17.979\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:17.978927Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' ', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:18.051\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:18.051296Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='2', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:18.127\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:18.126691Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='.', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:18.202\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:18.202349Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='3', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:18.276\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:18.276372Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='8', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:18.350\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:18.350155Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='E', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:18.427\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:18.426815Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='+', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:18.499\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:18.499403Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='0', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:18.572\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:18.572328Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='0', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:18.651\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:18.651188Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' k', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:18.725\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:18.724707Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='B', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:18.801\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:18.801156Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='q', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:18.881\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:18.880521Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' U', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:18.956\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:18.95613Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='2', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:19.029\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:19.029403Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='3', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:19.103\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:19.102785Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='5', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:19.176\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:19.175745Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' eq', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:19.250\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:19.250018Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='.\\n\\n', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:19.322\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:19.322545Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='**', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:19.398\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:19.397699Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='5', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:19.470\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:19.470165Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='.', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:19.542\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:19.542354Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' Ec', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:19.619\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:19.619669Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='otoxicity', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:19.693\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:19.693056Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' Fresh', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:19.767\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:19.767216Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' Water', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:19.841\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:19.840816Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' (', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:19.920\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:19.920322Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='EF', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:19.994\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:19.994099Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='W', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:20.066\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:20.066281Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='):', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:20.140\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:20.14054Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='**\\n', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:20.213\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:20.212985Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='-', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:20.287\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:20.287167Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' CT', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:20.361\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:20.360979Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='U', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:20.433\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:20.433576Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='e', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:20.506\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:20.506255Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=':', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:20.579\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:20.578772Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' ', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:20.652\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:20.651691Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='4', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:20.725\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:20.724752Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='.', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:20.799\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:20.799242Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='6', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:20.873\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:20.87359Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='7', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:20.946\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:20.945882Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='E', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:21.019\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:21.01881Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='+', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:21.092\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:21.092159Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='0', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:21.165\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:21.165189Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='1', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:21.238\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:21.237788Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='\\n\\n', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:21.310\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:21.310341Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='**', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:21.384\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:21.383654Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='6', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:21.457\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:21.457588Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='.', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:21.530\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:21.530243Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' Human', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:21.603\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:21.603186Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' Tox', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:21.676\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:21.676394Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='icity', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:21.750\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:21.750152Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=':**\\n', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:21.823\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:21.822739Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='-', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:21.895\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:21.895654Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' Cancer', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:21.968\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:21.968399Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' (', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:22.041\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:22.041363Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='HT', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:22.120\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:22.120129Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='C', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:22.194\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:22.194174Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='):', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:22.266\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:22.266601Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' ', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:22.339\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:22.338949Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='1', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:22.412\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:22.412515Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='.', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:22.485\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:22.484923Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='1', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:22.557\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:22.557313Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='7', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:22.631\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:22.63087Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='E', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:22.703\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:22.703494Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='-', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:22.778\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:22.778125Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='0', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:22.850\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:22.850416Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='9', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:22.924\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:22.923994Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' CT', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:22.996\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:22.996431Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='U', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:23.069\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:23.069012Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='h', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:23.143\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:23.143137Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='\\n', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:23.216\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:23.216085Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='-', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:23.288\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:23.288564Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' Non', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:23.362\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:23.361682Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='-cancer', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:23.437\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:23.436776Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' (', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:23.509\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:23.509289Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='HT', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:23.582\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:23.58252Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='NC', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:23.656\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:23.65607Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='):', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:23.729\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:23.728739Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' ', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:23.801\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:23.801418Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='5', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:23.874\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:23.874481Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='.', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:23.947\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:23.94689Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='6', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:24.022\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:24.021632Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='6', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:24.097\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:24.097463Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='E', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:24.171\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:24.17099Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='-', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:24.245\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:24.244967Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='0', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:24.322\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:24.321848Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='8', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:24.395\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:24.395141Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' CT', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:24.468\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:24.467828Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='U', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:24.541\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:24.540633Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='h', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:24.614\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:24.613922Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='\\n\\n', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:24.686\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:24.686255Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='**', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:24.774\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:24.773606Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='7', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:24.848\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:24.848438Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='.', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:24.921\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:24.92099Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' Land', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:24.994\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:24.993965Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' Use', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:25.067\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:25.067212Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=':**\\n', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:25.141\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:25.14138Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='-', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:25.215\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:25.214901Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' Pt', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:25.289\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:25.289386Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=':', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:25.363\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:25.362545Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' ', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:25.437\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:25.436864Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='3', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:25.509\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:25.509523Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='.', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:25.582\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:25.582031Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='6', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:25.655\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:25.655033Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='7', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:25.728\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:25.727704Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='E', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:25.800\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:25.800251Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='+', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:25.873\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:25.873016Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='0', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:25.946\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:25.945615Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='1', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:26.018\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:26.01801Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='\\n\\n', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:26.094\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:26.094165Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='**', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:26.167\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:26.167253Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='8', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:26.240\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:26.239902Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='.', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:26.313\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:26.312982Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' Waste', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:26.386\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:26.386313Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' Dis', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:26.459\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:26.459015Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='posal', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:26.532\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:26.532405Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=':**\\n', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:26.606\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:26.606124Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='-', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:26.681\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:26.68155Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' Hazard', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:26.756\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:26.756724Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='ous', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:26.829\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:26.829606Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' waste', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:26.904\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:26.904497Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' disposed', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:26.977\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:26.977418Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=':', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:27.051\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:27.051572Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' ', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:27.124\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:27.124259Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='8', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:27.197\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:27.196629Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='.', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:27.271\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:27.271307Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='8', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:27.344\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:27.343889Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='4', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:27.416\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:27.416397Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='E', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:27.489\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:27.489006Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='-', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:27.562\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:27.561853Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='0', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:27.636\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:27.635892Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='9', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:27.709\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:27.708766Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' kg', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:27.781\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:27.781292Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='\\n', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:27.855\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:27.855319Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='-', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:27.929\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:27.92963Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' Non', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:28.002\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:28.002216Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='-h', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:28.076\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:28.075776Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='azard', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:28.149\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:28.149568Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='ous', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:28.222\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:28.222194Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' waste', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:28.295\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:28.294715Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' disposed', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:28.367\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:28.367218Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=':', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:28.441\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:28.441263Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' ', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:28.514\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:28.514501Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='3', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:28.590\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:28.589927Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='.', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:28.663\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:28.663425Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='8', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:28.736\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:28.735916Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='3', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:28.809\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:28.809603Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='E', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:28.887\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:28.887079Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='-', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:28.961\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:28.961355Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='0', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:29.034\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:29.034568Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='2', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:29.107\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:29.107121Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' kg', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:29.180\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:29.179747Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='\\n', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:29.252\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:29.252299Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='-', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:29.326\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:29.326057Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' Radio', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:29.399\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:29.398926Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='active', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:29.473\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:29.472758Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' waste', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:29.545\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:29.545181Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' disposed', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:29.620\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:29.620224Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=':', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:29.693\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:29.693037Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' ', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:29.765\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:29.765637Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='2', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:29.838\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:29.838046Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='.', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:29.911\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:29.911071Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='4', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:29.983\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:29.983647Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='5', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:30.056\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:30.056224Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='E', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:30.129\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:30.129479Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='-', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:30.211\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:30.211169Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='0', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:30.289\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:30.288277Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='2', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:30.362\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:30.362121Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' kg', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:30.437\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:30.437325Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='\\n\\n', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:30.514\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:30.514289Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='**', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:30.592\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:30.591828Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='9', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:30.666\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:30.665734Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='.', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:30.738\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:30.738717Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' Material', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:30.813\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:30.813353Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' for', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:30.887\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:30.887518Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' Rec', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:30.963\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:30.962597Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='ycling', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:31.036\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:31.036628Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=':**\\n', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:31.110\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:31.110482Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='-', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:31.183\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:31.183521Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' Total', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:31.256\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:31.255972Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=':', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:31.330\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:31.330147Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' ', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:31.403\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:31.403398Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='2', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:31.476\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:31.476388Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='.', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:31.549\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:31.548823Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='5', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:31.621\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:31.621494Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='0', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:31.694\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:31.694319Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='E', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:31.768\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:31.768338Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='+', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:31.843\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:31.842789Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='0', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:31.917\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:31.916888Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='1', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:31.989\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:31.989601Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' kg', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:32.062\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:32.062372Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='\\n\\n', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:32.136\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:32.136047Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='**', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:32.209\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:32.209451Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='1', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:32.282\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:32.282154Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='0', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:32.356\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:32.356204Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='.', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:32.429\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:32.429028Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' Net', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:32.502\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:32.501777Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' Use', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:32.574\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:32.574292Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' of', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:32.648\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:32.647981Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' Fresh', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:32.722\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:32.722383Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' Water', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:32.796\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:32.795796Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=':**\\n', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:32.870\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:32.870135Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='-', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:32.943\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:32.943152Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' Total', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:33.023\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:33.022802Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=':', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:33.098\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:33.098056Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' ', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:33.171\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:33.170729Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='4', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:33.243\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:33.243702Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='.', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:33.317\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:33.317324Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='0', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:33.391\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:33.391584Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='3', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:33.465\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:33.464735Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='E', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:33.538\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:33.538535Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='-', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:33.612\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:33.611757Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='0', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:33.686\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:33.686471Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='2', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:33.761\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:33.760702Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' m', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:33.834\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:33.834235Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='³', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:33.915\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:06:33.914798Z' done=True done_reason='stop' total_duration=74514857375 load_duration=111366125 prompt_eval_count=4096 prompt_eval_duration=33521706167 eval_count=536 eval_duration=40509900331 message=Message(role='assistant', content='', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:33.918\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msme_kt_zh_collaboration_rag.baseline_rag\u001b[0m:\u001b[36mask\u001b[0m:\u001b[36m326\u001b[0m - \u001b[1mAnswer:\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:33.918\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msme_kt_zh_collaboration_rag.baseline_rag\u001b[0m:\u001b[36mask\u001b[0m:\u001b[36m327\u001b[0m - \u001b[1m  Here are the environmental impact indicators from the provided sources, separated into categories:\n",
      "\n",
      "**1. Greenhouse Gas Emissions (GWP-GHG):**\n",
      "- Upstream: 2.58E+00 kg CO2 eq.\n",
      "- Core: 5.09E+00 kg CO2 eq.\n",
      "- Downstream: 1.33E+00 kg CO2 eq.\n",
      "- Total: 9.00E+00 kg CO2 eq.\n",
      "\n",
      "**2. Global Warming Potential (GWP):**\n",
      "- Upstream:\n",
      "  - GHG: 4.57E-01 kg CO2 eq.\n",
      "  - Non-GHG: 3.63E-01 kg CO2 eq.\n",
      "- Core:\n",
      "  - GHG: 8.94E+00 kg CO2 eq.\n",
      "  - Non-GHG: 2.17E+00 kg CO2 eq.\n",
      "- Downstream:\n",
      "  - GHG: 2.56E+00 kg CO2 eq.\n",
      "  - Non-GHG: 3.83E-01 kg CO2 eq.\n",
      "- Total:\n",
      "  - GHG: 9.47E+00 kg CO2 eq.\n",
      "  - Non-GHG: 2.75E+00 kg CO2 eq.\n",
      "\n",
      "**3. Particulate Matter (PM):**\n",
      "- Disease incidences: 1.41E-07\n",
      "\n",
      "**4. Ionising Radiation:**\n",
      "- Human health: 2.38E+00 kBq U235 eq.\n",
      "\n",
      "**5. Ecotoxicity Fresh Water (EFW):**\n",
      "- CTUe: 4.67E+01\n",
      "\n",
      "**6. Human Toxicity:**\n",
      "- Cancer (HTC): 1.17E-09 CTUh\n",
      "- Non-cancer (HTNC): 5.66E-08 CTUh\n",
      "\n",
      "**7. Land Use:**\n",
      "- Pt: 3.67E+01\n",
      "\n",
      "**8. Waste Disposal:**\n",
      "- Hazardous waste disposed: 8.84E-09 kg\n",
      "- Non-hazardous waste disposed: 3.83E-02 kg\n",
      "- Radioactive waste disposed: 2.45E-02 kg\n",
      "\n",
      "**9. Material for Recycling:**\n",
      "- Total: 2.50E+01 kg\n",
      "\n",
      "**10. Net Use of Fresh Water:**\n",
      "- Total: 4.03E-02 m³\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:33.918\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msme_kt_zh_collaboration_rag.baseline_rag\u001b[0m:\u001b[36mask\u001b[0m:\u001b[36m328\u001b[0m - \u001b[1mSources (5):\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:33.919\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msme_kt_zh_collaboration_rag.baseline_rag\u001b[0m:\u001b[36mask\u001b[0m:\u001b[36m331\u001b[0m - \u001b[1m  '1_Product-Life-Cycle-Accounting-Reporting-Standard_041613.pdf'  |  '# **_P_**'\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:33.919\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msme_kt_zh_collaboration_rag.baseline_rag\u001b[0m:\u001b[36mask\u001b[0m:\u001b[36m331\u001b[0m - \u001b[1m  '3_EPD_pallet_relicyc.pdf'  |  '# LCA INFORMATION'\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:33.920\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msme_kt_zh_collaboration_rag.baseline_rag\u001b[0m:\u001b[36mask\u001b[0m:\u001b[36m331\u001b[0m - \u001b[1m  '3_EPD_pallet_relicyc.pdf'  |  '# REFERENCES'\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:33.920\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msme_kt_zh_collaboration_rag.baseline_rag\u001b[0m:\u001b[36mask\u001b[0m:\u001b[36m331\u001b[0m - \u001b[1m  '2_EPD_pallet_CPR.pdf'  |  '#### 6. Environmental performance assessment'\u001b[0m\n",
      "\u001b[32m2026-02-20 17:06:33.920\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msme_kt_zh_collaboration_rag.baseline_rag\u001b[0m:\u001b[36mask\u001b[0m:\u001b[36m331\u001b[0m - \u001b[1m  '4_EPD_pallet_Stabilplastik.pdf'  |  '#### **Results of the environmental performance indicators**'\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are the environmental impact indicators from the provided sources, separated into categories:\n",
      "\n",
      "**1. Greenhouse Gas Emissions (GWP-GHG):**\n",
      "- Upstream: 2.58E+00 kg CO2 eq.\n",
      "- Core: 5.09E+00 kg CO2 eq.\n",
      "- Downstream: 1.33E+00 kg CO2 eq.\n",
      "- Total: 9.00E+00 kg CO2 eq.\n",
      "\n",
      "**2. Global Warming Potential (GWP):**\n",
      "- Upstream:\n",
      "  - GHG: 4.57E-01 kg CO2 eq.\n",
      "  - Non-GHG: 3.63E-01 kg CO2 eq.\n",
      "- Core:\n",
      "  - GHG: 8.94E+00 kg CO2 eq.\n",
      "  - Non-GHG: 2.17E+00 kg CO2 eq.\n",
      "- Downstream:\n",
      "  - GHG: 2.56E+00 kg CO2 eq.\n",
      "  - Non-GHG: 3.83E-01 kg CO2 eq.\n",
      "- Total:\n",
      "  - GHG: 9.47E+00 kg CO2 eq.\n",
      "  - Non-GHG: 2.75E+00 kg CO2 eq.\n",
      "\n",
      "**3. Particulate Matter (PM):**\n",
      "- Disease incidences: 1.41E-07\n",
      "\n",
      "**4. Ionising Radiation:**\n",
      "- Human health: 2.38E+00 kBq U235 eq.\n",
      "\n",
      "**5. Ecotoxicity Fresh Water (EFW):**\n",
      "- CTUe: 4.67E+01\n",
      "\n",
      "**6. Human Toxicity:**\n",
      "- Cancer (HTC): 1.17E-09 CTUh\n",
      "- Non-cancer (HTNC): 5.66E-08 CTUh\n",
      "\n",
      "**7. Land Use:**\n",
      "- Pt: 3.67E+01\n",
      "\n",
      "**8. Waste Disposal:**\n",
      "- Hazardous waste disposed: 8.84E-09 kg\n",
      "- Non-hazardous waste disposed: 3.83E-02 kg\n",
      "- Radioactive waste disposed: 2.45E-02 kg\n",
      "\n",
      "**9. Material for Recycling:**\n",
      "- Total: 2.50E+01 kg\n",
      "\n",
      "**10. Net Use of Fresh Water:**\n",
      "- Total: 4.03E-02 m³\n"
     ]
    }
   ],
   "source": [
    "answer_claim = await ask(\n",
    "    agent,\n",
    "    \"How much lower is the carbon footprint of tesa ECO tape compared to standard tape?\",\n",
    ")\n",
    "print(answer_claim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5948f40",
   "metadata": {},
   "source": [
    "> Can you think of and find **other failure modes**?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p0-multiturn-md",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Multi-Turn Conversation\n",
    "\n",
    "The `ask()` function accepts a `history` argument, a list of prior `LLMMessage` objects. When history is provided the agent first **rewrites the query** to be self-contained (*\"it\"* becomes the actual product name) before retrieval.\n",
    "\n",
    "This prevents the retriever from embedding vague pronouns that match nothing in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "p0-multiturn-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "from conversational_toolkit.llms.base import LLMMessage, Roles\n",
    "\n",
    "history: list[LLMMessage] = []\n",
    "\n",
    "\n",
    "async def conversation_turn(query: str) -> str:\n",
    "    global history\n",
    "    answer = await agent.answer(QueryWithContext(query=query, history=history))\n",
    "    history.append(LLMMessage(role=Roles.USER, content=query))\n",
    "    history.append(LLMMessage(role=Roles.ASSISTANT, content=answer.content))\n",
    "    return answer.content\n",
    "\n",
    "\n",
    "# Turn 1: ask about a specific product\n",
    "reply1 = await conversation_turn(\n",
    "    \"Which pallets in our portfolio have a third-party verified EPD?\"\n",
    ")\n",
    "print(\"User: Which pallets in our portfolio have a third-party verified EPD?\")\n",
    "print(f\"Assistant: {reply1}\\n\")\n",
    "\n",
    "# Turn 2: follow-up using a pronoun — the agent should resolve \"it\" before retrieval\n",
    "reply2 = await conversation_turn(\n",
    "    \"What is the GWP figure reported in it for the Logypal 1?\"\n",
    ")\n",
    "print(\"User: What is the GWP figure reported in it for the Logypal 1?\")\n",
    "print(f\"Assistant: {reply2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p0-arch-md",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Running the Full Pipeline in One Call\n",
    "\n",
    "The `run_pipeline()` convenience function executes all five steps end-to-end. It is also\n",
    "what the `__main__` entry point calls.\n",
    "\n",
    "Use it for quick one-shot queries. Use the individual step functions above when you need\n",
    "to inspect intermediate results or iterate on a specific stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "p0-arch-code",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-02-20 17:04:08.628\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msme_kt_zh_collaboration_rag.baseline_rag\u001b[0m:\u001b[36mrun_pipeline\u001b[0m:\u001b[36m353\u001b[0m - \u001b[1mStarting Baseline RAG pipeline\u001b[0m\n",
      "\u001b[32m2026-02-20 17:04:08.628\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msme_kt_zh_collaboration_rag.baseline_rag\u001b[0m:\u001b[36mrun_pipeline\u001b[0m:\u001b[36m354\u001b[0m - \u001b[1mbackend='ollama'  model=None  max_files=5  reset_vs=False  top_k=5\u001b[0m\n",
      "\u001b[32m2026-02-20 17:04:08.631\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msme_kt_zh_collaboration_rag.baseline_rag\u001b[0m:\u001b[36mload_chunks\u001b[0m:\u001b[36m178\u001b[0m - \u001b[1mChunking 5 files from /Users/pkoerner/Desktop/Kanton_Zurich/sme-kt-zh-collaboration-rag/data\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-02-20 17:04:17.095\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msme_kt_zh_collaboration_rag.baseline_rag\u001b[0m:\u001b[36mload_chunks\u001b[0m:\u001b[36m187\u001b[0m - \u001b[34m\u001b[1m  1_Product-Life-Cycle-Accounting-Reporting-Standard_041613.pdf: 32 chunks\u001b[0m\n",
      "\u001b[32m2026-02-20 17:04:17.942\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msme_kt_zh_collaboration_rag.baseline_rag\u001b[0m:\u001b[36mload_chunks\u001b[0m:\u001b[36m187\u001b[0m - \u001b[34m\u001b[1m  2_EPD_pallet_CPR.pdf: 11 chunks\u001b[0m\n",
      "\u001b[32m2026-02-20 17:04:19.326\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msme_kt_zh_collaboration_rag.baseline_rag\u001b[0m:\u001b[36mload_chunks\u001b[0m:\u001b[36m187\u001b[0m - \u001b[34m\u001b[1m  3_EPD_pallet_relicyc.pdf: 17 chunks\u001b[0m\n",
      "\u001b[32m2026-02-20 17:04:20.635\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msme_kt_zh_collaboration_rag.baseline_rag\u001b[0m:\u001b[36mload_chunks\u001b[0m:\u001b[36m187\u001b[0m - \u001b[34m\u001b[1m  4_EPD_pallet_Stabilplastik.pdf: 2 chunks\u001b[0m\n",
      "\u001b[32m2026-02-20 17:04:20.635\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msme_kt_zh_collaboration_rag.baseline_rag\u001b[0m:\u001b[36mload_chunks\u001b[0m:\u001b[36m187\u001b[0m - \u001b[34m\u001b[1m  ART_customer_inquiry_frische_felder.md: 6 chunks\u001b[0m\n",
      "\u001b[32m2026-02-20 17:04:20.636\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msme_kt_zh_collaboration_rag.baseline_rag\u001b[0m:\u001b[36mload_chunks\u001b[0m:\u001b[36m191\u001b[0m - \u001b[1mDone, 68 chunks total\u001b[0m\n",
      "\u001b[32m2026-02-20 17:04:20.636\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msme_kt_zh_collaboration_rag.baseline_rag\u001b[0m:\u001b[36minspect_chunks\u001b[0m:\u001b[36m203\u001b[0m - \u001b[1m------ Chunk inspection -------\u001b[0m\n",
      "\u001b[32m2026-02-20 17:04:20.636\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msme_kt_zh_collaboration_rag.baseline_rag\u001b[0m:\u001b[36minspect_chunks\u001b[0m:\u001b[36m204\u001b[0m - \u001b[1mTotal chunks: 68; Source files: 5\u001b[0m\n",
      "\u001b[32m2026-02-20 17:04:20.636\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msme_kt_zh_collaboration_rag.baseline_rag\u001b[0m:\u001b[36minspect_chunks\u001b[0m:\u001b[36m206\u001b[0m - \u001b[1m1_Product-Life-Cycle-Accounting-Reporting-Standard_041613.pdf: 32 chunks\u001b[0m\n",
      "\u001b[32m2026-02-20 17:04:20.637\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msme_kt_zh_collaboration_rag.baseline_rag\u001b[0m:\u001b[36minspect_chunks\u001b[0m:\u001b[36m206\u001b[0m - \u001b[1m2_EPD_pallet_CPR.pdf: 11 chunks\u001b[0m\n",
      "\u001b[32m2026-02-20 17:04:20.637\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msme_kt_zh_collaboration_rag.baseline_rag\u001b[0m:\u001b[36minspect_chunks\u001b[0m:\u001b[36m206\u001b[0m - \u001b[1m3_EPD_pallet_relicyc.pdf: 17 chunks\u001b[0m\n",
      "\u001b[32m2026-02-20 17:04:20.637\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msme_kt_zh_collaboration_rag.baseline_rag\u001b[0m:\u001b[36minspect_chunks\u001b[0m:\u001b[36m206\u001b[0m - \u001b[1m4_EPD_pallet_Stabilplastik.pdf: 2 chunks\u001b[0m\n",
      "\u001b[32m2026-02-20 17:04:20.637\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msme_kt_zh_collaboration_rag.baseline_rag\u001b[0m:\u001b[36minspect_chunks\u001b[0m:\u001b[36m206\u001b[0m - \u001b[1mART_customer_inquiry_frische_felder.md: 6 chunks\u001b[0m\n",
      "\u001b[32m2026-02-20 17:04:20.638\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msme_kt_zh_collaboration_rag.baseline_rag\u001b[0m:\u001b[36minspect_chunks\u001b[0m:\u001b[36m207\u001b[0m - \u001b[1mSample (first 5):\u001b[0m\n",
      "\u001b[32m2026-02-20 17:04:20.638\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msme_kt_zh_collaboration_rag.baseline_rag\u001b[0m:\u001b[36minspect_chunks\u001b[0m:\u001b[36m210\u001b[0m - \u001b[1mSource and title: [1_Product-Life-Cycle-Accounting-Reporting-Standard_041613.pdf] '###### **_01 Introduction_**'\u001b[0m\n",
      "\u001b[32m2026-02-20 17:04:20.638\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msme_kt_zh_collaboration_rag.baseline_rag\u001b[0m:\u001b[36minspect_chunks\u001b[0m:\u001b[36m211\u001b[0m - \u001b[1mChunk content: '###### **_01 Introduction_**'\u001b[0m\n",
      "\u001b[32m2026-02-20 17:04:20.638\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msme_kt_zh_collaboration_rag.baseline_rag\u001b[0m:\u001b[36minspect_chunks\u001b[0m:\u001b[36m210\u001b[0m - \u001b[1mSource and title: [1_Product-Life-Cycle-Accounting-Reporting-Standard_041613.pdf] '# **_E_**'\u001b[0m\n",
      "\u001b[32m2026-02-20 17:04:20.639\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msme_kt_zh_collaboration_rag.baseline_rag\u001b[0m:\u001b[36minspect_chunks\u001b[0m:\u001b[36m211\u001b[0m - \u001b[1mChunk content: '# **_E_**\\n\\n\\n\\n_**missions of the anthropogenic greenhouse gases (GHG) that drive climate change**_\\n\\n_**and its impacts around the world are growing. According to climate scientists,**_\\n\\n_**global carbo'\u001b[0m\n",
      "\u001b[32m2026-02-20 17:04:20.639\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msme_kt_zh_collaboration_rag.baseline_rag\u001b[0m:\u001b[36minspect_chunks\u001b[0m:\u001b[36m210\u001b[0m - \u001b[1mSource and title: [1_Product-Life-Cycle-Accounting-Reporting-Standard_041613.pdf] '###### **_02 Defining Business Goals_**'\u001b[0m\n",
      "\u001b[32m2026-02-20 17:04:20.639\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msme_kt_zh_collaboration_rag.baseline_rag\u001b[0m:\u001b[36minspect_chunks\u001b[0m:\u001b[36m211\u001b[0m - \u001b[1mChunk content: '###### **_02 Defining Business Goals_**'\u001b[0m\n",
      "\u001b[32m2026-02-20 17:04:20.639\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msme_kt_zh_collaboration_rag.baseline_rag\u001b[0m:\u001b[36minspect_chunks\u001b[0m:\u001b[36m210\u001b[0m - \u001b[1mSource and title: [1_Product-Life-Cycle-Accounting-Reporting-Standard_041613.pdf] '# **_C_**'\u001b[0m\n",
      "\u001b[32m2026-02-20 17:04:20.639\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msme_kt_zh_collaboration_rag.baseline_rag\u001b[0m:\u001b[36minspect_chunks\u001b[0m:\u001b[36m211\u001b[0m - \u001b[1mChunk content: '# **_C_**\\n\\n\\n\\n_**ompanies should first identify their business goals before conducting product**_\\n\\n_**GHG inventories. Doing so can bring clarity and assist in selecting the appropriate**_\\n\\n_**methodol'\u001b[0m\n",
      "\u001b[32m2026-02-20 17:04:20.639\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msme_kt_zh_collaboration_rag.baseline_rag\u001b[0m:\u001b[36minspect_chunks\u001b[0m:\u001b[36m210\u001b[0m - \u001b[1mSource and title: [1_Product-Life-Cycle-Accounting-Reporting-Standard_041613.pdf] '###### **_03 Summary of Steps and Requirements_**'\u001b[0m\n",
      "\u001b[32m2026-02-20 17:04:20.640\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msme_kt_zh_collaboration_rag.baseline_rag\u001b[0m:\u001b[36minspect_chunks\u001b[0m:\u001b[36m211\u001b[0m - \u001b[1mChunk content: '###### **_03 Summary of Steps and Requirements_**\\n\\n [12] _**Product Life Cycle Accounting and Reporting Standard**_'\u001b[0m\n",
      "\u001b[32m2026-02-20 17:04:22.574\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.embeddings.sentence_transformer\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m57\u001b[0m - \u001b[34m\u001b[1mSentence Transformer embeddings model loaded: sentence-transformers/all-MiniLM-L6-v2 with kwargs: {}\u001b[0m\n",
      "\u001b[32m2026-02-20 17:04:22.581\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msme_kt_zh_collaboration_rag.baseline_rag\u001b[0m:\u001b[36mbuild_vector_store\u001b[0m:\u001b[36m235\u001b[0m - \u001b[1mVector store already contains 78 chunks — skipping embedding.\u001b[0m\n",
      "\u001b[32m2026-02-20 17:04:22.598\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.embeddings.sentence_transformer\u001b[0m:\u001b[36mget_embeddings\u001b[0m:\u001b[36m76\u001b[0m - \u001b[34m\u001b[1msentence-transformers/all-MiniLM-L6-v2 embeddings size: (1, 384)\u001b[0m\n",
      "\u001b[32m2026-02-20 17:04:22.600\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msme_kt_zh_collaboration_rag.baseline_rag\u001b[0m:\u001b[36minspect_retrieval\u001b[0m:\u001b[36m274\u001b[0m - \u001b[1mRetrieval for query: 'What sustainability certifications do the pallets in the portfolio have?'\u001b[0m\n",
      "\u001b[32m2026-02-20 17:04:22.600\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msme_kt_zh_collaboration_rag.baseline_rag\u001b[0m:\u001b[36mbuild_llm\u001b[0m:\u001b[36m129\u001b[0m - \u001b[1mLLM backend: Ollama (mistral-nemo:12b)\u001b[0m\n",
      "\u001b[32m2026-02-20 17:04:22.611\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m60\u001b[0m - \u001b[34m\u001b[1mOllama LLM loaded: mistral-nemo:12b; temperature: 0.3; seed: 42; tools: None; response_format: None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:04:22.612\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msme_kt_zh_collaboration_rag.baseline_rag\u001b[0m:\u001b[36mbuild_agent\u001b[0m:\u001b[36m306\u001b[0m - \u001b[1mRAG agent ready (top_k=5  query_expansion=0)\u001b[0m\n",
      "\u001b[32m2026-02-20 17:04:22.612\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msme_kt_zh_collaboration_rag.baseline_rag\u001b[0m:\u001b[36mask\u001b[0m:\u001b[36m323\u001b[0m - \u001b[1mQuery: 'What sustainability certifications do the pallets in the portfolio have?'\u001b[0m\n",
      "\u001b[32m2026-02-20 17:04:22.625\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.embeddings.sentence_transformer\u001b[0m:\u001b[36mget_embeddings\u001b[0m:\u001b[36m76\u001b[0m - \u001b[34m\u001b[1msentence-transformers/all-MiniLM-L6-v2 embeddings size: (1, 384)\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top-5 retrieved chunks (returned=5; showing a maximum of 1000 content characters):\n",
      "  [1] score=0.8083  file='3_EPD_pallet_relicyc.pdf'  title='# 40 YEARS OF SUSTAINABLE INNOVATION'\n",
      "       '# 40 YEARS OF SUSTAINABLE INNOVATION\\n\\nRelicyc has a long history in managing end-of-life plastic\\n\\nand wooden pallets: from recovery to reintroduction into the\\n\\nmarketplace, it gives the material a new lease on life. Over 40\\n\\nyears of experience has led the company to become a prominent\\n\\nplayer in the field and a partner that today’s environmental efficient customers can rely on.\\n\\nThe need for sustainability is what drives our model, whose focus\\n\\nis on re-using resources at the end of their life and routing them\\n\\nproperly for recycling so they can find new uses while bringing the\\n\\nbusinesses involved new value.'\n",
      "  [2] score=0.8940  file='3_EPD_pallet_relicyc.pdf'  title='# CONTENT DECLARATION'\n",
      "       '# CONTENT DECLARATION\\n\\nLogypal 1, classified as distribution packaging, is mainly composed (> 99%) of polyolefins and other\\n\\ntrace materials. The product under this study has a recycled plastic content of 100% and recycled\\n\\nmaterials are post-consumer plastic waste.\\n\\n\\n\\n\\n\\n_Table 3: Content declaration of pallet Logypal 1_\\n\\n The reported recycled content has been taken from certificates issued by the Kiwa certification\\n\\nbody (Accr. N.069B). For the product covered by the EPD, the reference certificate is Accr.\\n\\nNo. 021/2020 (the certificate dated 26-03-2023 shows a value of 100%, while the certificate dated\\n\\n04-02-2020 shows a value of 95%).\\n\\n |Col1|Col2|Col3|Col4|Col5|Col6|Col7| |---|---|---|---|---|---|---| |||||||| |||||||| |||||||| |||||||| |||||||| |||||||| |||||||15|'\n",
      "  [3] score=0.9262  file='3_EPD_pallet_relicyc.pdf'  title='# PRODUCT INFORMATION'\n",
      "       '# PRODUCT INFORMATION\\n\\nThis Environmental Product Declaration concerns the environmental\\n\\nimpacts associated with a model of recycled polypropylene pallet:\\n\\n Logypal 1 [®]\\n\\n All these pallets are produced with secondary raw materials\\n\\n(a mix of polypropylene and high density polyethylene).\\n\\nThese new plastic pallets are the real alternative to the ISPM-15\\n\\ntreated wooden pallet (HT standard phytosanitary treatment that\\n\\ncertifies the suitability of the material to the international regulations\\n\\ndrawn up by the IPPC), having a comparable cost, but without the\\n\\nbureaucracy and mandatory certifications for purchase.\\n\\nThese products are also light, resistant, washable and resistant to\\n\\nmold and humidity.\\n\\nThe main characteristics of the model of pallet under study are shown\\n\\nin the following table:\\n\\n\\n\\n_Table 1: Main characteristics of the studied pallet model_\\n\\n The packaging under study are intended to handling and transport\\n\\nof various kind of goods; for this reason falls into the catego'\n",
      "  [4] score=0.9374  file='3_EPD_pallet_relicyc.pdf'  title='# CLOSER TO CUSTOMERS THROUGH SHARED GOALS'\n",
      "       '# CLOSER TO CUSTOMERS THROUGH SHARED GOALS\\n\\nThe Relicyc system is specific, unique, exclusive and circular:\\n\\nwhatever way you look at it, it efficiently transforms the use\\n\\nand recovery of plastic pallets into the starting point of a\\n\\npartnership based on shared sustainability goals.\\n\\nRelicyc operates according to a modern “business system” view.\\n\\nIt starts off from the ideas of customers and suppliers to arrive\\n\\nat developing shared goals such as:\\n\\n Product efficiency\\n\\nLogistics efficiency\\n\\nService effectiveness\\n\\nKindness to the environment\\n\\n\\n\\n6\\n\\n\\n\\n7'\n",
      "  [5] score=0.9624  file='2_EPD_pallet_CPR.pdf'  title='#### 3. Product information'\n",
      "       \"#### 3. Product information\\n\\nUN CPC code\\n\\n 36490 Other articles for the conveyance or packing of goods, of plastics; stoppers, lids, caps and other closures, of plastics.\\n\\n Product description\\n\\n The Noè pallet, identified by code PR12, is a product developed and patented by CPR SYSTEM and produced by Newpal S.p.A. It is used by the CPR SYSTEM Group as a support for the plastic folding crates managed within the company's pooling system.\\n\\n This four-way, non-reversible pallet consists of a platform and crosspieces connected by a male-female locking system on the blocks. It is equipped with a Smart Label traceability tag that includes an RFID TAG, an EAN128 barcode, and a Data Matrix, allowing monitoring throughout the entire logistics chain. Additionally, the product is recyclable at the end of its life, easily washable and sanitizable, resistant to rust, and stable in determining its tare weight.\\n\\n The pallet has been designed with containment edges and specific technical features to mi\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-02-20 17:04:47.123\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:04:47.106784Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='Based', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:04:47.173\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:04:47.17261Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' on', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:04:47.247\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:04:47.246767Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' the', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:04:47.317\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:04:47.317191Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' provided', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:04:47.388\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:04:47.388176Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' sources', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:04:47.462\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:04:47.46124Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=',', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:04:47.534\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:04:47.533541Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' here', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:04:47.606\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:04:47.606131Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' are', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:04:47.678\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:04:47.678285Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' the', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:04:47.750\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:04:47.750139Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' sustainability', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:04:47.823\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:04:47.823485Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' cert', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:04:47.896\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:04:47.896256Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='ifications', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:04:47.968\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:04:47.968378Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' and', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:04:48.040\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:04:48.040274Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' recycling', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:04:48.115\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:04:48.115354Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' information', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:04:48.188\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:04:48.187781Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' for', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:04:48.266\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:04:48.265742Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' the', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:04:48.338\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:04:48.337831Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' pal', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:04:48.408\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:04:48.408246Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='lets', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:04:48.483\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:04:48.483336Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' in', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:04:48.578\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:04:48.575054Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' the', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:04:48.650\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:04:48.650547Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' portfolio', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:04:48.723\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:04:48.723285Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=':\\n\\n', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:04:48.795\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:04:48.795152Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='1', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:04:48.866\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:04:48.86575Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='.', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:04:48.937\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:04:48.937761Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' **', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:04:49.014\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:04:49.014639Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='Log', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:04:49.091\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:04:49.091043Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='yp', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:04:49.170\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:04:49.170447Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='al', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:04:49.245\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:04:49.245424Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' ', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:04:49.321\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:04:49.32112Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='1', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:04:49.395\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:04:49.395645Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='**:\\n', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:04:49.472\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:04:49.471837Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='  ', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:04:49.550\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:04:49.549221Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' -', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:04:49.628\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:04:49.627751Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' The', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:04:49.701\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:04:49.701089Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' Log', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:04:49.776\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:04:49.775763Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='yp', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:04:49.851\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:04:49.851101Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='al', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:04:49.929\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:04:49.929526Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' ', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:04:50.005\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:04:50.00505Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='1', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:04:50.078\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:04:50.078076Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' pal', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:04:50.151\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:04:50.151046Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='let', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:04:50.228\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:04:50.228458Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' is', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:04:50.307\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:04:50.306937Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' made', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:04:50.391\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:04:50.391429Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' from', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:04:50.467\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:04:50.467532Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' recycled', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:04:50.543\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:04:50.542924Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' plastic', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:04:50.619\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:04:50.619286Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' content', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:04:50.696\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:04:50.695824Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' of', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:04:50.770\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:04:50.770004Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' ', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:04:50.842\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:04:50.842245Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='1', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:04:50.914\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:04:50.914621Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='0', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:04:50.989\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:04:50.988759Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='0', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:04:51.062\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:04:51.062523Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='%,', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:04:51.138\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:04:51.138579Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' with', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:04:51.224\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:04:51.224438Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' recycled', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:04:51.304\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:04:51.304243Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' materials', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:04:51.378\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:04:51.378561Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' being', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:04:51.455\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:04:51.455325Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' post', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:04:51.529\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:04:51.52961Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='-consum', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:04:51.606\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:04:51.606081Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='er', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:04:51.680\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:04:51.679929Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' plastic', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:04:51.755\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:04:51.754664Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' waste', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:04:51.828\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:04:51.828262Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' (', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:04:51.905\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:04:51.904829Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='Source', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:04:51.978\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:04:51.978206Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=':', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:04:52.051\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:04:52.051032Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' a', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:04:52.128\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:04:52.128359Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='7', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:04:52.221\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:04:52.217734Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='3', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:04:52.303\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:04:52.303117Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='3', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:04:52.380\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:04:52.380127Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='b', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:04:52.460\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:04:52.459507Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='8', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:04:52.539\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:04:52.538631Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='4', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:04:52.615\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:04:52.615418Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='8', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:04:52.694\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:04:52.694597Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='-', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:04:52.771\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:04:52.771319Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='ef', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:04:52.851\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:04:52.851642Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='4', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:04:52.928\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:04:52.928499Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='7', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:04:53.006\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:04:53.006164Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='-', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:04:53.081\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:04:53.081161Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='4', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:04:53.156\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:04:53.156598Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='6', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:04:53.234\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:04:53.234559Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='9', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:04:53.314\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:04:53.314117Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='0', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:04:53.404\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:04:53.404296Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='-b', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:04:53.486\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:04:53.485922Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='d', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:04:53.565\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:04:53.565509Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='0', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:04:53.642\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:04:53.642247Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='c', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:04:53.719\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:04:53.719084Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='-c', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:04:53.797\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:04:53.797403Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='f', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:04:53.881\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:04:53.880818Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='4', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:04:53.966\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:04:53.965852Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='1', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:04:54.043\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:04:54.042647Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='a', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:04:54.124\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:04:54.123999Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='1', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:04:54.198\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:04:54.198028Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='3', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:04:54.274\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:04:54.273674Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='4', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:04:54.352\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:04:54.352656Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='abc', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:04:54.435\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:04:54.433891Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='3', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:04:54.519\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:04:54.519247Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=').\\n', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:04:54.590\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:04:54.589734Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='  ', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:04:54.660\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:04:54.659865Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' -', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:04:54.731\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:04:54.731229Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' It', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:04:54.802\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:04:54.801755Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' has', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:04:54.874\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:04:54.874402Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' Ki', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:04:54.947\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:04:54.946849Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='wa', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:04:55.018\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:04:55.018366Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' certification', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:04:55.090\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:04:55.089859Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' for', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:04:55.160\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:04:55.16036Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' its', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:04:55.232\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:04:55.231644Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' recycled', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:04:55.302\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:04:55.301954Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' content', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:04:55.372\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:04:55.37227Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' (', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:04:55.443\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:04:55.442624Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='Acc', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:04:55.514\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:04:55.514045Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='r', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:04:55.586\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:04:55.585811Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='.', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:04:55.657\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:04:55.6569Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' N', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:04:55.727\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:04:55.727066Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='.', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:04:55.798\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:04:55.797509Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='0', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:04:55.868\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:04:55.868291Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='6', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:04:55.939\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:04:55.938941Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='9', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:04:56.009\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:04:56.009247Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='B', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:04:56.081\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:04:56.080929Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=')', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:04:56.156\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:04:56.151673Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' (', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:04:56.228\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:04:56.227725Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='Source', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:04:56.300\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:04:56.300538Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=':', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:04:56.373\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:04:56.372711Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' a', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:04:56.445\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:04:56.44507Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='7', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:04:56.518\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:04:56.51826Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='3', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:04:56.591\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:04:56.590912Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='3', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:04:56.663\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:04:56.66328Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='b', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:04:56.743\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:04:56.742796Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='8', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:04:56.814\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:04:56.814327Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='4', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:04:56.886\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:04:56.885661Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='8', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:04:56.962\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:04:56.962517Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='-', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:04:57.038\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:04:57.038369Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='ef', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:04:57.114\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:04:57.113753Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='4', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:04:57.192\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:04:57.191677Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='7', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:04:57.271\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:04:57.270992Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='-', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:04:57.343\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:04:57.343378Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='4', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:04:57.417\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:04:57.417063Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='6', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:04:57.495\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:04:57.495503Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='9', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:04:57.572\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:04:57.572389Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='0', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:04:57.644\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:04:57.644433Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='-b', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:04:57.724\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:04:57.722526Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='d', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:04:57.809\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:04:57.808769Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='0', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:04:57.883\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:04:57.882914Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='c', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:04:57.958\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:04:57.958172Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='-c', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:04:58.030\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:04:58.030045Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='f', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:04:58.101\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:04:58.101441Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='4', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:04:58.175\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:04:58.175327Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='1', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:04:58.249\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:04:58.249401Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='a', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:04:58.321\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:04:58.321362Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='1', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:04:58.391\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:04:58.391386Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='3', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:04:58.462\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:04:58.46198Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='4', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:04:58.533\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:04:58.532838Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='abc', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:04:58.606\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:04:58.60631Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='3', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:04:58.678\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:04:58.678147Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=').\\n\\n', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:04:58.750\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:04:58.749961Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='2', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:04:58.822\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:04:58.822281Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='.', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:04:58.897\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:04:58.896763Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' **', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:04:58.968\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:04:58.968243Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='No', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:04:59.040\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:04:59.040168Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='è', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:04:59.129\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:04:59.128675Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' Pal', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:04:59.204\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:04:59.203955Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='let', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:04:59.274\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:04:59.274238Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='**:\\n', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:04:59.347\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:04:59.34691Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='  ', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:04:59.419\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:04:59.419594Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' -', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:04:59.491\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:04:59.491487Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' The', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:04:59.563\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:04:59.563469Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' No', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:04:59.638\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:04:59.635812Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='è', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:04:59.709\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:04:59.708886Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' pal', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:04:59.781\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:04:59.780863Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='let', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:04:59.853\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:04:59.852526Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' is', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:04:59.924\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:04:59.924358Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' made', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:04:59.997\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:04:59.997112Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' from', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:00.078\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:00.078235Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' secondary', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:00.153\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:00.153085Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' materials', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:00.225\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:00.225447Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' sourced', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:00.299\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:00.298734Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' from', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:00.372\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:00.371975Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' various', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:00.447\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:00.446935Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' recycling', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:00.525\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:00.52504Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' processes', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:00.597\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:00.597429Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=',', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:00.670\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:00.670068Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' including', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:00.742\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:00.741861Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' post', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:00.813\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:00.813162Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='-consum', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:00.885\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:00.885119Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='er', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:00.963\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:00.96329Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' beverage', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:01.035\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:01.035601Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' cart', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:01.108\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:01.107884Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='ons', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:01.180\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:01.179997Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' and', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:01.252\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:01.251901Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' obsolete', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:01.323\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:01.322617Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' No', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:01.395\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:01.395186Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='è', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:01.466\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:01.466166Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' pal', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:01.537\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:01.537206Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='lets', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:01.609\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:01.608834Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' (', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:01.679\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:01.67927Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='Source', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:01.752\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:01.751582Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=':', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:01.822\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:01.822106Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' ', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:01.893\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:01.892704Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='1', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:01.972\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:01.971684Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='d', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:02.045\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:02.045414Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='2', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:02.116\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:02.116294Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='a', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:02.187\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:02.186933Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='1', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:02.258\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:02.257626Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='b', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:02.329\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:02.328542Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='3', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:02.408\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:02.408146Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='5', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:02.492\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:02.4919Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='-', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:02.574\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:02.573599Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='6', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:02.655\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:02.655522Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='7', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:02.734\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:02.734553Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='ae', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:02.809\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:02.809236Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='-', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:02.888\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:02.887685Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='4', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:02.966\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:02.966272Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='a', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:03.046\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:03.046356Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='7', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:03.121\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:03.121234Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='6', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:03.197\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:03.196972Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='-a', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:03.274\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:03.274332Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='e', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:03.349\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:03.34944Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='6', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:03.425\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:03.425158Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='3', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:03.497\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:03.497615Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='-f', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:03.570\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:03.569881Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='5', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:03.644\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:03.644545Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='5', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:03.718\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:03.717945Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='5', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:03.793\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:03.792733Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='7', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:03.868\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:03.867804Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='6', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:03.939\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:03.939151Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='1', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:04.012\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:04.011631Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='1', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:04.083\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:04.08277Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='d', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:04.157\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:04.157362Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='9', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:04.232\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:04.231995Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='3', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:04.306\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:04.30663Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='d', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:04.380\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:04.380498Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=').\\n', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:04.455\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:04.455195Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='  ', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:04.529\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:04.52956Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' -', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:04.602\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:04.602513Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' It', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:04.676\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:04.675715Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' is', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:04.749\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:04.749026Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' rec', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:04.822\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:04.822082Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='ycl', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:04.895\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:04.895152Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='able', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:04.968\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:04.968383Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' at', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:05.042\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:05.041853Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' the', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:05.118\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:05.11827Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' end', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:05.192\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:05.191836Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' of', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:05.266\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:05.266251Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' its', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:05.340\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:05.33974Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' life', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:05.413\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:05.413327Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=',', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:05.486\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:05.486139Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' easily', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:05.560\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:05.559892Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' wash', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:05.634\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:05.634377Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='able', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:05.707\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:05.707639Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' and', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:05.781\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:05.781573Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' sanit', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:05.855\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:05.854934Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='izable', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:05.926\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:05.925546Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=',', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:05.996\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:05.9963Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' resistant', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:06.067\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:06.067188Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' to', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:06.138\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:06.137824Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' rust', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:06.212\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:06.212141Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=',', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:06.288\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:06.287717Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' and', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:06.361\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:06.361622Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' stable', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:06.434\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:06.434647Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' in', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:06.508\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:06.508049Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' determining', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:06.580\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:06.580338Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' its', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:06.652\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:06.651612Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' tare', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:06.732\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:06.730691Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' weight', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:06.817\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:06.817523Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' (', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:06.895\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:06.895004Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='Source', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:06.976\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:06.976139Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=':', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:07.051\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:07.051147Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' ', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:07.126\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:07.126273Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='1', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:07.202\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:07.20211Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='d', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:07.278\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:07.277748Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='2', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:07.351\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:07.351588Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='a', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:07.423\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:07.422727Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='1', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:07.494\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:07.493547Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='b', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:07.572\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:07.572065Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='3', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:07.646\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:07.645862Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='5', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:07.718\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:07.717882Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='-', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:07.791\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:07.791521Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='6', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:07.864\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:07.863932Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='7', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:07.937\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:07.937413Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='ae', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:08.010\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:08.010433Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='-', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:08.092\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:08.091911Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='4', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:08.164\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:08.164604Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='a', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:08.236\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:08.235637Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='7', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:08.309\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:08.308766Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='6', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:08.384\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:08.384646Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='-a', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:08.459\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:08.458767Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='e', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:08.530\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:08.52997Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='6', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:08.602\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:08.601588Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='3', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:08.674\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:08.674209Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='-f', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:08.747\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:08.747Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='5', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:08.820\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:08.819667Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='5', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:08.893\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:08.892665Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='5', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:08.965\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:08.965371Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='7', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:09.038\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:09.03787Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='6', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:09.110\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:09.11026Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='1', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:09.182\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:09.182373Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='1', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:09.254\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:09.254128Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='d', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:09.326\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:09.326405Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='9', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:09.403\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:09.400573Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='3', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:09.490\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:09.489995Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='d', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:09.566\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:09.566145Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=').\\n\\n', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:09.640\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:09.639203Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='For', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:09.719\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:09.718845Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' other', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:09.795\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:09.794983Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' pal', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:09.873\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:09.872822Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='lets', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:09.956\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:09.956453Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' mentioned', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:10.028\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:10.028045Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' but', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:10.101\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:10.101292Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' without', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:10.179\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:10.178751Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' specific', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:10.255\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:10.254757Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' recycling', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:10.326\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:10.326095Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' or', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:10.397\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:10.397553Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' certification', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:10.469\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:10.469164Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' details', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:10.541\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:10.540823Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=':\\n\\n', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:10.613\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:10.61309Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='-', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:10.685\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:10.685369Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' Rel', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:10.758\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:10.758249Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='icy', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:10.831\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:10.831232Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='c', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:10.904\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:10.904068Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' manages', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:10.981\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:10.980895Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' end', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:11.055\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:11.055413Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='-of', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:11.128\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:11.12794Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='-life', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:11.201\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:11.200959Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' plastic', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:11.276\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:11.275751Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' and', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:11.350\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:11.349359Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' wooden', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:11.424\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:11.424125Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' pal', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:11.496\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:11.495756Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='lets', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:11.572\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:11.571655Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' for', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:11.647\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:11.647131Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' reuse', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:11.721\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:11.720729Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' and', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:11.791\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:11.791588Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' recycling', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:11.862\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:11.862177Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' (', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:11.934\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:11.933624Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='Source', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:12.006\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:12.005547Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=':', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:12.078\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:12.077798Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' e', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:12.150\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:12.149705Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='4', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:12.221\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:12.220919Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='9', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:12.296\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:12.295852Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='bee', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:12.367\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:12.367461Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='6', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:12.438\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:12.43839Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='6', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:12.510\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:12.509576Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='-', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:12.581\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:12.580999Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='2', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:12.652\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:12.652192Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='4', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:12.723\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:12.723126Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='c', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:12.794\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:12.793823Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='1', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:12.865\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:12.864855Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='-', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:12.936\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:12.93556Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='4', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:13.007\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:13.00638Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='1', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:13.077\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:13.077225Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='0', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:13.148\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:13.148126Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='3', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:13.219\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:13.218735Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='-a', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:13.291\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:13.291172Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='0', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:13.362\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:13.361782Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='c', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:13.433\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:13.432737Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='8', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:13.504\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:13.503492Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='-', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:13.574\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:13.574384Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='5', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:13.646\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:13.646279Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='ea', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:13.719\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:13.71849Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='3', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:13.790\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:13.789766Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='2', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:13.861\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:13.86089Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='9', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:13.932\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:13.931581Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='7', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:14.002\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:14.002359Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='a', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:14.073\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:14.073357Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='4', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:14.144\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:14.144212Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='c', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:14.215\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:14.214851Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='9', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:14.286\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:14.286168Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='0', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:14.358\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:14.358032Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=').\\n', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:14.429\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:14.428879Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='-', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:14.501\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:14.50074Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' The', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:14.573\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:14.573403Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' company', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:14.646\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:14.646548Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' produces', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:14.720\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:14.720087Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' fully', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:14.792\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:14.792433Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' rec', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:14.865\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:14.864981Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='ycl', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:14.947\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:14.947507Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='able', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:15.021\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:15.021445Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' packaging', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:15.093\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:15.092867Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' and', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:15.164\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:15.163765Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' is', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:15.235\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:15.234924Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' registered', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:15.307\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:15.306769Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' with', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:15.378\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:15.378012Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' CO', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:15.451\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:15.45067Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='.N', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:15.524\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:15.523917Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='.I', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:15.596\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:15.595712Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='.P', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:15.677\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:15.677329Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='.', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:15.748\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:15.74783Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' for', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:15.824\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:15.823277Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' collection', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:15.897\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:15.897659Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' and', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:15.972\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:15.971929Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' recycling', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:16.042\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:16.04252Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' activities', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:16.113\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:16.113232Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' (', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:16.184\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:16.184217Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='Source', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:16.259\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:16.258732Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=':', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:16.334\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:16.33398Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=' ', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:16.405\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:16.40534Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='1', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:16.476\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:16.476534Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='f', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:16.550\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:16.549807Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='0', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:16.621\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:16.620733Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='5', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:16.707\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:16.706896Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='bf', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:16.779\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:16.778907Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='8', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:16.850\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:16.849864Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='6', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:16.927\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:16.926666Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='-', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:16.999\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:16.99907Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='9', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:17.074\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:17.073963Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='a', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:17.146\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:17.145635Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='9', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:17.218\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:17.218041Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='3', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:17.291\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:17.290796Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='-', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:17.366\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:17.36571Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='4', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:17.439\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:17.438976Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='3', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:17.509\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:17.509652Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='8', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:17.581\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:17.581281Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='4', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:17.656\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:17.655907Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='-b', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:17.731\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:17.730667Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='1', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:17.804\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:17.803892Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='d', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:17.878\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:17.877524Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='7', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:17.951\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:17.951291Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='-c', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:18.033\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:18.032943Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='0', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:18.105\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:18.105608Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='2', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:18.177\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:18.177074Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='1', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:18.251\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:18.251421Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='b', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:18.323\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:18.323245Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='3', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:18.394\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:18.3942Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='3', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:18.467\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:18.466651Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='9', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:18.538\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:18.537997Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='b', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:18.610\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:18.609808Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='7', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:18.684\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:18.684102Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='4', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:18.754\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:18.754612Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content='a', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:18.831\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:18.830841Z' done=False done_reason=None total_duration=None load_duration=None prompt_eval_count=None prompt_eval_duration=None eval_count=None eval_duration=None message=Message(role='assistant', content=').', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:18.906\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.ollama\u001b[0m:\u001b[36mgenerate_stream\u001b[0m:\u001b[36m116\u001b[0m - \u001b[34m\u001b[1mmodel='mistral-nemo:12b' created_at='2026-02-20T16:05:18.904957Z' done=True done_reason='stop' total_duration=56260888208 load_duration=5921035166 prompt_eval_count=1721 prompt_eval_duration=18496382167 eval_count=430 eval_duration=31663642957 message=Message(role='assistant', content='', thinking=None, images=None, tool_name=None, tool_calls=None) logprobs=None\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:18.911\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msme_kt_zh_collaboration_rag.baseline_rag\u001b[0m:\u001b[36mask\u001b[0m:\u001b[36m326\u001b[0m - \u001b[1mAnswer:\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:18.911\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msme_kt_zh_collaboration_rag.baseline_rag\u001b[0m:\u001b[36mask\u001b[0m:\u001b[36m327\u001b[0m - \u001b[1m  Based on the provided sources, here are the sustainability certifications and recycling information for the pallets in the portfolio:\n",
      "\n",
      "1. **Logypal 1**:\n",
      "   - The Logypal 1 pallet is made from recycled plastic content of 100%, with recycled materials being post-consumer plastic waste (Source: a733b848-ef47-4690-bd0c-cf41a134abc3).\n",
      "   - It has Kiwa certification for its recycled content (Accr. N.069B) (Source: a733b848-ef47-4690-bd0c-cf41a134abc3).\n",
      "\n",
      "2. **Noè Pallet**:\n",
      "   - The Noè pallet is made from secondary materials sourced from various recycling processes, including post-consumer beverage cartons and obsolete Noè pallets (Source: 1d2a1b35-67ae-4a76-ae63-f5557611d93d).\n",
      "   - It is recyclable at the end of its life, easily washable and sanitizable, resistant to rust, and stable in determining its tare weight (Source: 1d2a1b35-67ae-4a76-ae63-f5557611d93d).\n",
      "\n",
      "For other pallets mentioned but without specific recycling or certification details:\n",
      "\n",
      "- Relicyc manages end-of-life plastic and wooden pallets for reuse and recycling (Source: e49bee66-24c1-4103-a0c8-5ea3297a4c90).\n",
      "- The company produces fully recyclable packaging and is registered with CO.N.I.P. for collection and recycling activities (Source: 1f05bf86-9a93-4384-b1d7-c021b339b74a).\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:18.911\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msme_kt_zh_collaboration_rag.baseline_rag\u001b[0m:\u001b[36mask\u001b[0m:\u001b[36m328\u001b[0m - \u001b[1mSources (5):\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:18.911\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msme_kt_zh_collaboration_rag.baseline_rag\u001b[0m:\u001b[36mask\u001b[0m:\u001b[36m331\u001b[0m - \u001b[1m  '3_EPD_pallet_relicyc.pdf'  |  '# 40 YEARS OF SUSTAINABLE INNOVATION'\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:18.912\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msme_kt_zh_collaboration_rag.baseline_rag\u001b[0m:\u001b[36mask\u001b[0m:\u001b[36m331\u001b[0m - \u001b[1m  '3_EPD_pallet_relicyc.pdf'  |  '# CONTENT DECLARATION'\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:18.912\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msme_kt_zh_collaboration_rag.baseline_rag\u001b[0m:\u001b[36mask\u001b[0m:\u001b[36m331\u001b[0m - \u001b[1m  '3_EPD_pallet_relicyc.pdf'  |  '# PRODUCT INFORMATION'\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:18.913\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msme_kt_zh_collaboration_rag.baseline_rag\u001b[0m:\u001b[36mask\u001b[0m:\u001b[36m331\u001b[0m - \u001b[1m  '3_EPD_pallet_relicyc.pdf'  |  '# CLOSER TO CUSTOMERS THROUGH SHARED GOALS'\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:18.913\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msme_kt_zh_collaboration_rag.baseline_rag\u001b[0m:\u001b[36mask\u001b[0m:\u001b[36m331\u001b[0m - \u001b[1m  '2_EPD_pallet_CPR.pdf'  |  '#### 3. Product information'\u001b[0m\n",
      "\u001b[32m2026-02-20 17:05:18.913\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msme_kt_zh_collaboration_rag.baseline_rag\u001b[0m:\u001b[36mrun_pipeline\u001b[0m:\u001b[36m376\u001b[0m - \u001b[1m======= Baseline RAG pipeline — done =======\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided sources, here are the sustainability certifications and recycling information for the pallets in the portfolio:\n",
      "\n",
      "1. **Logypal 1**:\n",
      "   - The Logypal 1 pallet is made from recycled plastic content of 100%, with recycled materials being post-consumer plastic waste (Source: a733b848-ef47-4690-bd0c-cf41a134abc3).\n",
      "   - It has Kiwa certification for its recycled content (Accr. N.069B) (Source: a733b848-ef47-4690-bd0c-cf41a134abc3).\n",
      "\n",
      "2. **Noè Pallet**:\n",
      "   - The Noè pallet is made from secondary materials sourced from various recycling processes, including post-consumer beverage cartons and obsolete Noè pallets (Source: 1d2a1b35-67ae-4a76-ae63-f5557611d93d).\n",
      "   - It is recyclable at the end of its life, easily washable and sanitizable, resistant to rust, and stable in determining its tare weight (Source: 1d2a1b35-67ae-4a76-ae63-f5557611d93d).\n",
      "\n",
      "For other pallets mentioned but without specific recycling or certification details:\n",
      "\n",
      "- Relicyc manages end-of-life plastic and wooden pallets for reuse and recycling (Source: e49bee66-24c1-4103-a0c8-5ea3297a4c90).\n",
      "- The company produces fully recyclable packaging and is registered with CO.N.I.P. for collection and recycling activities (Source: 1f05bf86-9a93-4384-b1d7-c021b339b74a).\n"
     ]
    }
   ],
   "source": [
    "from sme_kt_zh_collaboration_rag.baseline_rag import run_pipeline\n",
    "\n",
    "answer = await run_pipeline(\n",
    "    backend=BACKEND,\n",
    "    query=\"What sustainability certifications do the pallets in the portfolio have?\",\n",
    "    reset_vs=False,\n",
    ")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p0-backends-md",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Switching LLM Backends\n",
    "\n",
    "The pipeline abstracts the LLM behind a common interface. Only `build_llm()` needs to change.\n",
    "\n",
    "| Backend | `BACKEND=` | Prerequisite |\n",
    "|---|---|---|\n",
    "| Ollama (local) | `\"ollama\"` | `ollama serve` + `ollama pull mistral-nemo:12b` |\n",
    "| OpenAI | `\"openai\"` | `OPENAI_API_KEY` env variable |\n",
    "| SDSC Qwen | `\"qwen\"` | `SDSC_QWEN3_32B_AWQ` env variable |\n",
    "\n",
    "You can also override the model within a backend:\n",
    "\n",
    "```python\n",
    "llm = build_llm(backend=\"openai\", model_name=\"gpt-4o\") # stronger model\n",
    "llm = build_llm(backend=\"ollama\", model_name=\"llama3.2\") # smaller local model\n",
    "```\n",
    "\n",
    "The RAG pipeline is **backend-agnostic**, the retrieval step is identical regardless of which LLM is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "p0-backends-code",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-02-20 16:11:56.724\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msme_kt_zh_collaboration_rag.baseline_rag\u001b[0m:\u001b[36mbuild_llm\u001b[0m:\u001b[36m110\u001b[0m - \u001b[1mLLM backend: OpenAI (gpt-4o-mini)\u001b[0m\n",
      "\u001b[32m2026-02-20 16:11:56.749\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.llms.openai\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m63\u001b[0m - \u001b[34m\u001b[1mOpenAI LLM loaded: gpt-4o-mini; temperature: 0.3; seed: 42; tools: None; tool_choice: None; response_format: {'type': 'text'}\u001b[0m\n",
      "\u001b[32m2026-02-20 16:11:56.750\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msme_kt_zh_collaboration_rag.baseline_rag\u001b[0m:\u001b[36mbuild_agent\u001b[0m:\u001b[36m306\u001b[0m - \u001b[1mRAG agent ready (top_k=5  query_expansion=0)\u001b[0m\n",
      "\u001b[32m2026-02-20 16:11:56.750\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msme_kt_zh_collaboration_rag.baseline_rag\u001b[0m:\u001b[36mask\u001b[0m:\u001b[36m323\u001b[0m - \u001b[1mQuery: 'What materials is the Lara pallet made from?'\u001b[0m\n",
      "\u001b[32m2026-02-20 16:11:56.859\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.embeddings.sentence_transformer\u001b[0m:\u001b[36mget_embeddings\u001b[0m:\u001b[36m76\u001b[0m - \u001b[34m\u001b[1msentence-transformers/all-MiniLM-L6-v2 embeddings size: (1, 384)\u001b[0m\n",
      "\u001b[32m2026-02-20 16:12:01.751\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msme_kt_zh_collaboration_rag.baseline_rag\u001b[0m:\u001b[36mask\u001b[0m:\u001b[36m326\u001b[0m - \u001b[1mAnswer:\u001b[0m\n",
      "\u001b[32m2026-02-20 16:12:01.752\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msme_kt_zh_collaboration_rag.baseline_rag\u001b[0m:\u001b[36mask\u001b[0m:\u001b[36m327\u001b[0m - \u001b[1m  The Lara pallet is primarily made from recycled plastics, specifically a mix of polypropylene and high-density polyethylene. The Logypal 1 model, which is a type of recycled polypropylene pallet, is composed of over 99% polyolefins and other trace materials, with a total recycled plastic content of 100%. This recycled content is derived from post-consumer plastic waste.\n",
      "\n",
      "In addition to the primary materials, the pallet may also include other components such as micronized aluminum, cellulose, resin, glass fibers, and various additives and pigments, which contribute to its overall structure and performance. The materials used are free from hazardous substances and are designed to be fully recyclable at the end of their life.\u001b[0m\n",
      "\u001b[32m2026-02-20 16:12:01.753\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msme_kt_zh_collaboration_rag.baseline_rag\u001b[0m:\u001b[36mask\u001b[0m:\u001b[36m328\u001b[0m - \u001b[1mSources (5):\u001b[0m\n",
      "\u001b[32m2026-02-20 16:12:01.753\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msme_kt_zh_collaboration_rag.baseline_rag\u001b[0m:\u001b[36mask\u001b[0m:\u001b[36m331\u001b[0m - \u001b[1m  '3_EPD_pallet_relicyc.pdf'  |  '# PRODUCT INFORMATION'\u001b[0m\n",
      "\u001b[32m2026-02-20 16:12:01.754\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msme_kt_zh_collaboration_rag.baseline_rag\u001b[0m:\u001b[36mask\u001b[0m:\u001b[36m331\u001b[0m - \u001b[1m  '3_EPD_pallet_relicyc.pdf'  |  '# CONTENT DECLARATION'\u001b[0m\n",
      "\u001b[32m2026-02-20 16:12:01.754\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msme_kt_zh_collaboration_rag.baseline_rag\u001b[0m:\u001b[36mask\u001b[0m:\u001b[36m331\u001b[0m - \u001b[1m  '2_EPD_pallet_CPR.pdf'  |  '#### 5. Content declaration'\u001b[0m\n",
      "\u001b[32m2026-02-20 16:12:01.755\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msme_kt_zh_collaboration_rag.baseline_rag\u001b[0m:\u001b[36mask\u001b[0m:\u001b[36m331\u001b[0m - \u001b[1m  '2_EPD_pallet_CPR.pdf'  |  '#### 3. Product information'\u001b[0m\n",
      "\u001b[32m2026-02-20 16:12:01.756\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msme_kt_zh_collaboration_rag.baseline_rag\u001b[0m:\u001b[36mask\u001b[0m:\u001b[36m331\u001b[0m - \u001b[1m  '3_EPD_pallet_relicyc.pdf'  |  '# 40 YEARS OF SUSTAINABLE INNOVATION'\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Test openai\n",
    "\n",
    "QUERY = \"What materials is the Lara pallet made from?\"\n",
    "\n",
    "llm_openai = build_llm(backend=\"openai\", model_name=\"gpt-4o-mini\")\n",
    "agent_openai = build_agent(vector_store, embedding_model, llm_openai)\n",
    "answer_openai = await ask(agent_openai, QUERY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p0-exercises-md",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Tasks\n",
    "\n",
    "1. **test set creation**: Go through the dataset and come up with questions and the corresponding correct answers that the RAG should give based on the query. ALso include trick questions, such as asking for information that is not in the data or for qhich contradicting infoormation exists. \n",
    "\n",
    "2. **Retrieval inspection** — Call `inspect_retrieval()` with different queries and inspect which files are returned? What do the scores tell you about how well the corpus covers this topic?\n",
    "\n",
    "3. **Top-k sensitivity**: Change `top_k` from 5 to 1. Does the answer to the the questions change? What about to 10? Is more always better?\n",
    "\n",
    "4. **System prompt ablation**: In `baseline_rag.py`, locate `SYSTEM_PROMPT`. Try changing it and then rebuild the agent and re-run the Lara Pallet query. Does the answer change?\n",
    "\n",
    "5. **Query phrasing**: The embedding model is sensitive to wording. Try `\"CO₂ footprint Logypal 1\"`, `\"carbon emissions recycled pallet\"`, and `\"GWP A1-A3 EPD pallet\"`. Do the top-1 chunk and score differ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "p0-exercises-code",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-02-20 17:04:03.251\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mconversational_toolkit.embeddings.sentence_transformer\u001b[0m:\u001b[36mget_embeddings\u001b[0m:\u001b[36m76\u001b[0m - \u001b[34m\u001b[1msentence-transformers/all-MiniLM-L6-v2 embeddings size: (1, 384)\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: 'PFAS-free tape declaration'  (top_k=5)\n",
      "  score=1.2978  Article-Document-CST-Synthetic-Rubber.pdf  '## **1. PRODUCT AND COMPANY IDENTIFICATION**'\n",
      "  score=1.3428  2_EPD_pallet_CPR.pdf  '#### 5. Content declaration'\n",
      "  score=1.3798  3_EPD_pallet_relicyc.pdf  '# CONTENT DECLARATION'\n",
      "  score=1.3867  2_EPD_pallet_CPR.pdf  '# Environmental Product Declaration'\n",
      "  score=1.3924  Article-Document-CST-Synthetic-Rubber.pdf  '## **15. REGULATORY INFORMATION**'\n"
     ]
    }
   ],
   "source": [
    "# Scratch cell — run your experiments here\n",
    "async def quick_retrieve(query: str, top_k: int = 5):\n",
    "    retriever = VectorStoreRetriever(embedding_model, vector_store, top_k=top_k)\n",
    "    results = await retriever.retrieve(query)\n",
    "    print(f\"Query: {query!r}  (top_k={top_k})\")\n",
    "    for r in results:\n",
    "        src = r.metadata.get(\"source_file\", \"?\")\n",
    "        print(f\"  score={r.score:.4f}  {src}  {r.title!r}\")\n",
    "\n",
    "\n",
    "await quick_retrieve(\"PFAS-free tape declaration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p0-summary-md",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "| Step | Function | \n",
    "|---|---|\n",
    "| 1. Load & chunk | `load_chunks(max_files)` |\n",
    "| 2. Embed & index | `build_vector_store(chunks, emb, reset)` |\n",
    "| 3. Inspect retrieval | `inspect_retrieval(query, vs, emb)` | \n",
    "| 4. Build agent | `build_agent(vs, emb, llm, top_k)` |\n",
    "| 5. Generate answer | `ask(agent, query, history)` |\n",
    "\n",
    "### Three Core Failure Modes (Addressed in Later Feature Tracks)\n",
    "- Wrong entity\n",
    "- Missing data presented as fact\n",
    "- Low recall\n",
    "\n",
    "**Next — Feature 1:** Explore chunking strategies and understand how chunk size affects retrieval quality.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_venv",
   "language": "python",
   "name": "my-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
