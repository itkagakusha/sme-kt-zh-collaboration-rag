import json

from loguru import logger
from conversational_toolkit.llms.base import LLM, LLMMessage, ToolCall, Roles, Function
from typing import Literal, AsyncGenerator, Optional, Any
from ollama import AsyncClient

from conversational_toolkit.tools.base import Tool
from conversational_toolkit.utils.metadata_provider import MetadataProvider


def message_to_ollama(msg: LLMMessage) -> dict[str, Any]:
    message = {
        "role": msg.role,
        "content": msg.content,
    }

    if msg.name:
        message["name"] = msg.name

    if msg.role == Roles.TOOL and msg.tool_call_id:
        message["tool_call_id"] = msg.tool_call_id

    if msg.role == Roles.ASSISTANT and msg.tool_calls:
        message["tool_calls"] = [  # type: ignore
            {
                "id": tc.id,
                "type": tc.type,
                "function": {
                    "name": tc.function.name,
                    "arguments": json.loads(tc.function.arguments),
                },
            }
            for tc in msg.tool_calls
        ]

    return message


class OllamaLLM(LLM):
    def __init__(
        self,
        temperature: float = 0.5,
        seed: int = 42,
        tools: Optional[list[Tool]] = None,
        tool_choice: str | None = None,
        model_name: str = "llama3.1",
        response_format: Literal["", "json"] = "",
        host: str | None = None,
    ):
        super().__init__()
        self.client = AsyncClient(host=host)
        self.model = model_name
        self.temperature = temperature
        self.seed = seed
        self.tools = tools
        self.tool_choice = tool_choice
        self.response_format = response_format
        logger.debug(
            f"Ollama LLM loaded: {model_name}; temperature: {temperature}; seed: {seed}; tools: {tools}; response_format: {response_format}"
        )

    async def generate(self, conversation: list[LLMMessage]) -> LLMMessage:
        """
        Generates a response based on the provided conversation.

        Parameters:
        conversation (list[dict[str, str]]): A list of messages in the format:
            [
                {"role": "system", "content": "You are a helpful assistant."},
                {"role": "user", "content": "Hello!"}
            ]

        Returns:
        dict: The completion generated by the model
        """
        completion = await self.client.chat(
            model=self.model,
            messages=[message_to_ollama(msg) for msg in conversation],
            format=self.response_format,
            tools=[tool.json_schema() for tool in self.tools] if self.tools else None,
        )
        logger.debug(f"Completion: {completion}")
        completion_message = completion.get("message", {})
        return LLMMessage(
            content=completion_message.get("content", ""),
            role=completion_message.get("role", "assistant"),
            tool_calls=completion_message.get("tool_calls", []),
        )

    async def generate_stream(self, conversation: list[LLMMessage]) -> AsyncGenerator[LLMMessage, None]:
        response = await self.client.chat(
            model=self.model,
            messages=[message_to_ollama(msg) for msg in conversation],
            format=self.response_format,
            tools=[tool.json_schema() for tool in self.tools] if self.tools else None,
            stream=True,
        )

        last_tool_call_sent = -1
        async for chunk in response:
            print(chunk)
            if not chunk["message"]:  # Last chunk has empty choices list
                continue
            if chunk["message"]:
                yield LLMMessage(content=chunk["message"]["content"])
            if chunk["message"].get("tool_calls", []):
                for index, tool_call in enumerate(chunk["message"]["tool_calls"]):
                    if index > last_tool_call_sent:
                        yield LLMMessage(
                            tool_calls=[
                                ToolCall(
                                    id=str(index),
                                    function=Function(
                                        name=tool_call.function.name, arguments=json.dumps(tool_call.function.arguments)
                                    ),
                                    type="function",
                                )
                            ]
                        )
                        last_tool_call_sent = index

        MetadataProvider.add_metadata(
            {
                "model": chunk["model"],
            }
        )
